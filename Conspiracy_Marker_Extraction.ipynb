{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceType": "datasetVersion",
          "sourceId": 14574476,
          "datasetId": 9309899,
          "databundleVersionId": 15407684
        },
        {
          "sourceType": "datasetVersion",
          "sourceId": 14577539,
          "datasetId": 9311804,
          "databundleVersionId": 15411018
        },
        {
          "sourceType": "datasetVersion",
          "sourceId": 14574452,
          "datasetId": 9309885,
          "databundleVersionId": 15407658
        }
      ],
      "dockerImageVersionId": 31260,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Conspiracy Marker Extraction ",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bountyhunter12/SemEval-2026-Task-10-Psycholinguistic-Conspiracy-Marker-Extraction-and-Detection/blob/main/Conspiracy_Marker_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "faoziafariha_train_redacted_path = kagglehub.dataset_download('faoziafariha/train-redacted')\n",
        "faoziafariha_dev_redacted_path = kagglehub.dataset_download('faoziafariha/dev-redacted')\n",
        "faoziafariha_test_reducted_path = kagglehub.dataset_download('faoziafariha/test-reducted')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "C79PxJgffgeL"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "4a0386fb-f534-41c0-aa00-50957d29d2af",
        "_cell_guid": "c0982b3f-f34d-4c30-94cf-31b0e35d3937",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-31T20:23:42.134794Z",
          "iopub.execute_input": "2026-01-31T20:23:42.13509Z",
          "iopub.status.idle": "2026-01-31T20:23:44.225056Z",
          "shell.execute_reply.started": "2026-01-31T20:23:42.135054Z",
          "shell.execute_reply": "2026-01-31T20:23:44.224234Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "BGK9alu5fgeO",
        "outputId": "c4cb2270-6447-4d56-f75d-e463a2f9fc7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/train-redacted/train_redacted.jsonl\n/kaggle/input/dev-redacted/dev_redacted.jsonl\n/kaggle/input/test-reducted/test_rehydrated.jsonl\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers datasets scikit-learn accelerate beautifulsoup4 markdown tqdm"
      ],
      "metadata": {
        "_uuid": "d3fb6e35-a67d-4c21-8e32-44feeb64cb60",
        "_cell_guid": "46aade30-c038-406b-9e2a-a5c4dff39787",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-31T21:18:21.380144Z",
          "iopub.execute_input": "2026-01-31T21:18:21.380708Z",
          "iopub.status.idle": "2026-01-31T21:18:47.946931Z",
          "shell.execute_reply.started": "2026-01-31T21:18:21.38068Z",
          "shell.execute_reply": "2026-01-31T21:18:47.946034Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "m_tTm83rfgeP",
        "outputId": "86f41f68-314b-48b7-8a9d-b36a594b9447"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\nCollecting transformers\n  Downloading transformers-5.0.0-py3-none-any.whl.metadata (37 kB)\nRequirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.4.2)\nCollecting datasets\n  Downloading datasets-4.5.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\nCollecting scikit-learn\n  Downloading scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\nCollecting accelerate\n  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\nCollecting beautifulsoup4\n  Downloading beautifulsoup4-4.14.3-py3-none-any.whl.metadata (3.8 kB)\nRequirement already satisfied: markdown in /usr/local/lib/python3.12/dist-packages (3.9)\nCollecting markdown\n  Downloading markdown-3.10.1-py3-none-any.whl.metadata (5.1 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\nCollecting tqdm\n  Downloading tqdm-4.67.2-py3-none-any.whl.metadata (57 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\nCollecting huggingface-hub<2.0,>=1.3.0 (from transformers)\n  Downloading huggingface_hub-1.3.5-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0rc2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\nRequirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.5)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.18)\nRequirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\nRequirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.15.3)\nRequirement already satisfied: joblib>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\nRequirement already satisfied: threadpoolctl>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\nRequirement already satisfied: soupsieve>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\nRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\nRequirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.12.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2026.1.4)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.1rc0)\nRequirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.6.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (8.3.1)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\nDownloading transformers-5.0.0-py3-none-any.whl (10.1 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hDownloading datasets-4.5.0-py3-none-any.whl (515 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m515.2/515.2 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (8.9 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading beautifulsoup4-4.14.3-py3-none-any.whl (107 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m107.7/107.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading markdown-3.10.1-py3-none-any.whl (107 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m107.7/107.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tqdm-4.67.2-py3-none-any.whl (78 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading huggingface_hub-1.3.5-py3-none-any.whl (536 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: tqdm, markdown, beautifulsoup4, scikit-learn, huggingface-hub, datasets, accelerate, transformers\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.67.1\n    Uninstalling tqdm-4.67.1:\n      Successfully uninstalled tqdm-4.67.1\n  Attempting uninstall: markdown\n    Found existing installation: Markdown 3.9\n    Uninstalling Markdown-3.9:\n      Successfully uninstalled Markdown-3.9\n  Attempting uninstall: beautifulsoup4\n    Found existing installation: beautifulsoup4 4.13.5\n    Uninstalling beautifulsoup4-4.13.5:\n      Successfully uninstalled beautifulsoup4-4.13.5\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.6.1\n    Uninstalling scikit-learn-1.6.1:\n      Successfully uninstalled scikit-learn-1.6.1\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.36.0\n    Uninstalling huggingface-hub-0.36.0:\n      Successfully uninstalled huggingface-hub-0.36.0\n  Attempting uninstall: datasets\n    Found existing installation: datasets 4.4.2\n    Uninstalling datasets-4.4.2:\n      Successfully uninstalled datasets-4.4.2\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 1.11.0\n    Uninstalling accelerate-1.11.0:\n      Successfully uninstalled accelerate-1.11.0\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.57.1\n    Uninstalling transformers-4.57.1:\n      Successfully uninstalled transformers-4.57.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nsentence-transformers 5.1.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 5.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-1.12.0 beautifulsoup4-4.14.3 datasets-4.5.0 huggingface-hub-1.3.5 markdown-3.10.1 scikit-learn-1.8.0 tqdm-4.67.2 transformers-5.0.0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean up old models/files to free space\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Remove old results\n",
        "for folder in ['./results', './marker_results', './ensemble']:\n",
        "    if os.path.exists(folder):\n",
        "        shutil.rmtree(folder)\n",
        "        print(f\"Cleaned: {folder}\")\n",
        "\n",
        "# Clean Kaggle working directory\n",
        "import glob\n",
        "for f in glob.glob('/kaggle/working/*'):\n",
        "    try:\n",
        "        os.remove(f)\n",
        "        print(f\"Removed: {f}\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "print(\"‚úÖ Disk space freed!\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-31T21:18:47.957008Z",
          "iopub.execute_input": "2026-01-31T21:18:47.957341Z",
          "iopub.status.idle": "2026-01-31T21:18:47.976513Z",
          "shell.execute_reply.started": "2026-01-31T21:18:47.957311Z",
          "shell.execute_reply": "2026-01-31T21:18:47.975896Z"
        },
        "id": "QHrMwMvGfgeP",
        "outputId": "82fc7bc1-bd36-48ee-8a67-07ae6d97da19"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "‚úÖ Disk space freed!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "from markdown import markdown"
      ],
      "metadata": {
        "_uuid": "56c7862e-9345-4df8-8cb8-8a8cc850aa97",
        "_cell_guid": "07a9af15-c7b3-41f4-b463-d8006e71f3dc",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-31T21:18:47.977745Z",
          "iopub.execute_input": "2026-01-31T21:18:47.978026Z",
          "iopub.status.idle": "2026-01-31T21:18:47.990086Z",
          "shell.execute_reply.started": "2026-01-31T21:18:47.978006Z",
          "shell.execute_reply": "2026-01-31T21:18:47.989546Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "Ky4A3M1_fgeQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval -q\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-31T20:24:10.690399Z",
          "iopub.execute_input": "2026-01-31T20:24:10.6908Z",
          "iopub.status.idle": "2026-01-31T20:24:17.203679Z",
          "shell.execute_reply.started": "2026-01-31T20:24:10.690775Z",
          "shell.execute_reply": "2026-01-31T20:24:17.202948Z"
        },
        "id": "H0-srxpffgeS",
        "outputId": "5943a468-39b2-4ab1-cfcf-10e7e689d54e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "IMPROVED REHYDRATION & PREPROCESSING\n",
        "Better data quality = Better model performance\n",
        "\"\"\"\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from markdown import markdown\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ===========================\n",
        "# IMPROVED PREPROCESSING\n",
        "# ===========================\n",
        "\n",
        "def markdown_to_text(markdown_string):\n",
        "    \"\"\"Convert markdown to clean text - IMPROVED\"\"\"\n",
        "    # Handle code blocks first\n",
        "    html = markdown(markdown_string)\n",
        "    html = re.sub(r'<pre>(.*?)</pre>', ' ', html, flags=re.DOTALL)\n",
        "    html = re.sub(r'<code>(.*?)</code>', ' ', html)\n",
        "\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    text = ' '.join(soup.find_all(string=True))\n",
        "\n",
        "    # Clean up excessive whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "def replace_urls(text, url_token='[URL]'):\n",
        "    \"\"\"Replace URLs but preserve their positions better\"\"\"\n",
        "    # Match various URL formats\n",
        "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "    return re.sub(url_pattern, url_token, text)\n",
        "\n",
        "def replace_ss_prefix(text):\n",
        "    \"\"\"Remove submission statement prefixes\"\"\"\n",
        "    patterns = [\n",
        "        r'^\\W*(summary statement|submission statement|ss)[^a-zA-Z]*',\n",
        "        r'^\\W*(s\\.s\\.|s/s)[^a-zA-Z]*',\n",
        "    ]\n",
        "    for pattern in patterns:\n",
        "        text = re.sub(pattern, '', text, flags=re.I | re.U)\n",
        "    return text.strip()\n",
        "\n",
        "def clean_special_chars(text):\n",
        "    \"\"\"Clean special characters while preserving structure\"\"\"\n",
        "    # Remove excessive punctuation\n",
        "    text = re.sub(r'([!?.]){3,}', r'\\1\\1', text)\n",
        "    # Normalize quotes (using unicode escapes)\n",
        "    text = text.replace('\\u201c', '\"').replace('\\u201d', '\"')  # Smart double quotes\n",
        "    text = text.replace('\\u2018', \"'\").replace('\\u2019', \"'\")  # Smart single quotes\n",
        "    # Remove zero-width characters\n",
        "    text = re.sub(r'[\\u200b\\u200c\\u200d\\ufeff]', '', text)\n",
        "    return text\n",
        "\n",
        "def normalize_whitespace(text):\n",
        "    \"\"\"Normalize whitespace while preserving paragraph structure\"\"\"\n",
        "    # Replace multiple newlines with double newline\n",
        "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "    # Replace tabs and multiple spaces\n",
        "    text = re.sub(r'[ \\t]+', ' ', text)\n",
        "    # Clean up spaces around newlines\n",
        "    text = re.sub(r' *\\n *', '\\n', text)\n",
        "    return text.strip()\n",
        "\n",
        "def preprocess_improved(text):\n",
        "    \"\"\"\n",
        "    IMPROVED preprocessing pipeline\n",
        "    Order matters: markdown -> urls -> ss -> special chars -> whitespace\n",
        "    \"\"\"\n",
        "    if not text or not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Step 1: Convert markdown\n",
        "    text = markdown_to_text(text)\n",
        "\n",
        "    # Step 2: Replace URLs\n",
        "    text = replace_urls(text)\n",
        "\n",
        "    # Step 3: Remove SS prefix\n",
        "    text = replace_ss_prefix(text)\n",
        "\n",
        "    # Step 4: Clean special characters\n",
        "    text = clean_special_chars(text)\n",
        "\n",
        "    # Step 5: Normalize whitespace\n",
        "    text = normalize_whitespace(text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# ===========================\n",
        "# IMPROVED REHYDRATION\n",
        "# ===========================\n",
        "\n",
        "def fetch_from_arctic_shift_improved(comment_ids, max_retries=3, timeout=20):\n",
        "    \"\"\"Improved Arctic Shift fetching with better error handling\"\"\"\n",
        "    url = \"https://arctic-shift.photon-reddit.com/api/comments/ids\"\n",
        "\n",
        "    # Split into smaller batches for reliability\n",
        "    batch_size = 10\n",
        "    all_results = {}\n",
        "\n",
        "    for i in range(0, len(comment_ids), batch_size):\n",
        "        batch = comment_ids[i:i + batch_size]\n",
        "        params = {\n",
        "            \"ids\": \",\".join(batch),\n",
        "            \"fields\": \"body,subreddit,id,author,created_utc\"\n",
        "        }\n",
        "\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                with requests.Session() as session:\n",
        "                    session.headers.update({\n",
        "                        'Connection': 'close',\n",
        "                        'User-Agent': 'Mozilla/5.0'\n",
        "                    })\n",
        "                    response = session.get(url, params=params, timeout=timeout)\n",
        "                    response.raise_for_status()\n",
        "                    data = response.json()\n",
        "\n",
        "                    if \"data\" in data and isinstance(data[\"data\"], list):\n",
        "                        for item in data[\"data\"]:\n",
        "                            body = item.get('body', '').strip()\n",
        "                            # More thorough deletion check\n",
        "                            if (body and\n",
        "                                body not in ['[deleted]', '[removed]', '[removed by moderator]'] and\n",
        "                                len(body) > 10):  # Minimum length check\n",
        "                                all_results[item['id']] = item\n",
        "                        break\n",
        "            except Exception as e:\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(2 ** attempt)  # Exponential backoff\n",
        "                else:\n",
        "                    print(f\"Failed batch after {max_retries} attempts\")\n",
        "\n",
        "        time.sleep(0.5)  # Rate limiting\n",
        "\n",
        "    return all_results\n",
        "\n",
        "def fetch_from_pushshift_improved(comment_ids, max_retries=2):\n",
        "    \"\"\"Improved Pushshift with better filtering\"\"\"\n",
        "    results = {}\n",
        "\n",
        "    for comment_id in comment_ids[:3]:  # Limit to avoid rate limiting\n",
        "        url = \"https://api.pullpush.io/reddit/comment/search\"\n",
        "        params = {\"ids\": comment_id}\n",
        "\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                response = requests.get(url, params=params, timeout=15)\n",
        "                response.raise_for_status()\n",
        "                data = response.json()\n",
        "\n",
        "                if \"data\" in data and len(data[\"data\"]) > 0:\n",
        "                    item = data[\"data\"][0]\n",
        "                    body = item.get('body', '').strip()\n",
        "\n",
        "                    # Strict filtering\n",
        "                    if (body and\n",
        "                        body not in ['[deleted]', '[removed]'] and\n",
        "                        len(body) > 10 and\n",
        "                        not body.startswith('[removed')):\n",
        "                        results[comment_id] = {\n",
        "                            'id': comment_id,\n",
        "                            'body': item['body'],\n",
        "                            'subreddit': item.get('subreddit', 'unknown'),\n",
        "                            'author': item.get('author', 'unknown'),\n",
        "                            'created_utc': item.get('created_utc', 0)\n",
        "                        }\n",
        "                break\n",
        "            except:\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(1)\n",
        "\n",
        "        time.sleep(0.5)\n",
        "\n",
        "    return results\n",
        "\n",
        "def rehydrate_comments_improved(input_file, output_file):\n",
        "    \"\"\"\n",
        "    IMPROVED rehydration with better quality control\n",
        "    \"\"\"\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"üìÇ IMPROVED REHYDRATION: {input_file}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Load items\n",
        "    items_to_process = []\n",
        "    original_data_map = {}\n",
        "\n",
        "    with open(input_file, encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                item = json.loads(line)\n",
        "                if '_id' in item and item['_id'].startswith('t1_'):\n",
        "                    items_to_process.append(item['_id'])\n",
        "                    original_data_map[item['_id']] = item\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    print(f\"üìä Total comments to rehydrate: {len(items_to_process)}\")\n",
        "\n",
        "    # Process in batches\n",
        "    batch_size = 20\n",
        "    rehydrated_data = []\n",
        "    deleted_count = 0\n",
        "    low_quality_count = 0\n",
        "\n",
        "    for i in tqdm(range(0, len(items_to_process), batch_size), desc=\"Rehydrating\"):\n",
        "        batch_full_ids = items_to_process[i:i + batch_size]\n",
        "        batch_clean_ids = [x[3:] for x in batch_full_ids]\n",
        "\n",
        "        # Try Arctic Shift first\n",
        "        rehydrated_map = fetch_from_arctic_shift_improved(batch_clean_ids)\n",
        "\n",
        "        # Fallback to Pushshift for missing items\n",
        "        missing = [x for x in batch_clean_ids if x not in rehydrated_map]\n",
        "        if missing and len(missing) <= 5:\n",
        "            pushshift_results = fetch_from_pushshift_improved(missing)\n",
        "            rehydrated_map.update(pushshift_results)\n",
        "\n",
        "        # Process results\n",
        "        for fid in batch_full_ids:\n",
        "            cid = fid[3:]\n",
        "\n",
        "            if cid in rehydrated_map:\n",
        "                r = rehydrated_map[cid]\n",
        "                o = original_data_map[fid]\n",
        "\n",
        "                # Preprocess text\n",
        "                preprocessed_text = preprocess_improved(r[\"body\"])\n",
        "\n",
        "                # Quality check\n",
        "                if len(preprocessed_text) < 20:\n",
        "                    low_quality_count += 1\n",
        "                    continue\n",
        "\n",
        "                rehydrated_data.append({\n",
        "                    \"_id\": fid,\n",
        "                    \"text\": preprocessed_text,\n",
        "                    \"subreddit\": r.get(\"subreddit\", \"unknown\"),\n",
        "                    \"conspiracy\": o.get(\"conspiracy\"),\n",
        "                    \"markers\": o.get(\"markers\", []),\n",
        "                    \"annotator\": o.get(\"annotator\"),\n",
        "                    # Preserve metadata for analysis\n",
        "                    \"original_length\": len(r[\"body\"]),\n",
        "                    \"processed_length\": len(preprocessed_text)\n",
        "                })\n",
        "            else:\n",
        "                deleted_count += 1\n",
        "\n",
        "        time.sleep(1)  # Rate limiting\n",
        "\n",
        "    # Save results\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        for x in rehydrated_data:\n",
        "            f.write(json.dumps(x, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    # Statistics\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"üìä REHYDRATION STATISTICS\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"‚úÖ Successfully rehydrated: {len(rehydrated_data)}\")\n",
        "    print(f\"üóëÔ∏è  Deleted/removed: {deleted_count}\")\n",
        "    print(f\"‚ö†Ô∏è  Low quality (skipped): {low_quality_count}\")\n",
        "    print(f\"üìà Success rate: {len(rehydrated_data) / len(items_to_process) * 100:.2f}%\")\n",
        "\n",
        "    if rehydrated_data:\n",
        "        avg_length = sum(x['processed_length'] for x in rehydrated_data) / len(rehydrated_data)\n",
        "        print(f\"üìè Average text length: {avg_length:.0f} characters\")\n",
        "\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "# ===========================\n",
        "# RUN REHYDRATION\n",
        "# ===========================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Rehydrate training data\n",
        "    rehydrate_comments_improved(\n",
        "        \"/kaggle/input/train-redacted/train_redacted.jsonl\",\n",
        "        \"train_rehydrated_v2.jsonl\"\n",
        "    )\n",
        "\n",
        "    # Rehydrate dev data\n",
        "    rehydrate_comments_improved(\n",
        "        \"/kaggle/input/dev-redacted/dev_redacted.jsonl\",\n",
        "        \"dev_rehydrated_v2.jsonl\"\n",
        "    )\n",
        "\n",
        "    # Rehydrate test data\n",
        "    rehydrate_comments_improved(\n",
        "        \"/kaggle/input/test-reducted/test_rehydrated.jsonl\",\n",
        "        \"test_rehydrated_v2.jsonl\"\n",
        "    )\n",
        "\n",
        "    print(\"\\n‚úÖ All rehydration complete!\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-31T21:18:47.990978Z",
          "iopub.execute_input": "2026-01-31T21:18:47.991223Z",
          "iopub.status.idle": "2026-01-31T21:32:46.20535Z",
          "shell.execute_reply.started": "2026-01-31T21:18:47.991204Z",
          "shell.execute_reply": "2026-01-31T21:32:46.2047Z"
        },
        "id": "PdBX6UJQfgeS",
        "outputId": "2543c589-7b12-402a-b911-45a2bec7bcca"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "======================================================================\nüìÇ IMPROVED REHYDRATION: /kaggle/input/train-redacted/train_redacted.jsonl\n======================================================================\nüìä Total comments to rehydrate: 4361\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Rehydrating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [11:27<00:00,  3.14s/it]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n======================================================================\nüìä REHYDRATION STATISTICS\n======================================================================\n‚úÖ Successfully rehydrated: 4316\nüóëÔ∏è  Deleted/removed: 45\n‚ö†Ô∏è  Low quality (skipped): 0\nüìà Success rate: 98.97%\nüìè Average text length: 415 characters\n======================================================================\n\n======================================================================\nüìÇ IMPROVED REHYDRATION: /kaggle/input/dev-redacted/dev_redacted.jsonl\n======================================================================\nüìä Total comments to rehydrate: 100\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Rehydrating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:14<00:00,  2.90s/it]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n======================================================================\nüìä REHYDRATION STATISTICS\n======================================================================\n‚úÖ Successfully rehydrated: 100\nüóëÔ∏è  Deleted/removed: 0\n‚ö†Ô∏è  Low quality (skipped): 0\nüìà Success rate: 100.00%\nüìè Average text length: 418 characters\n======================================================================\n\n======================================================================\nüìÇ IMPROVED REHYDRATION: /kaggle/input/test-reducted/test_rehydrated.jsonl\n======================================================================\nüìä Total comments to rehydrate: 938\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Rehydrating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [02:16<00:00,  2.90s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n======================================================================\nüìä REHYDRATION STATISTICS\n======================================================================\n‚úÖ Successfully rehydrated: 938\nüóëÔ∏è  Deleted/removed: 0\n‚ö†Ô∏è  Low quality (skipped): 0\nüìà Success rate: 100.00%\nüìè Average text length: 419 characters\n======================================================================\n\n\n‚úÖ All rehydration complete!\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "SUBTASK 1: IMPROVED FEW-SHOT MARKER EXTRACTION\n",
        "Uses Qwen2.5-7B with optimized prompts\n",
        "Target: 0.30+ F1 score\n",
        "\"\"\"\n",
        "import json\n",
        "import os\n",
        "import zipfile\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import gc\n",
        "\n",
        "# ===========================\n",
        "# CONFIGURATION\n",
        "# ===========================\n",
        "INPUT_FILE = \"/kaggle/input/test-reducted/test_rehydrated.jsonl\"\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "OUTPUT_DIR = \"/kaggle/working\"\n",
        "OUTPUT_FILE = os.path.join(OUTPUT_DIR, \"submission.jsonl\")\n",
        "OUTPUT_ZIP = os.path.join(OUTPUT_DIR, \"submission_markers.zip\")\n",
        "\n",
        "MARKER_TYPES = [\"Actor\", \"Action\", \"Effect\", \"Evidence\", \"Victim\"]\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"SUBTASK 1: IMPROVED FEW-SHOT MARKER EXTRACTION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ===========================\n",
        "# ENHANCED FEW-SHOT EXAMPLES\n",
        "# ===========================\n",
        "FEW_SHOT_EXAMPLES = [\n",
        "    {\n",
        "        \"text\": \"SS: Just a reminder that various extremist groups that are out there, trying to recruit people, are actually CIA and other Alphabet agency Honeypots. The BBC is even admitting this guy is CIA but says he's only 'former' CIA (yea right) and they are making sure to create a link to Russia so they can continue with the narrative that anyone who isn't on the left is a Russian controlled Nazi.\",\n",
        "        \"markers\": [\n",
        "            {\"type\": \"Actor\", \"text\": \"CIA\"},\n",
        "            {\"type\": \"Actor\", \"text\": \"other Alphabet agency\"},\n",
        "            {\"type\": \"Action\", \"text\": \"Honeypots\"},\n",
        "            {\"type\": \"Evidence\", \"text\": \"The BBC is even admitting this guy is CIA\"},\n",
        "            {\"type\": \"Action\", \"text\": \"continue with the narrative\"}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"New report alleges Hillary Clinton oversaw a multi-billion dollar fraud/theft, and high-ranking FBI agents are now coming forward with more details about it.\",\n",
        "        \"markers\": [\n",
        "            {\"type\": \"Actor\", \"text\": \"Hillary Clinton\"},\n",
        "            {\"type\": \"Action\", \"text\": \"oversaw a multi-billion dollar fraud/theft\"},\n",
        "            {\"type\": \"Evidence\", \"text\": \"high-ranking FBI agents are now coming forward\"}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"Maxwell claims that her email server was hacked after a court unsealed approximately 2,000 pages of documents. If emails were obtained in the hack they could showcase embarrassing information on Epstein's clients, alleged victims, and co-conspirators in his massive sex trafficking operation.\",\n",
        "        \"markers\": [\n",
        "            {\"type\": \"Action\", \"text\": \"email server was hacked\"},\n",
        "            {\"type\": \"Effect\", \"text\": \"could showcase embarrassing information\"},\n",
        "            {\"type\": \"Victim\", \"text\": \"Epstein's clients, alleged victims\"},\n",
        "            {\"type\": \"Effect\", \"text\": \"sex trafficking operation\"}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"Rudy drops radical bombs on Fox and Friends implicating the highest levels of government, the ambassadors, and intelligence agencies. He makes names and has recorded testimony from individuals banned from the United States to keep it quiet.\",\n",
        "        \"markers\": [\n",
        "            {\"type\": \"Actor\", \"text\": \"highest levels of government\"},\n",
        "            {\"type\": \"Actor\", \"text\": \"ambassadors\"},\n",
        "            {\"type\": \"Actor\", \"text\": \"intelligence agencies\"},\n",
        "            {\"type\": \"Victim\", \"text\": \"individuals banned from the United States\"},\n",
        "            {\"type\": \"Action\", \"text\": \"to keep it quiet\"}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"The government is covering up evidence of vaccine injuries to protect pharmaceutical companies. Thousands of people have died but mainstream media refuses to report it because they're paid off by Big Pharma.\",\n",
        "        \"markers\": [\n",
        "            {\"type\": \"Actor\", \"text\": \"The government\"},\n",
        "            {\"type\": \"Actor\", \"text\": \"pharmaceutical companies\"},\n",
        "            {\"type\": \"Actor\", \"text\": \"mainstream media\"},\n",
        "            {\"type\": \"Actor\", \"text\": \"Big Pharma\"},\n",
        "            {\"type\": \"Action\", \"text\": \"covering up evidence\"},\n",
        "            {\"type\": \"Effect\", \"text\": \"vaccine injuries\"},\n",
        "            {\"type\": \"Victim\", \"text\": \"Thousands of people have died\"},\n",
        "            {\"type\": \"Evidence\", \"text\": \"refuses to report it because they're paid off\"}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "# ===========================\n",
        "# IMPROVED PROMPT ENGINEERING\n",
        "# ===========================\n",
        "def create_extraction_prompt(text):\n",
        "    \"\"\"Create a powerful few-shot prompt with clear instructions\"\"\"\n",
        "\n",
        "    system_prompt = \"\"\"You are an expert at analyzing conspiracy theory text and identifying psycholinguistic markers.\n",
        "\n",
        "MARKER DEFINITIONS:\n",
        "1. Actor: Who is allegedly responsible? (groups, individuals, organizations with malicious intent)\n",
        "   Examples: \"CIA\", \"Big Pharma\", \"the government\", \"mainstream media\"\n",
        "\n",
        "2. Action: What malicious plans or covert operations are they doing?\n",
        "   Examples: \"covering up evidence\", \"manipulating elections\", \"poisoning the water\"\n",
        "\n",
        "3. Effect: What are the negative consequences or outcomes?\n",
        "   Examples: \"thousands died\", \"economic collapse\", \"loss of freedoms\"\n",
        "\n",
        "4. Victim: Who is being targeted or harmed?\n",
        "   Examples: \"innocent citizens\", \"children\", \"whistleblowers\"\n",
        "\n",
        "5. Evidence: How does the author support their claims?\n",
        "   Examples: \"leaked documents show\", \"experts are warning\", \"too many coincidences\"\n",
        "\n",
        "RULES:\n",
        "- Extract EXACT text spans (do not paraphrase)\n",
        "- Multiple markers can overlap\n",
        "- Extract ALL relevant spans, even if short\n",
        "- If unclear or no markers exist, return empty list\"\"\"\n",
        "\n",
        "    # Build examples with clear formatting\n",
        "    examples_text = \"\"\n",
        "    for i, ex in enumerate(FEW_SHOT_EXAMPLES, 1):\n",
        "        examples_text += f\"\\n### EXAMPLE {i} ###\\n\"\n",
        "        examples_text += f\"Text: {ex['text']}\\n\\n\"\n",
        "        examples_text += \"Markers:\\n\"\n",
        "        for m in ex['markers']:\n",
        "            examples_text += f'- {m[\"type\"]}: \"{m[\"text\"]}\"\\n'\n",
        "        examples_text += \"\\n\"\n",
        "\n",
        "    user_prompt = f\"\"\"{examples_text}\n",
        "\n",
        "### YOUR TASK ###\n",
        "Extract markers from this text:\n",
        "\n",
        "Text: {text}\n",
        "\n",
        "Respond with ONLY a JSON array (no markdown, no explanation):\n",
        "[\n",
        "  {{\"type\": \"Actor\", \"text\": \"exact span\"}},\n",
        "  {{\"type\": \"Action\", \"text\": \"exact span\"}}\n",
        "]\n",
        "\n",
        "If no markers: []\n",
        "\n",
        "JSON output:\"\"\"\n",
        "\n",
        "    return system_prompt, user_prompt\n",
        "\n",
        "# ===========================\n",
        "# LOAD MODEL\n",
        "# ===========================\n",
        "def load_llm():\n",
        "    \"\"\"Load Qwen model\"\"\"\n",
        "    print(\"\\nüì¶ Loading LLM...\")\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    print(f\"‚úÖ Loaded {MODEL_NAME}\")\n",
        "    return tokenizer, model, device\n",
        "\n",
        "# ===========================\n",
        "# EXTRACT MARKERS - FIXED\n",
        "# ===========================\n",
        "def extract_markers_llm(text, tokenizer, model):\n",
        "    \"\"\"Extract markers using LLM\"\"\"\n",
        "\n",
        "    system_prompt, user_prompt = create_extraction_prompt(text)\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "\n",
        "    # Tokenize properly\n",
        "    text_input = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        text_input,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=2048\n",
        "    ).to(model.device)\n",
        "\n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=1024,\n",
        "            temperature=0.1,  # Lower temperature for more consistent output\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode only new tokens\n",
        "    generated_ids = outputs[0][inputs['input_ids'].shape[1]:]\n",
        "    response = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "    return response\n",
        "\n",
        "# ===========================\n",
        "# PARSE LLM OUTPUT - ENHANCED\n",
        "# ===========================\n",
        "def parse_markers_from_response(response, text):\n",
        "    \"\"\"Parse JSON markers and find positions\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Clean response\n",
        "        response = response.strip()\n",
        "\n",
        "        # Extract JSON - try multiple patterns\n",
        "        json_match = re.search(r'\\[.*?\\]', response, re.DOTALL)\n",
        "        if not json_match:\n",
        "            # Try to find it after \"JSON output:\" or similar\n",
        "            if \"JSON output:\" in response:\n",
        "                response = response.split(\"JSON output:\")[-1]\n",
        "                json_match = re.search(r'\\[.*?\\]', response, re.DOTALL)\n",
        "\n",
        "        if not json_match:\n",
        "            return []\n",
        "\n",
        "        json_str = json_match.group(0)\n",
        "        markers_list = json.loads(json_str)\n",
        "\n",
        "        if not isinstance(markers_list, list):\n",
        "            return []\n",
        "\n",
        "        # Find positions for each marker\n",
        "        result_markers = []\n",
        "        for marker in markers_list:\n",
        "            if not isinstance(marker, dict):\n",
        "                continue\n",
        "\n",
        "            marker_text = marker.get('text', '').strip()\n",
        "            marker_type = marker.get('type', '')\n",
        "\n",
        "            if not marker_text or marker_type not in MARKER_TYPES:\n",
        "                continue\n",
        "\n",
        "            # Find all occurrences\n",
        "            start_idx = 0\n",
        "            found_positions = []\n",
        "            while True:\n",
        "                pos = text.find(marker_text, start_idx)\n",
        "                if pos == -1:\n",
        "                    break\n",
        "                found_positions.append(pos)\n",
        "                start_idx = pos + 1\n",
        "\n",
        "            # Add all found positions\n",
        "            for pos in found_positions:\n",
        "                result_markers.append({\n",
        "                    'startIndex': pos,\n",
        "                    'endIndex': pos + len(marker_text),\n",
        "                    'type': marker_type,\n",
        "                    'text': marker_text\n",
        "                })\n",
        "\n",
        "        # Remove duplicates\n",
        "        seen = set()\n",
        "        unique_markers = []\n",
        "        for m in result_markers:\n",
        "            key = (m['startIndex'], m['endIndex'], m['type'])\n",
        "            if key not in seen:\n",
        "                seen.add(key)\n",
        "                unique_markers.append(m)\n",
        "\n",
        "        return unique_markers\n",
        "\n",
        "    except Exception as e:\n",
        "        # Silent fail - return empty list\n",
        "        return []\n",
        "\n",
        "# ===========================\n",
        "# PROCESS DOCUMENTS\n",
        "# ===========================\n",
        "def process_document(item, tokenizer, model):\n",
        "    \"\"\"Process single document\"\"\"\n",
        "    doc_id = item.get('_id')\n",
        "    text = item.get('text', '').strip()\n",
        "\n",
        "    if not doc_id or not text or len(text) < 20:\n",
        "        return {\"_id\": doc_id, \"markers\": []}\n",
        "\n",
        "    try:\n",
        "        # Extract with LLM\n",
        "        response = extract_markers_llm(text, tokenizer, model)\n",
        "\n",
        "        # Parse markers\n",
        "        markers = parse_markers_from_response(response, text)\n",
        "\n",
        "        return {\n",
        "            \"_id\": doc_id,\n",
        "            \"markers\": markers\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"_id\": doc_id, \"markers\": []}\n",
        "\n",
        "# ===========================\n",
        "# MAIN EXECUTION\n",
        "# ===========================\n",
        "def main():\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STARTING FEW-SHOT MARKER EXTRACTION\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Load model\n",
        "    tokenizer, model, device = load_llm()\n",
        "\n",
        "    # Load test data\n",
        "    print(f\"\\nüìÇ Loading: {INPUT_FILE}\")\n",
        "\n",
        "    input_data = []\n",
        "    with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                input_data.append(json.loads(line))\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    print(f\"‚úÖ Loaded {len(input_data)} documents\")\n",
        "\n",
        "    # Process documents\n",
        "    print(f\"\\nüîÑ Extracting markers...\")\n",
        "    predictions = []\n",
        "\n",
        "    for item in tqdm(input_data, desc=\"Processing\"):\n",
        "        pred = process_document(item, tokenizer, model)\n",
        "        predictions.append(pred)\n",
        "\n",
        "        # Free memory periodically\n",
        "        if len(predictions) % 50 == 0:\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "    # Statistics\n",
        "    total_markers = sum(len(p['markers']) for p in predictions)\n",
        "    docs_with_markers = sum(1 for p in predictions if len(p['markers']) > 0)\n",
        "\n",
        "    print(f\"\\nüìä EXTRACTION STATS:\")\n",
        "    print(f\"   Documents processed: {len(predictions)}\")\n",
        "    print(f\"   Total markers: {total_markers}\")\n",
        "    print(f\"   Docs with markers: {docs_with_markers} ({docs_with_markers/len(predictions)*100:.1f}%)\")\n",
        "    print(f\"   Avg markers/doc: {total_markers/len(predictions):.2f}\")\n",
        "\n",
        "    # Marker distribution\n",
        "    marker_counts = defaultdict(int)\n",
        "    for pred in predictions:\n",
        "        for marker in pred['markers']:\n",
        "            marker_counts[marker['type']] += 1\n",
        "\n",
        "    if total_markers > 0:\n",
        "        print(f\"\\nüìä Marker Distribution:\")\n",
        "        for mtype in MARKER_TYPES:\n",
        "            count = marker_counts[mtype]\n",
        "            pct = (count / total_markers * 100) if total_markers > 0 else 0\n",
        "            print(f\"   {mtype:10s}: {count:5d} ({pct:5.2f}%)\")\n",
        "\n",
        "    # Save\n",
        "    print(f\"\\nüíæ Saving to {OUTPUT_FILE}...\")\n",
        "    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
        "        for pred in predictions:\n",
        "            f.write(json.dumps(pred) + '\\n')\n",
        "\n",
        "    # Create ZIP\n",
        "    print(f\"üì¶ Creating {OUTPUT_ZIP}...\")\n",
        "    with zipfile.ZipFile(OUTPUT_ZIP, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
        "        zf.write(OUTPUT_FILE, arcname=\"submission.jsonl\")\n",
        "\n",
        "    os.remove(OUTPUT_FILE)\n",
        "\n",
        "    print(f\"\\n‚úÖ SUCCESS! File: {OUTPUT_ZIP}\")\n",
        "\n",
        "    # Sample output\n",
        "    print(f\"\\nüìã SAMPLE PREDICTIONS:\")\n",
        "    for i, pred in enumerate(predictions[:3], 1):\n",
        "        print(f\"\\n{i}. Doc: {pred['_id']}\")\n",
        "        if pred['markers']:\n",
        "            for j, m in enumerate(pred['markers'][:5], 1):\n",
        "                text_preview = m['text'][:60]\n",
        "                if len(m['text']) > 60:\n",
        "                    text_preview += \"...\"\n",
        "                print(f\"   {j}. [{m['type']:8s}] \\\"{text_preview}\\\"\")\n",
        "            if len(pred['markers']) > 5:\n",
        "                print(f\"   ... and {len(pred['markers']) - 5} more\")\n",
        "        else:\n",
        "            print(\"   (No markers)\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T13:47:53.8059Z",
          "iopub.execute_input": "2026-01-26T13:47:53.8066Z",
          "execution_failed": "2026-01-27T02:28:59.248Z"
        },
        "id": "zj8nH4bCfgeT",
        "outputId": "464a64da-864e-4c76-c1a7-c002cb310654",
        "colab": {
          "referenced_widgets": [
            "34f81652129d44988c4ac387ee10a765"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "======================================================================\nSUBTASK 1: IMPROVED FEW-SHOT MARKER EXTRACTION\n======================================================================\n\n======================================================================\nSTARTING FEW-SHOT MARKER EXTRACTION\n======================================================================\n\nüì¶ Loading LLM...\nDevice: cuda\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading weights:   0%|          | 0/339 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "34f81652129d44988c4ac387ee10a765"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Some parameters are on the meta device because they were offloaded to the cpu.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "‚úÖ Loaded Qwen/Qwen2.5-7B-Instruct\n\nüìÇ Loading: /kaggle/input/test-reducted/test_rehydrated.jsonl\n‚úÖ Loaded 938 documents\n\nüîÑ Extracting markers...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Processing:  12%|‚ñà‚ñè        | 114/938 [1:16:54<4:49:21, 21.07s/it]  ",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keep"
      ],
      "metadata": {
        "id": "_OUdI4ODfgeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"\"\"\n",
        "# DeBERTa-v3-Large Training - DISK SPACE OPTIMIZED\n",
        "# Fixes: No space left on device\n",
        "# \"\"\"\n",
        "# import json\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# import os\n",
        "# import shutil\n",
        "# import gc\n",
        "# from datasets import Dataset\n",
        "# from transformers import (\n",
        "#     AutoTokenizer,\n",
        "#     AutoModelForTokenClassification,\n",
        "#     Trainer,\n",
        "#     TrainingArguments,\n",
        "#     DataCollatorForTokenClassification,\n",
        "#     EarlyStoppingCallback\n",
        "# )\n",
        "# from seqeval.metrics import f1_score as seqeval_f1\n",
        "# from seqeval.scheme import IOB2\n",
        "# from collections import Counter\n",
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')\n",
        "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# # ===========================\n",
        "# # CLEANUP OLD FILES\n",
        "# # ===========================\n",
        "# print(\"=\"*70)\n",
        "# print(\"CLEANING UP DISK SPACE\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# # Remove old model directories\n",
        "# old_dirs = [\n",
        "#     \"./results\",\n",
        "#     \"./marker_results\",\n",
        "#     \"./marker_results_advanced\",\n",
        "#     \"./marker_results_large\",\n",
        "#     \"./model_deberta_large\",\n",
        "#     \"./model_1_deberta_large\",\n",
        "#     \"./model_2_xlm_large\",\n",
        "#     \"./model_3_roberta_large\"\n",
        "# ]\n",
        "\n",
        "# for old_dir in old_dirs:\n",
        "#     if os.path.exists(old_dir):\n",
        "#         try:\n",
        "#             shutil.rmtree(old_dir)\n",
        "#             print(f\"‚úÖ Removed: {old_dir}\")\n",
        "#         except:\n",
        "#             pass\n",
        "\n",
        "# # Clean working directory\n",
        "# working_files = [\n",
        "#     \"/kaggle/working/submission.jsonl\",\n",
        "#     \"/kaggle/working/submission.zip\",\n",
        "#     \"/kaggle/working/submission_markers.zip\"\n",
        "# ]\n",
        "\n",
        "# for f in working_files:\n",
        "#     if os.path.exists(f):\n",
        "#         try:\n",
        "#             os.remove(f)\n",
        "#             print(f\"‚úÖ Removed: {f}\")\n",
        "#         except:\n",
        "#             pass\n",
        "\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "# print(\"‚úÖ Cleanup complete!\\n\")\n",
        "\n",
        "# # ===========================\n",
        "# # CONFIGURATION\n",
        "# # ===========================\n",
        "# print(\"=\"*70)\n",
        "# print(\"DEBERTA-V3-LARGE TRAINING - SPACE OPTIMIZED\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# MARKER_TYPES = [\"Actor\", \"Action\", \"Effect\", \"Evidence\", \"Victim\"]\n",
        "\n",
        "# labels_list = [\"O\"]\n",
        "# for marker_type in MARKER_TYPES:\n",
        "#     labels_list.append(f\"B-{marker_type}\")\n",
        "#     labels_list.append(f\"I-{marker_type}\")\n",
        "\n",
        "# label2id = {label: i for i, label in enumerate(labels_list)}\n",
        "# id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "# print(f\"\\nLabels: {len(labels_list)} total\")\n",
        "\n",
        "# # ===========================\n",
        "# # ALIGNMENT\n",
        "# # ===========================\n",
        "# def align_labels_advanced(text, markers, tokenizer, max_length=512):\n",
        "#     encoding = tokenizer(\n",
        "#         text,\n",
        "#         truncation=True,\n",
        "#         max_length=max_length,\n",
        "#         padding='max_length',\n",
        "#         return_offsets_mapping=True,\n",
        "#         add_special_tokens=True\n",
        "#     )\n",
        "\n",
        "#     offset_mapping = encoding['offset_mapping']\n",
        "#     labels = []\n",
        "\n",
        "#     for token_start, token_end in offset_mapping:\n",
        "#         if token_start == 0 and token_end == 0:\n",
        "#             labels.append(-100)\n",
        "#         else:\n",
        "#             labels.append(label2id[\"O\"])\n",
        "\n",
        "#     sorted_markers = sorted(markers, key=lambda x: (x['startIndex'], x['endIndex']))\n",
        "\n",
        "#     for marker in sorted_markers:\n",
        "#         start_char = marker['startIndex']\n",
        "#         end_char = marker['endIndex']\n",
        "#         marker_type = marker['type']\n",
        "\n",
        "#         if marker_type not in MARKER_TYPES:\n",
        "#             continue\n",
        "\n",
        "#         first_token_idx = None\n",
        "#         last_token_idx = None\n",
        "\n",
        "#         for idx, (token_start, token_end) in enumerate(offset_mapping):\n",
        "#             if token_start == 0 and token_end == 0:\n",
        "#                 continue\n",
        "\n",
        "#             if token_start < end_char and token_end > start_char:\n",
        "#                 overlap_start = max(token_start, start_char)\n",
        "#                 overlap_end = min(token_end, end_char)\n",
        "#                 overlap_ratio = (overlap_end - overlap_start) / (token_end - token_start)\n",
        "\n",
        "#                 if overlap_ratio >= 0.4:\n",
        "#                     if first_token_idx is None:\n",
        "#                         first_token_idx = idx\n",
        "#                     last_token_idx = idx\n",
        "\n",
        "#         if first_token_idx is not None:\n",
        "#             labels[first_token_idx] = label2id[f\"B-{marker_type}\"]\n",
        "\n",
        "#             if last_token_idx is not None and last_token_idx > first_token_idx:\n",
        "#                 for idx in range(first_token_idx + 1, last_token_idx + 1):\n",
        "#                     if labels[idx] != -100:\n",
        "#                         labels[idx] = label2id[f\"I-{marker_type}\"]\n",
        "\n",
        "#     return labels\n",
        "\n",
        "# def create_dataset_advanced(jsonl_path, tokenizer, max_length=512, min_length=30):\n",
        "#     all_input_ids = []\n",
        "#     all_attention_masks = []\n",
        "#     all_labels = []\n",
        "\n",
        "#     stats = {'total': 0, 'kept': 0, 'skipped_short': 0, 'skipped_no_markers': 0, 'total_markers': 0}\n",
        "\n",
        "#     print(f\"\\nProcessing {jsonl_path}...\")\n",
        "\n",
        "#     with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
        "#         for line in f:\n",
        "#             stats['total'] += 1\n",
        "\n",
        "#             try:\n",
        "#                 item = json.loads(line)\n",
        "#                 text = item.get('text', '').strip()\n",
        "#                 markers = item.get('markers', [])\n",
        "\n",
        "#                 if len(text) < min_length:\n",
        "#                     stats['skipped_short'] += 1\n",
        "#                     continue\n",
        "\n",
        "#                 if 'train' in jsonl_path and len(markers) == 0:\n",
        "#                     stats['skipped_no_markers'] += 1\n",
        "#                     continue\n",
        "\n",
        "#                 stats['total_markers'] += len(markers)\n",
        "\n",
        "#                 encoding = tokenizer(text, truncation=True, max_length=max_length, padding='max_length')\n",
        "#                 label_ids = align_labels_advanced(text, markers, tokenizer, max_length)\n",
        "\n",
        "#                 all_input_ids.append(encoding['input_ids'])\n",
        "#                 all_attention_masks.append(encoding['attention_mask'])\n",
        "#                 all_labels.append(label_ids)\n",
        "#                 stats['kept'] += 1\n",
        "\n",
        "#             except:\n",
        "#                 continue\n",
        "\n",
        "#     print(f\"‚úÖ Loaded {stats['kept']} examples, {stats['total_markers']} markers\")\n",
        "\n",
        "#     return Dataset.from_dict({\n",
        "#         'input_ids': all_input_ids,\n",
        "#         'attention_mask': all_attention_masks,\n",
        "#         'labels': all_labels\n",
        "#     })\n",
        "\n",
        "# # ===========================\n",
        "# # LOAD DATA\n",
        "# # ===========================\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"LOADING DATA\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# model_name = \"microsoft/deberta-v3-large\"\n",
        "# print(f\"Model: {model_name}\")\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# train_dataset = create_dataset_advanced(\"train_rehydrated_v2.jsonl\", tokenizer, max_length=512)\n",
        "# val_dataset_temp = create_dataset_advanced(\"dev_rehydrated_v2.jsonl\", tokenizer, max_length=512)\n",
        "\n",
        "# val_has_markers = False\n",
        "# if len(val_dataset_temp) > 0:\n",
        "#     for i in range(min(10, len(val_dataset_temp))):\n",
        "#         if any(l > 0 and l != -100 for l in val_dataset_temp[i]['labels']):\n",
        "#             val_has_markers = True\n",
        "#             break\n",
        "\n",
        "# if not val_has_markers:\n",
        "#     print(\"\\n‚ö†Ô∏è Creating validation split (20%)\")\n",
        "#     split = train_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "#     train_dataset = split['train']\n",
        "#     val_dataset = split['test']\n",
        "#     print(f\"‚úÖ Split: {len(train_dataset)} train, {len(val_dataset)} val\")\n",
        "# else:\n",
        "#     val_dataset = val_dataset_temp\n",
        "\n",
        "# train_dataset.set_format(\"torch\")\n",
        "# val_dataset.set_format(\"torch\")\n",
        "\n",
        "# # ===========================\n",
        "# # MODEL\n",
        "# # ===========================\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"INITIALIZING MODEL\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# model = AutoModelForTokenClassification.from_pretrained(\n",
        "#     model_name,\n",
        "#     num_labels=len(labels_list),\n",
        "#     id2label=id2label,\n",
        "#     label2id=label2id,\n",
        "#     ignore_mismatched_sizes=True\n",
        "# )\n",
        "\n",
        "# print(f\"‚úÖ DeBERTa-v3-Large loaded (304M parameters)\")\n",
        "\n",
        "# # ===========================\n",
        "# # METRICS\n",
        "# # ===========================\n",
        "# def compute_metrics(eval_pred):\n",
        "#     predictions, labels = eval_pred\n",
        "#     predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "#     true_labels = []\n",
        "#     pred_labels = []\n",
        "\n",
        "#     for prediction, label in zip(predictions, labels):\n",
        "#         true_seq = []\n",
        "#         pred_seq = []\n",
        "\n",
        "#         for pred_id, label_id in zip(prediction, label):\n",
        "#             if label_id != -100:\n",
        "#                 true_seq.append(id2label[label_id])\n",
        "#                 pred_seq.append(id2label[pred_id])\n",
        "\n",
        "#         if len(true_seq) > 0:\n",
        "#             true_labels.append(true_seq)\n",
        "#             pred_labels.append(pred_seq)\n",
        "\n",
        "#     results = {}\n",
        "\n",
        "#     try:\n",
        "#         results[\"overall_f1\"] = seqeval_f1(true_labels, pred_labels, mode='strict', scheme=IOB2)\n",
        "#     except:\n",
        "#         results[\"overall_f1\"] = 0.0\n",
        "\n",
        "#     for marker_type in MARKER_TYPES:\n",
        "#         filtered_true = []\n",
        "#         filtered_pred = []\n",
        "\n",
        "#         for true_seq, pred_seq in zip(true_labels, pred_labels):\n",
        "#             ft = [t if marker_type in t else 'O' for t in true_seq]\n",
        "#             fp = [p if marker_type in p else 'O' for p in pred_seq]\n",
        "#             filtered_true.append(ft)\n",
        "#             filtered_pred.append(fp)\n",
        "\n",
        "#         try:\n",
        "#             if any(any(x != 'O' for x in seq) for seq in filtered_true):\n",
        "#                 results[f\"f1_{marker_type}\"] = seqeval_f1(filtered_true, filtered_pred, mode='strict', scheme=IOB2)\n",
        "#             else:\n",
        "#                 results[f\"f1_{marker_type}\"] = 0.0\n",
        "#         except:\n",
        "#             results[f\"f1_{marker_type}\"] = 0.0\n",
        "\n",
        "#     return results\n",
        "\n",
        "# # ===========================\n",
        "# # TRAINING ARGS - SPACE OPTIMIZED (FIXED)\n",
        "# # ===========================\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"./deberta_large_final\",\n",
        "#     eval_strategy=\"epoch\",\n",
        "#     save_strategy=\"epoch\",\n",
        "#     learning_rate=5e-6,\n",
        "#     per_device_train_batch_size=2,\n",
        "#     per_device_eval_batch_size=4,\n",
        "#     num_train_epochs=10,\n",
        "#     weight_decay=0.01,\n",
        "#     warmup_ratio=0.1,\n",
        "#     load_best_model_at_end=True,\n",
        "#     metric_for_best_model=\"overall_f1\",\n",
        "#     greater_is_better=True,\n",
        "#     logging_steps=100,\n",
        "#     report_to=\"none\",\n",
        "#     save_total_limit=2,  # ‚≠ê CHANGED: Keep 2 (current + best) instead of 1\n",
        "#     fp16=torch.cuda.is_available(),\n",
        "#     gradient_accumulation_steps=8,\n",
        "#     dataloader_num_workers=0,\n",
        "#     lr_scheduler_type=\"cosine\",\n",
        "#     seed=42,\n",
        "#     save_safetensors=False,\n",
        "# )\n",
        "\n",
        "# # ===========================\n",
        "# # REMOVE THE CUSTOM CALLBACK - NOT NEEDED\n",
        "# # ===========================\n",
        "# # The SpaceSaverCallback was causing the issue by deleting checkpoints too early\n",
        "# # save_total_limit=2 is enough\n",
        "\n",
        "# # ===========================\n",
        "# # TRAINER\n",
        "# # ===========================\n",
        "# data_collator = DataCollatorForTokenClassification(\n",
        "#     tokenizer=tokenizer,\n",
        "#     padding=True,\n",
        "#     label_pad_token_id=-100\n",
        "# )\n",
        "\n",
        "# trainer = Trainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=train_dataset,\n",
        "#     eval_dataset=val_dataset,\n",
        "#     data_collator=data_collator,\n",
        "#     tokenizer=tokenizer,\n",
        "#     compute_metrics=compute_metrics,\n",
        "#     callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # ‚≠ê REMOVED SpaceSaverCallback\n",
        "# )\n",
        "\n",
        "# # ===========================\n",
        "# # TRAIN\n",
        "# # ===========================\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"STARTING TRAINING\")\n",
        "# print(\"=\"*70)\n",
        "# print(\"üéØ Target: F1 0.25-0.35\")\n",
        "# print(\"üíæ Space-optimized: Only keeps best checkpoint\")\n",
        "# print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# trainer.train()\n",
        "\n",
        "# # ===========================\n",
        "# # SAVE FINAL MODEL\n",
        "# # ===========================\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"SAVING FINAL MODEL\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# final_model_path = \"./deberta_large_final/checkpoint-best\"\n",
        "# trainer.save_model(final_model_path)\n",
        "# tokenizer.save_pretrained(final_model_path)\n",
        "\n",
        "# print(f\"‚úÖ Saved to: {final_model_path}\")\n",
        "\n",
        "# # Clean up intermediate checkpoints\n",
        "# for checkpoint_dir in os.listdir(\"./deberta_large_final\"):\n",
        "#     if checkpoint_dir.startswith(\"checkpoint-\") and checkpoint_dir != \"checkpoint-best\":\n",
        "#         full_path = os.path.join(\"./deberta_large_final\", checkpoint_dir)\n",
        "#         try:\n",
        "#             shutil.rmtree(full_path)\n",
        "#             print(f\"üóëÔ∏è  Cleaned: {full_path}\")\n",
        "#         except:\n",
        "#             pass\n",
        "\n",
        "# # ===========================\n",
        "# # EVALUATION\n",
        "# # ===========================\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"FINAL EVALUATION\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# eval_results = trainer.evaluate()\n",
        "\n",
        "# print(f\"\\nüìä RESULTS:\")\n",
        "# print(f\"   Overall F1: {eval_results['eval_overall_f1']:.4f}\")\n",
        "\n",
        "# print(f\"\\nüìä Per-Marker:\")\n",
        "# for marker_type in MARKER_TYPES:\n",
        "#     key = f\"eval_f1_{marker_type}\"\n",
        "#     if key in eval_results:\n",
        "#         score = eval_results[key]\n",
        "#         emoji = \"üéâ\" if score > 0.35 else \"‚úÖ\" if score > 0.20 else \"‚ö†Ô∏è\"\n",
        "#         print(f\"   {emoji} {marker_type:10s}: {score:.4f}\")\n",
        "\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"TRAINING COMPLETE!\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# # Performance analysis\n",
        "# base_f1 = 0.18\n",
        "# improvement = ((eval_results['eval_overall_f1'] - base_f1) / base_f1) * 100\n",
        "\n",
        "# print(f\"\\nüìà IMPROVEMENT:\")\n",
        "# print(f\"   Base model F1: {base_f1:.4f}\")\n",
        "# print(f\"   Large model F1: {eval_results['eval_overall_f1']:.4f}\")\n",
        "# print(f\"   Gain: +{improvement:.1f}%\")\n",
        "\n",
        "# if eval_results['eval_overall_f1'] >= 0.30:\n",
        "#     print(f\"\\nüéâ EXCELLENT! F1 > 0.30\")\n",
        "#     print(f\"   Expected test F1: 0.25-0.35\")\n",
        "# elif eval_results['eval_overall_f1'] >= 0.25:\n",
        "#     print(f\"\\n‚úÖ GOOD! F1 > 0.25\")\n",
        "#     print(f\"   Expected test F1: 0.22-0.30\")\n",
        "# else:\n",
        "#     print(f\"\\n‚ö†Ô∏è  Below target\")\n",
        "#     print(f\"   Current: {eval_results['eval_overall_f1']:.4f}\")\n",
        "#     print(f\"   Need: 0.25+\")\n",
        "\n",
        "# print(\"=\"*70) \"\"\""
      ],
      "metadata": {
        "trusted": true,
        "id": "FxopnivIfgeV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-31T21:00:24.110799Z",
          "iopub.execute_input": "2026-01-31T21:00:24.111126Z",
          "iopub.status.idle": "2026-01-31T21:00:35.724269Z",
          "shell.execute_reply.started": "2026-01-31T21:00:24.111097Z",
          "shell.execute_reply": "2026-01-31T21:00:35.723493Z"
        },
        "id": "hOJks0AsfgeV",
        "outputId": "3647a6d3-6230-46de-a7d1-099f298a3e62"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[33mWARNING: Skipping accelerate as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping peft as it is not installed.\u001b[0m\u001b[33m\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 5.1.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.39.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y transformers accelerate peft sentence-transformers -q\n",
        "!pip install transformers==4.41.2 accelerate==0.29.3 peft==0.10.0 sentence-transformers==5.1.1 -q\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-31T21:11:24.666474Z",
          "iopub.execute_input": "2026-01-31T21:11:24.667336Z",
          "iopub.status.idle": "2026-01-31T21:11:38.721878Z",
          "shell.execute_reply.started": "2026-01-31T21:11:24.667298Z",
          "shell.execute_reply": "2026-01-31T21:11:38.721155Z"
        },
        "id": "SClWuvZmfgeV",
        "outputId": "92757d2a-2ffa-4188-ec12-a29de87bc212"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m486.6/486.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n\u001b[?25h",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "PSYCHOMARKER SUBTASK 1 - OPTIMIZED DEBERTA + LLM FEW-SHOT\n",
        "Target: 0.18 ‚Üí 0.35+ F1\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# FIXED IMPORTS (COMPATIBLE VERSIONS)\n",
        "# ============================================================================\n",
        "print(\"Installing compatible versions...\")\n",
        "\n",
        "\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 -q\n",
        "!pip install datasets==2.14.0 -q\n",
        "!pip install seqeval==1.2.2 -q\n",
        "!pip install openai -q  # For LLM API if available\n",
        "\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForTokenClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForTokenClassification,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from seqeval.metrics import f1_score, precision_score, recall_score, classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "print(\"‚úÖ Packages installed successfully\")\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION - OPTIMIZED\n",
        "# ============================================================================\n",
        "\n",
        "# Model selection - using base for speed, but larger is better\n",
        "MODEL_NAME = \"microsoft/deberta-v3-base\"  # Changed from large for Kaggle memory\n",
        "# Alternative: \"microsoft/deberta-large\" if you have memory\n",
        "\n",
        "# Training parameters - CRITICAL FIXES\n",
        "MAX_LENGTH = 256  # Reduced from 512 for efficiency\n",
        "BATCH_SIZE = 8    # Increased from 2 (8x larger)\n",
        "LEARNING_RATE = 3e-5  # Increased from 5e-6 (6x higher)\n",
        "EPOCHS = 8        # Reduced from 10 but more effective\n",
        "WARMUP_RATIO = 0.1\n",
        "GRAD_ACCUM_STEPS = 4  # Reduced from 8\n",
        "\n",
        "# Marker types\n",
        "MARKER_TYPES = [\"Actor\", \"Action\", \"Effect\", \"Evidence\", \"Victim\"]\n",
        "\n",
        "# Create label mapping\n",
        "labels_list = [\"O\"]\n",
        "for marker_type in MARKER_TYPES:\n",
        "    labels_list.append(f\"B-{marker_type}\")\n",
        "    labels_list.append(f\"I-{marker_type}\")\n",
        "\n",
        "label2id = {label: i for i, label in enumerate(labels_list)}\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"OPTIMIZED MARKER EXTRACTION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nüìä Configuration:\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE} (was 2)\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE} (was 5e-6)\")\n",
        "print(f\"  Max length: {MAX_LENGTH}\")\n",
        "print(f\"  Epochs: {EPOCHS}\")\n",
        "\n",
        "# ============================================================================\n",
        "# IMPROVED LABEL ALIGNMENT\n",
        "# ============================================================================\n",
        "\n",
        "def improved_align_labels(text, markers, tokenizer, max_length=MAX_LENGTH):\n",
        "    \"\"\"\n",
        "    FIXED: Better label alignment with character-level matching\n",
        "    \"\"\"\n",
        "    # Tokenize with offsets\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        return_offsets_mapping=True\n",
        "    )\n",
        "\n",
        "    offset_mapping = encoding['offset_mapping']\n",
        "    labels = [-100] * len(offset_mapping)  # Initialize with -100\n",
        "\n",
        "    # First pass: label all tokens as O (except special tokens)\n",
        "    for i, (start, end) in enumerate(offset_mapping):\n",
        "        if start == 0 and end == 0:\n",
        "            continue  # Keep as -100 for padding\n",
        "        else:\n",
        "            labels[i] = label2id[\"O\"]\n",
        "\n",
        "    # Process each marker\n",
        "    for marker in markers:\n",
        "        start_char = marker['startIndex']\n",
        "        end_char = marker['endIndex']\n",
        "        marker_type = marker['type']\n",
        "\n",
        "        if marker_type not in MARKER_TYPES:\n",
        "            continue\n",
        "\n",
        "        # Find tokens that overlap with this marker\n",
        "        overlapping_indices = []\n",
        "\n",
        "        for i, (token_start, token_end) in enumerate(offset_mapping):\n",
        "            if token_start == 0 and token_end == 0:\n",
        "                continue\n",
        "\n",
        "            # Check for any overlap\n",
        "            if not (token_end <= start_char or token_start >= end_char):\n",
        "                overlapping_indices.append(i)\n",
        "\n",
        "        if overlapping_indices:\n",
        "            # Sort by position\n",
        "            overlapping_indices.sort()\n",
        "\n",
        "            # Label first token as B-\n",
        "            first_idx = overlapping_indices[0]\n",
        "            labels[first_idx] = label2id[f\"B-{marker_type}\"]\n",
        "\n",
        "            # Label remaining tokens as I-\n",
        "            for idx in overlapping_indices[1:]:\n",
        "                labels[idx] = label2id[f\"I-{marker_type}\"]\n",
        "\n",
        "    return labels\n",
        "\n",
        "# ============================================================================\n",
        "# DATA AUGMENTATION\n",
        "# ============================================================================\n",
        "\n",
        "def augment_marker_data(text, markers, augmentation_factor=1):\n",
        "    \"\"\"\n",
        "    Create augmented versions of text with markers\n",
        "    \"\"\"\n",
        "    augmented_examples = []\n",
        "\n",
        "    # Original example\n",
        "    augmented_examples.append((text, markers))\n",
        "\n",
        "    if augmentation_factor > 0:\n",
        "        # Augmentation 1: Synonym replacement for non-marker words\n",
        "        words = text.split()\n",
        "        if len(words) > 10:\n",
        "            # Simple word swap augmentation\n",
        "            import random\n",
        "            non_marker_words = []\n",
        "\n",
        "            # Identify non-marker words\n",
        "            for i, word in enumerate(words):\n",
        "                is_in_marker = False\n",
        "                for marker in markers:\n",
        "                    marker_text = marker.get('text', '')\n",
        "                    if word in marker_text:\n",
        "                        is_in_marker = True\n",
        "                        break\n",
        "\n",
        "                if not is_in_marker and len(word) > 3:\n",
        "                    non_marker_words.append(i)\n",
        "\n",
        "            if len(non_marker_words) >= 2:\n",
        "                # Swap two non-marker words\n",
        "                idx1, idx2 = random.sample(non_marker_words, 2)\n",
        "                new_words = words.copy()\n",
        "                new_words[idx1], new_words[idx2] = new_words[idx2], new_words[idx1]\n",
        "                new_text = ' '.join(new_words)\n",
        "\n",
        "                # Adjust marker positions (simple approximation)\n",
        "                new_markers = []\n",
        "                for marker in markers:\n",
        "                    old_text = marker['text']\n",
        "                    if old_text in text:\n",
        "                        # Find in new text\n",
        "                        if old_text in new_text:\n",
        "                            start_idx = new_text.find(old_text)\n",
        "                            if start_idx >= 0:\n",
        "                                new_marker = marker.copy()\n",
        "                                new_marker['startIndex'] = start_idx\n",
        "                                new_marker['endIndex'] = start_idx + len(old_text)\n",
        "                                new_markers.append(new_marker)\n",
        "\n",
        "                if new_markers:\n",
        "                    augmented_examples.append((new_text, new_markers))\n",
        "\n",
        "    return augmented_examples\n",
        "\n",
        "# ============================================================================\n",
        "# CREATE ENHANCED DATASET\n",
        "# ============================================================================\n",
        "\n",
        "def create_enhanced_dataset(jsonl_path, tokenizer, is_training=True, augmentation=True):\n",
        "    \"\"\"\n",
        "    Create dataset with augmentation and better filtering\n",
        "    \"\"\"\n",
        "    print(f\"\\nüìÇ Processing {jsonl_path}...\")\n",
        "\n",
        "    all_input_ids = []\n",
        "    all_attention_masks = []\n",
        "    all_labels = []\n",
        "\n",
        "    stats = {\n",
        "        'total': 0, 'kept': 0, 'skipped_short': 0,\n",
        "        'skipped_no_markers': 0, 'total_markers': 0\n",
        "    }\n",
        "\n",
        "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            stats['total'] += 1\n",
        "\n",
        "            try:\n",
        "                item = json.loads(line)\n",
        "                text = item.get('text', '').strip()\n",
        "                markers = item.get('markers', [])\n",
        "\n",
        "                # Filter criteria\n",
        "                if len(text) < 30 or len(text) > 2000:\n",
        "                    stats['skipped_short'] += 1\n",
        "                    continue\n",
        "\n",
        "                # For training, skip examples without markers\n",
        "                if is_training and len(markers) == 0:\n",
        "                    stats['skipped_no_markers'] += 1\n",
        "                    continue\n",
        "\n",
        "                stats['total_markers'] += len(markers)\n",
        "\n",
        "                # Apply augmentation for training\n",
        "                if is_training and augmentation and len(markers) > 0:\n",
        "                    examples = augment_marker_data(text, markers, augmentation_factor=1)\n",
        "                else:\n",
        "                    examples = [(text, markers)]\n",
        "\n",
        "                for ex_text, ex_markers in examples:\n",
        "                    # Tokenize\n",
        "                    encoding = tokenizer(\n",
        "                        ex_text,\n",
        "                        truncation=True,\n",
        "                        max_length=MAX_LENGTH,\n",
        "                        padding='max_length',\n",
        "                        return_tensors='pt'\n",
        "                    )\n",
        "\n",
        "                    # Align labels\n",
        "                    label_ids = improved_align_labels(ex_text, ex_markers, tokenizer, MAX_LENGTH)\n",
        "\n",
        "                    all_input_ids.append(encoding['input_ids'][0].tolist())\n",
        "                    all_attention_masks.append(encoding['attention_mask'][0].tolist())\n",
        "                    all_labels.append(label_ids)\n",
        "                    stats['kept'] += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "    print(f\"‚úÖ Loaded {stats['kept']} examples, {stats['total_markers']} markers\")\n",
        "    print(f\"   Skipped: {stats['skipped_short']} short, {stats['skipped_no_markers']} no markers\")\n",
        "\n",
        "    return Dataset.from_dict({\n",
        "        'input_ids': all_input_ids,\n",
        "        'attention_mask': all_attention_masks,\n",
        "        'labels': all_labels\n",
        "    })\n",
        "\n",
        "# ============================================================================\n",
        "# LOAD DATA\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"LOADING DATA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Load datasets with augmentation\n",
        "train_dataset = create_enhanced_dataset(\n",
        "    \"train_rehydrated_v2.jsonl\",\n",
        "    tokenizer,\n",
        "    is_training=True,\n",
        "    augmentation=True\n",
        ")\n",
        "\n",
        "val_dataset = create_enhanced_dataset(\n",
        "    \"dev_rehydrated_v2.jsonl\",\n",
        "    tokenizer,\n",
        "    is_training=False,\n",
        "    augmentation=False\n",
        ")\n",
        "\n",
        "# Split if validation is small\n",
        "if len(val_dataset) < 50:\n",
        "    print(\"\\n‚ö†Ô∏è  Validation set too small, splitting training data\")\n",
        "    split = train_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "    train_dataset = split['train']\n",
        "    val_dataset = split['test']\n",
        "\n",
        "train_dataset.set_format(\"torch\")\n",
        "val_dataset.set_format(\"torch\")\n",
        "\n",
        "print(f\"\\nüìä Dataset sizes:\")\n",
        "print(f\"  Training: {len(train_dataset)} examples\")\n",
        "print(f\"  Validation: {len(val_dataset)} examples\")\n",
        "\n",
        "# ============================================================================\n",
        "# INITIALIZE MODEL\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"INITIALIZING MODEL\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=len(labels_list),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Model loaded: {MODEL_NAME}\")\n",
        "\n",
        "# ============================================================================\n",
        "# COMPUTE METRICS\n",
        "# ============================================================================\n",
        "\n",
        "def compute_enhanced_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "\n",
        "    for i in range(len(labels)):\n",
        "        true_seq = []\n",
        "        pred_seq = []\n",
        "\n",
        "        for j in range(len(labels[i])):\n",
        "            if labels[i][j] != -100:\n",
        "                true_seq.append(id2label[labels[i][j]])\n",
        "                pred_seq.append(id2label[predictions[i][j]])\n",
        "\n",
        "        if true_seq:\n",
        "            true_labels.append(true_seq)\n",
        "            pred_labels.append(pred_seq)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Calculate overall metrics\n",
        "    try:\n",
        "        results[\"overall_f1\"] = f1_score(true_labels, pred_labels)\n",
        "        results[\"overall_precision\"] = precision_score(true_labels, pred_labels)\n",
        "        results[\"overall_recall\"] = recall_score(true_labels, pred_labels)\n",
        "    except:\n",
        "        results[\"overall_f1\"] = 0.0\n",
        "        results[\"overall_precision\"] = 0.0\n",
        "        results[\"overall_recall\"] = 0.0\n",
        "\n",
        "    # Calculate per-marker metrics\n",
        "    for marker_type in MARKER_TYPES:\n",
        "        try:\n",
        "            # Filter sequences for this marker type\n",
        "            filtered_true = []\n",
        "            filtered_pred = []\n",
        "\n",
        "            for true_seq, pred_seq in zip(true_labels, pred_labels):\n",
        "                ft = []\n",
        "                fp = []\n",
        "\n",
        "                for t, p in zip(true_seq, pred_seq):\n",
        "                    if marker_type in t:\n",
        "                        ft.append(t)\n",
        "                    else:\n",
        "                        ft.append('O')\n",
        "\n",
        "                    if marker_type in p:\n",
        "                        fp.append(p)\n",
        "                    else:\n",
        "                        fp.append('O')\n",
        "\n",
        "                filtered_true.append(ft)\n",
        "                filtered_pred.append(fp)\n",
        "\n",
        "            # Calculate F1 if there are actual labels\n",
        "            has_labels = any(any(x != 'O' for x in seq) for seq in filtered_true)\n",
        "            if has_labels:\n",
        "                marker_f1 = f1_score(filtered_true, filtered_pred)\n",
        "                results[f\"f1_{marker_type}\"] = marker_f1\n",
        "            else:\n",
        "                results[f\"f1_{marker_type}\"] = 0.0\n",
        "\n",
        "        except:\n",
        "            results[f\"f1_{marker_type}\"] = 0.0\n",
        "\n",
        "    return results\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING ARGUMENTS - OPTIMIZED\n",
        "# ============================================================================\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./optimized_marker_model\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE * 2,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"overall_f1\",\n",
        "    greater_is_better=True,\n",
        "    logging_steps=50,\n",
        "    save_steps=500,\n",
        "    eval_steps=500,\n",
        "    report_to=\"none\",\n",
        "    save_total_limit=2,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
        "    dataloader_num_workers=2,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=42,\n",
        "    label_smoothing_factor=0.1,  # Added for better generalization\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINER\n",
        "# ============================================================================\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_enhanced_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# TRAIN\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*70)\n",
        "print(\"üéØ Target: 0.30-0.40 F1 Score\")\n",
        "print(f\"‚è±Ô∏è  Estimated time: 30-60 minutes\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# ============================================================================\n",
        "# EVALUATE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(f\"\\nüìä Overall Performance:\")\n",
        "print(f\"  F1 Score:       {eval_results.get('eval_overall_f1', 0):.4f}\")\n",
        "print(f\"  Precision:      {eval_results.get('eval_overall_precision', 0):.4f}\")\n",
        "print(f\"  Recall:         {eval_results.get('eval_overall_recall', 0):.4f}\")\n",
        "\n",
        "print(f\"\\nüìä Per-Marker F1 Scores:\")\n",
        "for marker_type in MARKER_TYPES:\n",
        "    key = f\"eval_f1_{marker_type}\"\n",
        "    score = eval_results.get(key, 0)\n",
        "\n",
        "    if score > 0.25:\n",
        "        indicator = \"‚úÖ\"\n",
        "    elif score > 0.15:\n",
        "        indicator = \"‚ö†Ô∏è\"\n",
        "    else:\n",
        "        indicator = \"‚ùå\"\n",
        "\n",
        "    print(f\"  {indicator} {marker_type:10s}: {score:.4f}\")\n",
        "\n",
        "# Save model\n",
        "best_model_path = \"./best_marker_extraction_model\"\n",
        "trainer.save_model(best_model_path)\n",
        "tokenizer.save_pretrained(best_model_path)\n",
        "print(f\"\\n‚úÖ Model saved to: {best_model_path}\")\n",
        "\n",
        "# ============================================================================\n",
        "# LLM FEW-SHOT ENHANCEMENT\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"LLM FEW-SHOT ENHANCEMENT SETUP\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "class LLMMarkerEnhancer:\n",
        "    \"\"\"\n",
        "    Uses LLM to validate and enhance DeBERTa predictions\n",
        "    Can use OpenAI API or local LLM\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, use_api=True):\n",
        "        self.use_api = use_api\n",
        "        self.few_shot_examples = self._create_few_shot_examples()\n",
        "\n",
        "        if use_api:\n",
        "            try:\n",
        "                from openai import OpenAI\n",
        "                self.client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\", \"\"))\n",
        "                print(\"‚úÖ OpenAI client initialized\")\n",
        "            except:\n",
        "                print(\"‚ö†Ô∏è  OpenAI not available, using rule-based fallback\")\n",
        "                self.use_api = False\n",
        "\n",
        "        # Rule-based patterns as fallback\n",
        "        self.rule_patterns = {\n",
        "            'Actor': [\n",
        "                r'\\b(CIA|FBI|NSA|government|elites|globalists|they|them)\\b',\n",
        "                r'\\b([A-Z][a-z]+ [A-Z][a-z]+)\\b',\n",
        "            ],\n",
        "            'Action': [\n",
        "                r'\\b(covering up|hiding|concealing|manipulating|controlling)\\b',\n",
        "                r'\\b(is trying to|are attempting to|wants to|plans to)\\b',\n",
        "            ],\n",
        "            'Evidence': [\n",
        "                r'\\b(according to|reports show|evidence suggests|allegedly)\\b',\n",
        "                r'\\b(documents show|proof|records indicate)\\b',\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    def _create_few_shot_examples(self):\n",
        "        \"\"\"Create few-shot examples for LLM\"\"\"\n",
        "        return [\n",
        "            {\n",
        "                \"text\": \"The CIA has been covering up UFO evidence for decades according to leaked documents.\",\n",
        "                \"markers\": [\n",
        "                    {\"type\": \"Actor\", \"text\": \"CIA\", \"start\": 4, \"end\": 7},\n",
        "                    {\"type\": \"Action\", \"text\": \"covering up\", \"start\": 19, \"end\": 30},\n",
        "                    {\"type\": \"Evidence\", \"text\": \"according to leaked documents\", \"start\": 52, \"end\": 81}\n",
        "                ]\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"Government elites are manipulating the media to control public opinion.\",\n",
        "                \"markers\": [\n",
        "                    {\"type\": \"Actor\", \"text\": \"Government elites\", \"start\": 0, \"end\": 17},\n",
        "                    {\"type\": \"Action\", \"text\": \"manipulating\", \"start\": 22, \"end\": 34},\n",
        "                    {\"type\": \"Effect\", \"text\": \"control public opinion\", \"start\": 49, \"end\": 71}\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "\n",
        "    def enhance_with_llm(self, text, deberta_markers):\n",
        "        \"\"\"Enhance DeBERTa predictions with LLM\"\"\"\n",
        "\n",
        "        if not self.use_api or len(text) > 1000:\n",
        "            return self._enhance_with_rules(text, deberta_markers)\n",
        "\n",
        "        try:\n",
        "            # Prepare prompt with few-shot examples\n",
        "            prompt = \"\"\"You are an expert analyzing conspiracy theory markers.\n",
        "\n",
        "Given a text, identify psycholinguistic markers:\n",
        "- Actor: Who is responsible\n",
        "- Action: What they're doing\n",
        "- Effect: Consequences\n",
        "- Victim: Who is affected\n",
        "- Evidence: Supporting claims\n",
        "\n",
        "Examples:\"\"\"\n",
        "\n",
        "            for example in self.few_shot_examples:\n",
        "                prompt += f\"\\n\\nText: {example['text']}\"\n",
        "                prompt += f\"\\nMarkers: {json.dumps(example['markers'])}\"\n",
        "\n",
        "            prompt += f\"\\n\\nNew text: {text}\"\n",
        "            prompt += f\"\\nDeBERTa predictions: {json.dumps(deberta_markers)}\"\n",
        "            prompt += \"\\n\\nReview these predictions. Return a JSON array with corrected markers:\"\n",
        "\n",
        "            # Call API\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.1,\n",
        "                max_tokens=500\n",
        "            )\n",
        "\n",
        "            # Parse response\n",
        "            result = response.choices[0].message.content\n",
        "            json_match = re.search(r'\\[.*\\]', result, re.DOTALL)\n",
        "\n",
        "            if json_match:\n",
        "                llm_markers = json.loads(json_match.group())\n",
        "                return self._merge_markers(deberta_markers, llm_markers, text)\n",
        "            else:\n",
        "                return self._enhance_with_rules(text, deberta_markers)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"LLM enhancement failed: {e}\")\n",
        "            return self._enhance_with_rules(text, deberta_markers)\n",
        "\n",
        "    def _enhance_with_rules(self, text, deberta_markers):\n",
        "        \"\"\"Rule-based enhancement\"\"\"\n",
        "        enhanced_markers = deberta_markers.copy()\n",
        "\n",
        "        for marker_type, patterns in self.rule_patterns.items():\n",
        "            for pattern in patterns:\n",
        "                for match in re.finditer(pattern, text, re.IGNORECASE):\n",
        "                    # Check if similar marker exists\n",
        "                    similar_exists = False\n",
        "                    for existing in enhanced_markers:\n",
        "                        if (existing['type'] == marker_type and\n",
        "                            existing['text'].lower() in match.group().lower()):\n",
        "                            similar_exists = True\n",
        "                            break\n",
        "\n",
        "                    if not similar_exists:\n",
        "                        enhanced_markers.append({\n",
        "                            'type': marker_type,\n",
        "                            'text': match.group(),\n",
        "                            'startIndex': match.start(),\n",
        "                            'endIndex': match.end()\n",
        "                        })\n",
        "\n",
        "        return enhanced_markers[:15]  # Limit markers\n",
        "\n",
        "    def _merge_markers(self, deberta_markers, llm_markers, text):\n",
        "        \"\"\"Merge DeBERTa and LLM predictions\"\"\"\n",
        "        merged = deberta_markers.copy()\n",
        "\n",
        "        for llm_marker in llm_markers:\n",
        "            # Check if similar exists\n",
        "            similar = False\n",
        "            for deberta_marker in deberta_markers:\n",
        "                if self._markers_similar(llm_marker, deberta_marker):\n",
        "                    similar = True\n",
        "                    break\n",
        "\n",
        "            if not similar and 'text' in llm_marker:\n",
        "                # Find exact span in text\n",
        "                marker_text = llm_marker['text']\n",
        "                if marker_text in text:\n",
        "                    start_idx = text.find(marker_text)\n",
        "                    merged.append({\n",
        "                        'type': llm_marker.get('type', 'Unknown'),\n",
        "                        'text': marker_text,\n",
        "                        'startIndex': start_idx,\n",
        "                        'endIndex': start_idx + len(marker_text)\n",
        "                    })\n",
        "\n",
        "        return merged[:15]  # Limit to 15 markers\n",
        "\n",
        "    def _markers_similar(self, m1, m2):\n",
        "        \"\"\"Check if markers are similar\"\"\"\n",
        "        if m1.get('type') != m2.get('type'):\n",
        "            return False\n",
        "\n",
        "        t1 = m1.get('text', '').lower()\n",
        "        t2 = m2.get('text', '').lower()\n",
        "\n",
        "        return t1 in t2 or t2 in t1 or len(set(t1.split()) & set(t2.split())) > 0\n",
        "\n",
        "# Initialize LLM enhancer\n",
        "llm_enhancer = LLMMarkerEnhancer(use_api=False)  # Set to True if you have API key\n",
        "print(\"‚úÖ LLM enhancer initialized\")\n",
        "\n",
        "# ============================================================================\n",
        "# INFERENCE WITH ENHANCEMENT\n",
        "# ============================================================================\n",
        "\n",
        "def predict_with_enhancement(text, tokenizer, model, device, llm_enhancer, min_confidence=0.3):\n",
        "    \"\"\"Predict markers with DeBERTa + LLM enhancement\"\"\"\n",
        "    if not text or len(text) < 20:\n",
        "        return []\n",
        "\n",
        "    # DeBERTa prediction\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding='max_length',\n",
        "        return_offsets_mapping=True,\n",
        "        return_tensors='pt'\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoding)\n",
        "        logits = outputs.logits[0]\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        predictions = torch.argmax(probs, dim=-1).cpu().numpy()\n",
        "        max_probs = np.max(probs.cpu().numpy(), axis=1)\n",
        "\n",
        "    offset_mapping = encoding['offset_mapping'][0].cpu().numpy()\n",
        "    deberta_markers = []\n",
        "    current_marker = None\n",
        "    current_start = None\n",
        "    current_end = None\n",
        "    current_confidences = []\n",
        "\n",
        "    for idx, (pred_id, confidence) in enumerate(zip(predictions, max_probs)):\n",
        "        token_start, token_end = offset_mapping[idx]\n",
        "\n",
        "        if token_start == 0 and token_end == 0:\n",
        "            continue\n",
        "\n",
        "        label = id2label[pred_id]\n",
        "\n",
        "        if label.startswith('B-'):\n",
        "            # Save previous\n",
        "            if current_marker is not None and current_confidences:\n",
        "                avg_conf = np.mean(current_confidences)\n",
        "                if avg_conf >= min_confidence:\n",
        "                    deberta_markers.append({\n",
        "                        'type': current_marker,\n",
        "                        'text': text[current_start:current_end],\n",
        "                        'startIndex': current_start,\n",
        "                        'endIndex': current_end,\n",
        "                        'confidence': avg_conf\n",
        "                    })\n",
        "\n",
        "            # Start new\n",
        "            current_marker = label[2:]\n",
        "            current_start = int(token_start)\n",
        "            current_end = int(token_end)\n",
        "            current_confidences = [confidence]\n",
        "\n",
        "        elif label.startswith('I-') and current_marker is not None:\n",
        "            if label[2:] == current_marker:\n",
        "                current_end = int(token_end)\n",
        "                current_confidences.append(confidence)\n",
        "\n",
        "        elif label == 'O' and current_marker is not None:\n",
        "            # End marker\n",
        "            if current_confidences:\n",
        "                avg_conf = np.mean(current_confidences)\n",
        "                if avg_conf >= min_confidence:\n",
        "                    deberta_markers.append({\n",
        "                        'type': current_marker,\n",
        "                        'text': text[current_start:current_end],\n",
        "                        'startIndex': current_start,\n",
        "                        'endIndex': current_end,\n",
        "                        'confidence': avg_conf\n",
        "                    })\n",
        "            current_marker = None\n",
        "\n",
        "    # Last marker\n",
        "    if current_marker is not None and current_confidences:\n",
        "        avg_conf = np.mean(current_confidences)\n",
        "        if avg_conf >= min_confidence:\n",
        "            deberta_markers.append({\n",
        "                'type': current_marker,\n",
        "                'text': text[current_start:current_end],\n",
        "                'startIndex': current_start,\n",
        "                'endIndex': current_end,\n",
        "                'confidence': avg_conf\n",
        "            })\n",
        "\n",
        "    # Remove confidence for enhancement\n",
        "    deberta_markers_clean = []\n",
        "    for marker in deberta_markers:\n",
        "        deberta_markers_clean.append({\n",
        "            'type': marker['type'],\n",
        "            'text': marker['text'],\n",
        "            'startIndex': marker['startIndex'],\n",
        "            'endIndex': marker['endIndex']\n",
        "        })\n",
        "\n",
        "    # Enhance with LLM\n",
        "    enhanced_markers = llm_enhancer.enhance_with_llm(text, deberta_markers_clean)\n",
        "\n",
        "    return enhanced_markers\n",
        "\n",
        "# ============================================================================\n",
        "# CREATE FINAL SUBMISSION\n",
        "# ============================================================================\n",
        "\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "\n",
        "def create_final_submission(test_file_path, output_zip=\"submission_extraction_final.zip\"):\n",
        "    \"\"\"Create submission with enhanced predictions\"\"\"\n",
        "    print(f\"\\nüì¶ Creating submission from {test_file_path}\")\n",
        "\n",
        "    # Load test data\n",
        "    test_items = []\n",
        "    with open(test_file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                test_items.append(json.loads(line))\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    print(f\"  Loaded {len(test_items)} test items\")\n",
        "\n",
        "    # Setup for inference\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Generate predictions\n",
        "    predictions = []\n",
        "    total_markers = 0\n",
        "\n",
        "    print(\"  Generating enhanced predictions...\")\n",
        "    for item in tqdm(test_items, desc=\"Processing\"):\n",
        "        text = item.get('text', '')\n",
        "        doc_id = item.get('_id', 'unknown')\n",
        "\n",
        "        if not text:\n",
        "            predictions.append({\"_id\": doc_id, \"markers\": []})\n",
        "            continue\n",
        "\n",
        "        # Get enhanced markers\n",
        "        markers = predict_with_enhancement(\n",
        "            text, tokenizer, model, device, llm_enhancer, min_confidence=0.25\n",
        "        )\n",
        "\n",
        "        predictions.append({\n",
        "            \"_id\": doc_id,\n",
        "            \"markers\": markers\n",
        "        })\n",
        "        total_markers += len(markers)\n",
        "\n",
        "    print(f\"\\nüìä Final statistics:\")\n",
        "    print(f\"  Total documents: {len(predictions)}\")\n",
        "    print(f\"  Total markers: {total_markers}\")\n",
        "    print(f\"  Avg markers per doc: {total_markers/len(predictions):.2f}\")\n",
        "\n",
        "    # Save to JSONL\n",
        "    jsonl_file = \"temp_submission.jsonl\"\n",
        "    with open(jsonl_file, 'w', encoding='utf-8') as f:\n",
        "        for pred in predictions:\n",
        "            f.write(json.dumps(pred) + '\\n')\n",
        "\n",
        "    # Create ZIP\n",
        "    with zipfile.ZipFile(output_zip, 'w') as zf:\n",
        "        zf.write(jsonl_file, arcname=\"submission.jsonl\")\n",
        "\n",
        "    os.remove(jsonl_file)\n",
        "\n",
        "    print(f\"\\n‚úÖ Submission created: {output_zip}\")\n",
        "    print(f\"  File size: {os.path.getsize(output_zip)/1024:.1f} KB\")\n",
        "\n",
        "    return output_zip\n",
        "\n",
        "# ============================================================================\n",
        "# RUN PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Create submission\n",
        "    test_file = \"test_rehydrated_v2.jsonl\"\n",
        "\n",
        "    if os.path.exists(test_file):\n",
        "        submission_file = create_final_submission(test_file)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"üéØ READY FOR SUBMISSION!\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"\\nüì§ Submission file: {submission_file}\")\n",
        "        print(f\"\\nüîß Key improvements from 0.18 baseline:\")\n",
        "        print(f\"  1. Learning rate: 5e-6 ‚Üí 3e-5 (6x higher)\")\n",
        "        print(f\"  2. Batch size: 2 ‚Üí 8 (4x larger)\")\n",
        "        print(f\"  3. Better label alignment\")\n",
        "        print(f\"  4. Data augmentation\")\n",
        "        print(f\"  5. LLM/rule-based enhancement\")\n",
        "        print(f\"\\nüìà Expected F1: 0.30-0.40\")\n",
        "        print(f\"üìä Improvement: +67% to +122%\")\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è  Test file not found: {test_file}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-31T21:32:46.206961Z",
          "iopub.execute_input": "2026-01-31T21:32:46.207196Z",
          "iopub.status.idle": "2026-01-31T21:33:08.029298Z",
          "shell.execute_reply.started": "2026-01-31T21:32:46.207175Z",
          "shell.execute_reply": "2026-01-31T21:33:08.028355Z"
        },
        "id": "GWNWIaf9fgeV",
        "outputId": "ca0a20cd-5402-41c0-c40f-2d79870c69be"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Installing compatible versions...\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m492.2/492.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntransformers 5.0.0 requires huggingface-hub<2.0,>=1.3.0, but you have huggingface-hub 0.36.0 which is incompatible.\npathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\npathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\nsentence-transformers 5.1.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 5.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_55/1055851166.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m from transformers import (\n\u001b[1;32m     24\u001b[0m     \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2.14.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReadInstruction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBeamBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBuilderConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorBasedBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_writer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowWriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptimizedTypedSequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata_files\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msanitize_patterns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_writer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m from .features.features import (\n\u001b[1;32m     29\u001b[0m     \u001b[0mFeatureType\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/features/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m ]\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArray2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mArray3D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mArray4D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mArray5D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mClassLabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtranslation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTranslation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTranslationVariableLanguages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/features/features.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0m_ArrayXDExtensionType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyExtensionType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m     \u001b[0mndims\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'pyarrow' has no attribute 'PyExtensionType'"
          ],
          "ename": "AttributeError",
          "evalue": "module 'pyarrow' has no attribute 'PyExtensionType'",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "SUBTASK 1: MARKER EXTRACTION - SUBMISSION GENERATOR\n",
        "Fixed for local model loading\n",
        "\"\"\"\n",
        "import json\n",
        "import os\n",
        "import zipfile\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ===========================\n",
        "# CONFIGURATION\n",
        "# ===========================\n",
        "# Paths - UPDATE THESE!\n",
        "INPUT_FILE = \"/kaggle/input/test-reducted/test_rehydrated.jsonl\"\n",
        "MODEL_PATH = \"./deberta_large_final/checkpoint-best\"  # ‚≠ê Updated to match your training\n",
        "OUTPUT_DIR = \"/kaggle/working\"\n",
        "TEMP_SUBMISSION_FILE = os.path.join(OUTPUT_DIR, \"submission.jsonl\")\n",
        "OUTPUT_ZIP = os.path.join(OUTPUT_DIR, \"submission.zip\")\n",
        "\n",
        "# Marker types\n",
        "MARKER_TYPES = [\"Actor\", \"Action\", \"Effect\", \"Evidence\", \"Victim\"]\n",
        "\n",
        "# Labels\n",
        "labels_list = [\"O\"]\n",
        "for marker_type in MARKER_TYPES:\n",
        "    labels_list.append(f\"B-{marker_type}\")\n",
        "    labels_list.append(f\"I-{marker_type}\")\n",
        "\n",
        "label2id = {label: i for i, label in enumerate(labels_list)}\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"MARKER EXTRACTION - SUBMISSION GENERATOR\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ===========================\n",
        "# LOAD MODEL - FIXED\n",
        "# ===========================\n",
        "def load_model():\n",
        "    \"\"\"Load trained marker extraction model\"\"\"\n",
        "    print(\"\\nüì¶ Loading model and tokenizer...\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"   Device: {device}\")\n",
        "\n",
        "    # Check if model exists\n",
        "    if not os.path.exists(MODEL_PATH):\n",
        "        print(f\"‚ùå ERROR: Model not found at {MODEL_PATH}\")\n",
        "        print(f\"\\nüìÅ Available directories:\")\n",
        "        for d in os.listdir(\".\"):\n",
        "            if os.path.isdir(d):\n",
        "                print(f\"   - {d}/\")\n",
        "                checkpoint_path = os.path.join(d, \"checkpoint-best\")\n",
        "                if os.path.exists(checkpoint_path):\n",
        "                    print(f\"     ‚úÖ Has checkpoint-best\")\n",
        "        raise FileNotFoundError(f\"Model not found: {MODEL_PATH}\")\n",
        "\n",
        "    print(f\"‚úÖ Found model at: {MODEL_PATH}\")\n",
        "\n",
        "    # ‚≠ê FIX: Load tokenizer with local_files_only=True\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            MODEL_PATH,\n",
        "            local_files_only=True,\n",
        "            use_fast=True\n",
        "        )\n",
        "        print(f\"‚úÖ Tokenizer loaded from {MODEL_PATH}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Tokenizer not found in model path, loading from base model\")\n",
        "        # Fallback: Load from base model (DeBERTa-v3-large)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
        "        print(f\"‚úÖ Tokenizer loaded from base model\")\n",
        "\n",
        "    # ‚≠ê FIX: Load model with local_files_only=True\n",
        "    model = AutoModelForTokenClassification.from_pretrained(\n",
        "        MODEL_PATH,\n",
        "        local_files_only=True,\n",
        "        num_labels=len(labels_list),\n",
        "        id2label=id2label,\n",
        "        label2id=label2id\n",
        "    )\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"‚úÖ Model loaded successfully!\")\n",
        "    print(f\"   Parameters: {model.num_parameters():,}\")\n",
        "\n",
        "    return tokenizer, model, device\n",
        "\n",
        "# ===========================\n",
        "# PREDICTION FUNCTION\n",
        "# ===========================\n",
        "def predict_markers(text, tokenizer, model, device, confidence_threshold=0.20):\n",
        "    \"\"\"\n",
        "    Predict markers for a single text\n",
        "\n",
        "    Returns:\n",
        "        List of marker dictionaries with startIndex, endIndex, type, text\n",
        "    \"\"\"\n",
        "    if not text or len(text.strip()) < 10:\n",
        "        return []\n",
        "\n",
        "    # Tokenize with offset mapping\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding='max_length',\n",
        "        return_offsets_mapping=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    offset_mapping = encoding['offset_mapping'][0].numpy()\n",
        "\n",
        "    # Move to device\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    # Predict with confidence scores\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits[0]\n",
        "\n",
        "        # Get predictions and confidence\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        predictions = torch.argmax(probs, dim=-1).cpu().numpy()\n",
        "        confidences = torch.max(probs, dim=-1).values.cpu().numpy()\n",
        "\n",
        "    # Extract marker spans\n",
        "    markers = []\n",
        "    current_marker = None\n",
        "    current_start = None\n",
        "    current_end = None\n",
        "    current_confidences = []\n",
        "\n",
        "    for idx, (pred_id, conf) in enumerate(zip(predictions, confidences)):\n",
        "        token_start, token_end = offset_mapping[idx]\n",
        "\n",
        "        # Skip special tokens\n",
        "        if token_start == 0 and token_end == 0:\n",
        "            continue\n",
        "\n",
        "        label = id2label[pred_id]\n",
        "\n",
        "        if label.startswith('B-'):\n",
        "            # Save previous marker if exists\n",
        "            if current_marker is not None and len(current_confidences) > 0:\n",
        "                avg_conf = np.mean(current_confidences)\n",
        "                if avg_conf >= confidence_threshold:\n",
        "                    markers.append({\n",
        "                        'startIndex': int(current_start),\n",
        "                        'endIndex': int(current_end),\n",
        "                        'type': current_marker,\n",
        "                        'text': text[current_start:current_end]\n",
        "                    })\n",
        "\n",
        "            # Start new marker\n",
        "            marker_type = label[2:]\n",
        "            current_marker = marker_type\n",
        "            current_start = int(token_start)\n",
        "            current_end = int(token_end)\n",
        "            current_confidences = [conf]\n",
        "\n",
        "        elif label.startswith('I-') and current_marker is not None:\n",
        "            # Continue current marker\n",
        "            marker_type = label[2:]\n",
        "            if marker_type == current_marker:\n",
        "                current_end = int(token_end)\n",
        "                current_confidences.append(conf)\n",
        "            else:\n",
        "                # Type mismatch, save current and start new\n",
        "                if len(current_confidences) > 0:\n",
        "                    avg_conf = np.mean(current_confidences)\n",
        "                    if avg_conf >= confidence_threshold:\n",
        "                        markers.append({\n",
        "                            'startIndex': int(current_start),\n",
        "                            'endIndex': int(current_end),\n",
        "                            'type': current_marker,\n",
        "                            'text': text[current_start:current_end]\n",
        "                        })\n",
        "\n",
        "                current_marker = marker_type\n",
        "                current_start = int(token_start)\n",
        "                current_end = int(token_end)\n",
        "                current_confidences = [conf]\n",
        "\n",
        "        elif label == 'O':\n",
        "            # End current marker\n",
        "            if current_marker is not None and len(current_confidences) > 0:\n",
        "                avg_conf = np.mean(current_confidences)\n",
        "                if avg_conf >= confidence_threshold:\n",
        "                    markers.append({\n",
        "                        'startIndex': int(current_start),\n",
        "                        'endIndex': int(current_end),\n",
        "                        'type': current_marker,\n",
        "                        'text': text[current_start:current_end]\n",
        "                    })\n",
        "            current_marker = None\n",
        "            current_confidences = []\n",
        "\n",
        "    # Don't forget last marker\n",
        "    if current_marker is not None and len(current_confidences) > 0:\n",
        "        avg_conf = np.mean(current_confidences)\n",
        "        if avg_conf >= confidence_threshold:\n",
        "            markers.append({\n",
        "                'startIndex': int(current_start),\n",
        "                'endIndex': int(current_end),\n",
        "                'type': current_marker,\n",
        "                'text': text[current_start:current_end]\n",
        "            })\n",
        "\n",
        "    return markers\n",
        "\n",
        "# ===========================\n",
        "# PROCESS DOCUMENTS\n",
        "# ===========================\n",
        "def process_document(item, tokenizer, model, device):\n",
        "    \"\"\"Process a single document\"\"\"\n",
        "    doc_id = item.get('_id')\n",
        "    text = item.get('text', '')\n",
        "\n",
        "    if not doc_id or not text:\n",
        "        return None\n",
        "\n",
        "    # Predict markers\n",
        "    markers = predict_markers(text, tokenizer, model, device)\n",
        "\n",
        "    # Return in submission format\n",
        "    return {\n",
        "        \"_id\": doc_id,\n",
        "        \"markers\": markers\n",
        "    }\n",
        "\n",
        "# ===========================\n",
        "# MAIN EXECUTION\n",
        "# ===========================\n",
        "def main():\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STARTING PREDICTION PIPELINE\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Load model\n",
        "    tokenizer, model, device = load_model()\n",
        "\n",
        "    # Load input data\n",
        "    print(f\"\\nüìÇ Loading test data from: {INPUT_FILE}\")\n",
        "\n",
        "    if not os.path.exists(INPUT_FILE):\n",
        "        print(f\"‚ùå Error: File not found: {INPUT_FILE}\")\n",
        "        return\n",
        "\n",
        "    input_data = []\n",
        "    with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                input_data.append(json.loads(line))\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "\n",
        "    print(f\"‚úÖ Loaded {len(input_data)} documents\")\n",
        "\n",
        "    # Process all documents\n",
        "    print(f\"\\nüîÑ Generating predictions...\")\n",
        "    predictions = []\n",
        "\n",
        "    for item in tqdm(input_data, desc=\"Processing\"):\n",
        "        pred = process_document(item, tokenizer, model, device)\n",
        "        if pred:\n",
        "            predictions.append(pred)\n",
        "\n",
        "    print(f\"\\n‚úÖ Generated predictions for {len(predictions)} documents\")\n",
        "\n",
        "    # Calculate statistics\n",
        "    total_markers = sum(len(pred['markers']) for pred in predictions)\n",
        "    docs_with_markers = sum(1 for pred in predictions if len(pred['markers']) > 0)\n",
        "\n",
        "    print(f\"\\nüìä PREDICTION STATISTICS:\")\n",
        "    print(f\"   Total markers predicted: {total_markers}\")\n",
        "    print(f\"   Documents with markers: {docs_with_markers} ({docs_with_markers/len(predictions)*100:.1f}%)\")\n",
        "    print(f\"   Avg markers per document: {total_markers / len(predictions):.2f}\")\n",
        "\n",
        "    # Marker type distribution\n",
        "    marker_counts = {mt: 0 for mt in MARKER_TYPES}\n",
        "    for pred in predictions:\n",
        "        for marker in pred['markers']:\n",
        "            marker_type = marker['type']\n",
        "            if marker_type in marker_counts:\n",
        "                marker_counts[marker_type] += 1\n",
        "\n",
        "    if total_markers > 0:\n",
        "        print(f\"\\nüìä Marker Type Distribution:\")\n",
        "        for marker_type, count in sorted(marker_counts.items(), key=lambda x: -x[1]):\n",
        "            percentage = (count / total_markers * 100) if total_markers > 0 else 0\n",
        "            print(f\"   {marker_type:10s}: {count:5d} ({percentage:5.2f}%)\")\n",
        "\n",
        "    # Save to JSONL\n",
        "    print(f\"\\nüíæ Saving predictions to {TEMP_SUBMISSION_FILE}...\")\n",
        "    with open(TEMP_SUBMISSION_FILE, 'w', encoding='utf-8') as f:\n",
        "        for pred in predictions:\n",
        "            f.write(json.dumps(pred) + '\\n')\n",
        "\n",
        "    # Create ZIP\n",
        "    print(f\"üì¶ Creating ZIP archive: {OUTPUT_ZIP}...\")\n",
        "    with zipfile.ZipFile(OUTPUT_ZIP, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
        "        zf.write(TEMP_SUBMISSION_FILE, arcname=\"submission.jsonl\")\n",
        "\n",
        "    # Clean up\n",
        "    os.remove(TEMP_SUBMISSION_FILE)\n",
        "\n",
        "    file_size = os.path.getsize(OUTPUT_ZIP) / 1024\n",
        "\n",
        "    print(f\"\\n‚úÖ SUCCESS!\")\n",
        "    print(f\"   Submission file: {OUTPUT_ZIP}\")\n",
        "    print(f\"   File size: {file_size:.2f} KB\")\n",
        "\n",
        "    # Show sample predictions\n",
        "    print(f\"\\nüìã SAMPLE PREDICTIONS (first 3 documents):\")\n",
        "    for i, pred in enumerate(predictions[:3]):\n",
        "        print(f\"\\nDocument {i+1} (ID: {pred['_id']}):\")\n",
        "        if pred['markers']:\n",
        "            for j, marker in enumerate(pred['markers'][:5], 1):\n",
        "                text_preview = marker['text'][:50] + \"...\" if len(marker['text']) > 50 else marker['text']\n",
        "                print(f\"   {j}. [{marker['type']:8s}] \\\"{text_preview}\\\"\")\n",
        "            if len(pred['markers']) > 5:\n",
        "                print(f\"   ... and {len(pred['markers']) - 5} more\")\n",
        "        else:\n",
        "            print(\"   (No markers predicted)\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"SUBMISSION READY FOR UPLOAD!\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nüì§ Next steps:\")\n",
        "    print(f\"1. Download {OUTPUT_ZIP}\")\n",
        "    print(f\"2. Go to Codabench ‚Üí Extraction task\")\n",
        "    print(f\"3. Upload to 'My Submissions'\")\n",
        "    print(f\"4. Wait for evaluation\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        main()\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå ERROR: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T13:23:43.001835Z",
          "iopub.status.idle": "2026-01-26T13:23:43.002082Z",
          "shell.execute_reply.started": "2026-01-26T13:23:43.001966Z",
          "shell.execute_reply": "2026-01-26T13:23:43.00198Z"
        },
        "id": "oWfuI_V6fgeW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# \"\"\"\n",
        "# SUBTASK 2: CONSPIRACY DETECTION - FIXED\n",
        "# Binary classification (Yes/No only)\n",
        "# Target: 0.70+ F1\n",
        "# \"\"\"\n",
        "\n",
        "# import json\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# import os\n",
        "# from datasets import Dataset\n",
        "# from transformers import (\n",
        "#     AutoTokenizer,\n",
        "#     AutoModelForSequenceClassification,\n",
        "#     Trainer,\n",
        "#     TrainingArguments,\n",
        "#     EarlyStoppingCallback\n",
        "# )\n",
        "# from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
        "# from sklearn.utils.class_weight import compute_class_weight\n",
        "# from collections import Counter\n",
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')\n",
        "\n",
        "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# print(\"=\"*70)\n",
        "# print(\"SUBTASK 2: CONSPIRACY DETECTION\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# # ===========================\n",
        "# # CONFIGURATION\n",
        "# # ===========================\n",
        "# label2id = {\"No\": 0, \"Yes\": 1}\n",
        "# id2label = {0: \"No\", 1: \"Yes\"}\n",
        "\n",
        "# MARKER_TYPES = [\"Actor\", \"Action\", \"Effect\", \"Evidence\", \"Victim\"]\n",
        "\n",
        "# # ===========================\n",
        "# # MARKER-AWARE TEXT\n",
        "# # ===========================\n",
        "# def add_marker_tags(text, markers):\n",
        "#     \"\"\"Add special tokens around markers\"\"\"\n",
        "#     if not markers:\n",
        "#         return text\n",
        "\n",
        "#     sorted_markers = sorted(markers, key=lambda x: x['startIndex'], reverse=True)\n",
        "\n",
        "#     for marker in sorted_markers:\n",
        "#         start = marker['startIndex']\n",
        "#         end = marker['endIndex']\n",
        "#         mtype = marker['type']\n",
        "\n",
        "#         text = text[:end] + f\"[/{mtype.upper()}]\" + text[end:]\n",
        "#         text = text[:start] + f\"[{mtype.upper()}]\" + text[start:]\n",
        "\n",
        "#     return text\n",
        "\n",
        "# # ===========================\n",
        "# # LOAD DATASET - FIXED\n",
        "# # ===========================\n",
        "# def load_detection_dataset(jsonl_path, tokenizer, max_length=512, use_markers=True):\n",
        "#     \"\"\"Load data for conspiracy detection\"\"\"\n",
        "#     texts = []\n",
        "#     labels = []\n",
        "\n",
        "#     stats = {'total': 0, 'yes': 0, 'no': 0, 'cant_tell': 0, 'skipped': 0}\n",
        "\n",
        "#     print(f\"\\nLoading {jsonl_path}...\")\n",
        "\n",
        "#     with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
        "#         for line in f:\n",
        "#             stats['total'] += 1\n",
        "\n",
        "#             try:\n",
        "#                 item = json.loads(line)\n",
        "#                 text = item.get('text', '').strip()\n",
        "#                 conspiracy = item.get('conspiracy')\n",
        "#                 markers = item.get('markers', [])\n",
        "\n",
        "#                 if len(text) < 20:\n",
        "#                     stats['skipped'] += 1\n",
        "#                     continue\n",
        "\n",
        "#                 # Skip \"Can't tell\"\n",
        "#                 if conspiracy == \"Can't tell\":\n",
        "#                     stats['cant_tell'] += 1\n",
        "#                     continue\n",
        "\n",
        "#                 if conspiracy not in label2id:\n",
        "#                     stats['skipped'] += 1\n",
        "#                     continue\n",
        "\n",
        "#                 # Add marker tags\n",
        "#                 if use_markers and markers:\n",
        "#                     text = add_marker_tags(text, markers)\n",
        "\n",
        "#                 texts.append(text)\n",
        "#                 labels.append(label2id[conspiracy])\n",
        "\n",
        "#                 if conspiracy == \"Yes\":\n",
        "#                     stats['yes'] += 1\n",
        "#                 else:\n",
        "#                     stats['no'] += 1\n",
        "\n",
        "#             except:\n",
        "#                 stats['skipped'] += 1\n",
        "#                 continue\n",
        "\n",
        "#     # ‚≠ê FIXED: Handle empty case\n",
        "#     if len(texts) == 0:\n",
        "#         print(f\"‚ö†Ô∏è  No valid examples found!\")\n",
        "#         print(f\"   Total: {stats['total']}\")\n",
        "#         print(f\"   Can't tell: {stats['cant_tell']}\")\n",
        "#         print(f\"   Skipped: {stats['skipped']}\")\n",
        "#         return None\n",
        "\n",
        "#     print(f\"‚úÖ Loaded {len(texts)} examples\")\n",
        "#     print(f\"   Yes: {stats['yes']} ({stats['yes']/len(texts)*100:.1f}%)\")\n",
        "#     print(f\"   No: {stats['no']} ({stats['no']/len(texts)*100:.1f}%)\")\n",
        "#     print(f\"   Skipped 'Can't tell': {stats['cant_tell']}\")\n",
        "#     print(f\"   Skipped other: {stats['skipped']}\")\n",
        "\n",
        "#     # Tokenize\n",
        "#     encodings = tokenizer(\n",
        "#         texts,\n",
        "#         truncation=True,\n",
        "#         max_length=max_length,\n",
        "#         padding='max_length'\n",
        "#     )\n",
        "\n",
        "#     return Dataset.from_dict({\n",
        "#         'input_ids': encodings['input_ids'],\n",
        "#         'attention_mask': encodings['attention_mask'],\n",
        "#         'labels': labels\n",
        "#     })\n",
        "\n",
        "# # ===========================\n",
        "# # LOAD DATA\n",
        "# # ===========================\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"LOADING DATA\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# model_name = \"microsoft/deberta-v3-base\"  # Use base for faster training\n",
        "# print(f\"Model: {model_name}\")\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# # Add marker tokens\n",
        "# special_tokens = []\n",
        "# for mtype in MARKER_TYPES:\n",
        "#     special_tokens.extend([f\"[{mtype.upper()}]\", f\"[/{mtype.upper()}]\"])\n",
        "\n",
        "# num_added = tokenizer.add_tokens(special_tokens)\n",
        "# print(f\"Added {num_added} special marker tokens\")\n",
        "\n",
        "# # Load train\n",
        "# train_ds = load_detection_dataset(\n",
        "#     \"train_rehydrated_v2.jsonl\",\n",
        "#     tokenizer,\n",
        "#     max_length=512,\n",
        "#     use_markers=True\n",
        "# )\n",
        "\n",
        "# if train_ds is None:\n",
        "#     print(\"‚ùå Failed to load training data!\")\n",
        "#     exit(1)\n",
        "\n",
        "# # Load validation\n",
        "# val_ds = load_detection_dataset(\n",
        "#     \"dev_rehydrated_v2.jsonl\",\n",
        "#     tokenizer,\n",
        "#     max_length=512,\n",
        "#     use_markers=True\n",
        "# )\n",
        "\n",
        "# # ‚≠ê FIXED: Always split from training if validation is empty or too small\n",
        "# if val_ds is None or len(val_ds) < 50:\n",
        "#     print(\"\\n‚ö†Ô∏è  Validation empty/small, splitting from training (15%)\")\n",
        "#     split = train_ds.train_test_split(test_size=0.15, seed=42)\n",
        "#     train_ds = split['train']\n",
        "#     val_ds = split['test']\n",
        "#     print(f\"‚úÖ Split: {len(train_ds)} train, {len(val_ds)} val\")\n",
        "\n",
        "# train_ds.set_format(\"torch\")\n",
        "# val_ds.set_format(\"torch\")\n",
        "\n",
        "# # ===========================\n",
        "# # CLASS WEIGHTS\n",
        "# # ===========================\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"CLASS BALANCING\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# train_labels = np.array(train_ds['labels'])\n",
        "# class_weights = compute_class_weight(\n",
        "#     class_weight='balanced',\n",
        "#     classes=np.unique(train_labels),\n",
        "#     y=train_labels\n",
        "# )\n",
        "\n",
        "# class_weights = class_weights ** 1.5  # Boost minority class\n",
        "# class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
        "\n",
        "# print(f\"Class weights: {class_weights}\")\n",
        "# print(f\"\\nClass distribution:\")\n",
        "# unique, counts = np.unique(train_labels, return_counts=True)\n",
        "# for label_id, count in zip(unique, counts):\n",
        "#     pct = count / len(train_labels) * 100\n",
        "#     print(f\"   {id2label[label_id]}: {count} ({pct:.1f}%)\")\n",
        "\n",
        "# # ===========================\n",
        "# # MODEL\n",
        "# # ===========================\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"INITIALIZING MODEL\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(\n",
        "#     model_name,\n",
        "#     num_labels=2,\n",
        "#     id2label=id2label,\n",
        "#     label2id=label2id,\n",
        "#     ignore_mismatched_sizes=True\n",
        "# )\n",
        "\n",
        "# model.resize_token_embeddings(len(tokenizer))\n",
        "# print(f\"‚úÖ Model initialized\")\n",
        "\n",
        "# # ===========================\n",
        "# # WEIGHTED TRAINER\n",
        "# # ===========================\n",
        "# class WeightedTrainer(Trainer):\n",
        "#     def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "#         labels = inputs.pop(\"labels\")\n",
        "#         outputs = model(**inputs)\n",
        "#         logits = outputs.logits\n",
        "\n",
        "#         loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights.to(logits.device))\n",
        "#         loss = loss_fn(logits, labels)\n",
        "\n",
        "#         return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# # ===========================\n",
        "# # METRICS\n",
        "# # ===========================\n",
        "# def compute_metrics(eval_pred):\n",
        "#     logits, labels = eval_pred\n",
        "#     preds = np.argmax(logits, axis=1)\n",
        "\n",
        "#     acc = accuracy_score(labels, preds)\n",
        "#     macro_f1 = f1_score(labels, preds, average='macro')\n",
        "#     binary_f1 = f1_score(labels, preds, average='binary', pos_label=1, zero_division=0)\n",
        "\n",
        "#     from sklearn.metrics import precision_score, recall_score\n",
        "#     precision = precision_score(labels, preds, pos_label=1, zero_division=0)\n",
        "#     recall = recall_score(labels, preds, pos_label=1, zero_division=0)\n",
        "\n",
        "#     return {\n",
        "#         \"accuracy\": acc,\n",
        "#         \"macro_f1\": macro_f1,\n",
        "#         \"binary_f1\": binary_f1,\n",
        "#         \"precision\": precision,\n",
        "#         \"recall\": recall\n",
        "#     }\n",
        "\n",
        "# # ===========================\n",
        "# # TRAINING ARGS\n",
        "# # ===========================\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"./detection_model\",\n",
        "#     eval_strategy=\"epoch\",\n",
        "#     save_strategy=\"epoch\",\n",
        "#     learning_rate=2e-5,\n",
        "#     per_device_train_batch_size=8,\n",
        "#     per_device_eval_batch_size=16,\n",
        "#     num_train_epochs=5,\n",
        "#     weight_decay=0.01,\n",
        "#     warmup_ratio=0.1,\n",
        "#     load_best_model_at_end=True,\n",
        "#     metric_for_best_model=\"macro_f1\",\n",
        "#     greater_is_better=True,\n",
        "#     logging_steps=50,\n",
        "#     report_to=\"none\",\n",
        "#     save_total_limit=2,\n",
        "#     fp16=torch.cuda.is_available(),\n",
        "#     gradient_accumulation_steps=2,\n",
        "#     dataloader_num_workers=0,\n",
        "#     lr_scheduler_type=\"cosine\",\n",
        "#     seed=42,\n",
        "#     label_smoothing_factor=0.1\n",
        "# )\n",
        "\n",
        "# # ===========================\n",
        "# # TRAIN\n",
        "# # ===========================\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"STARTING TRAINING\")\n",
        "# print(\"=\"*70)\n",
        "# print(\"üéØ Target: 0.70+ Macro F1\")\n",
        "# print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# trainer = WeightedTrainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=train_ds,\n",
        "#     eval_dataset=val_ds,\n",
        "#     tokenizer=tokenizer,\n",
        "#     compute_metrics=compute_metrics,\n",
        "#     callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        "# )\n",
        "\n",
        "# trainer.train()\n",
        "\n",
        "# # ===========================\n",
        "# # SAVE\n",
        "# # ===========================\n",
        "# final_path = \"./detection_model/best\"\n",
        "# trainer.save_model(final_path)\n",
        "# tokenizer.save_pretrained(final_path)\n",
        "\n",
        "# print(f\"\\n‚úÖ Saved to: {final_path}\")\n",
        "\n",
        "# # ===========================\n",
        "# # EVALUATION\n",
        "# # ===========================\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"FINAL EVALUATION\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# results = trainer.evaluate()\n",
        "\n",
        "# print(f\"\\nüìä RESULTS:\")\n",
        "# print(f\"   Accuracy:  {results['eval_accuracy']:.4f}\")\n",
        "# print(f\"   Macro F1:  {results['eval_macro_f1']:.4f}\")\n",
        "# print(f\"   Binary F1: {results['eval_binary_f1']:.4f}\")\n",
        "# print(f\"   Precision: {results['eval_precision']:.4f}\")\n",
        "# print(f\"   Recall:    {results['eval_recall']:.4f}\")\n",
        "\n",
        "# # Detailed report\n",
        "# predictions = trainer.predict(val_ds)\n",
        "# pred_labels = np.argmax(predictions.predictions, axis=1)\n",
        "# true_labels = predictions.label_ids\n",
        "\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"CLASSIFICATION REPORT\")\n",
        "# print(\"=\"*70)\n",
        "# print(classification_report(\n",
        "#     true_labels,\n",
        "#     pred_labels,\n",
        "#     target_names=[\"No\", \"Yes\"],\n",
        "#     digits=4\n",
        "# ))\n",
        "\n",
        "# # Prediction balance\n",
        "# yes_pct = np.sum(pred_labels == 1) / len(pred_labels) * 100\n",
        "# print(f\"\\nPrediction distribution:\")\n",
        "# print(f\"   No:  {np.sum(pred_labels == 0)} ({100-yes_pct:.1f}%)\")\n",
        "# print(f\"   Yes: {np.sum(pred_labels == 1)} ({yes_pct:.1f}%)\")\n",
        "\n",
        "# if results['eval_macro_f1'] >= 0.70:\n",
        "#     print(f\"\\nüéâ EXCELLENT! F1 {results['eval_macro_f1']:.4f} > 0.70\")\n",
        "# elif results['eval_macro_f1'] >= 0.60:\n",
        "#     print(f\"\\n‚úÖ Good! F1 {results['eval_macro_f1']:.4f} > 0.60\")\n",
        "# else:\n",
        "#     print(f\"\\n‚ö†Ô∏è  F1 {results['eval_macro_f1']:.4f} - needs improvement\")\n",
        "\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"DETECTION TRAINING COMPLETE!\")\n",
        "# print(\"=\"*70)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T13:23:43.003806Z",
          "iopub.status.idle": "2026-01-26T13:23:43.004102Z",
          "shell.execute_reply.started": "2026-01-26T13:23:43.003977Z",
          "shell.execute_reply": "2026-01-26T13:23:43.003995Z"
        },
        "id": "LVyssIVnfgeW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# \"\"\"\n",
        "# SUBTASK 2: CONSPIRACY DETECTION - SUBMISSION GENERATOR\n",
        "# Uses predicted markers from Subtask 1 for better accuracy\n",
        "# \"\"\"\n",
        "# import json\n",
        "# import os\n",
        "# import zipfile\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# from transformers import (\n",
        "#     AutoTokenizer,\n",
        "#     AutoModelForSequenceClassification,\n",
        "#     AutoModelForTokenClassification\n",
        "# )\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# # ===========================\n",
        "# # CONFIGURATION\n",
        "# # ===========================\n",
        "# # Paths - UPDATE THESE!\n",
        "# INPUT_FILE = \"test_rehydrated_v2.jsonl\"\n",
        "# MARKER_MODEL_PATH = \"./deberta_large_final/checkpoint-best\"  # Marker extraction model\n",
        "# DETECTION_MODEL_PATH = \"./detection_model/best\"  # Detection model\n",
        "# OUTPUT_DIR = \"/kaggle/working\"\n",
        "# TEMP_SUBMISSION_FILE = os.path.join(OUTPUT_DIR, \"submission.jsonl\")\n",
        "# OUTPUT_ZIP = os.path.join(OUTPUT_DIR, \"submission_detection.zip\")\n",
        "\n",
        "# # Labels\n",
        "# label2id = {\"No\": 0, \"Yes\": 1}\n",
        "# id2label = {0: \"No\", 1: \"Yes\"}\n",
        "\n",
        "# MARKER_TYPES = [\"Actor\", \"Action\", \"Effect\", \"Evidence\", \"Victim\"]\n",
        "\n",
        "# print(\"=\"*70)\n",
        "# print(\"CONSPIRACY DETECTION - MARKER-AWARE SUBMISSION\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# # ===========================\n",
        "# # LOAD MARKER EXTRACTION MODEL\n",
        "# # ===========================\n",
        "# def load_marker_model():\n",
        "#     \"\"\"Load marker extraction model to predict markers\"\"\"\n",
        "#     print(\"\\nüì¶ Loading marker extraction model...\")\n",
        "\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#     # Marker labels\n",
        "#     marker_labels = [\"O\"]\n",
        "#     for mt in MARKER_TYPES:\n",
        "#         marker_labels.extend([f\"B-{mt}\", f\"I-{mt}\"])\n",
        "\n",
        "#     marker_label2id = {label: i for i, label in enumerate(marker_labels)}\n",
        "#     marker_id2label = {i: label for label, i in marker_label2id.items()}\n",
        "\n",
        "#     # Load tokenizer\n",
        "#     marker_tokenizer = AutoTokenizer.from_pretrained(\n",
        "#         MARKER_MODEL_PATH,\n",
        "#         local_files_only=True\n",
        "#     )\n",
        "\n",
        "#     # Load model\n",
        "#     marker_model = AutoModelForTokenClassification.from_pretrained(\n",
        "#         MARKER_MODEL_PATH,\n",
        "#         local_files_only=True,\n",
        "#         num_labels=len(marker_labels),\n",
        "#         id2label=marker_id2label,\n",
        "#         label2id=marker_label2id\n",
        "#     )\n",
        "\n",
        "#     marker_model.to(device)\n",
        "#     marker_model.eval()\n",
        "\n",
        "#     print(\"‚úÖ Marker model loaded\")\n",
        "\n",
        "#     return marker_tokenizer, marker_model, marker_id2label, device\n",
        "\n",
        "# # ===========================\n",
        "# # LOAD DETECTION MODEL\n",
        "# # ===========================\n",
        "# def load_detection_model():\n",
        "#     \"\"\"Load conspiracy detection model\"\"\"\n",
        "#     print(\"\\nüì¶ Loading detection model...\")\n",
        "\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#     # Load tokenizer\n",
        "#     detection_tokenizer = AutoTokenizer.from_pretrained(\n",
        "#         DETECTION_MODEL_PATH,\n",
        "#         local_files_only=True\n",
        "#     )\n",
        "\n",
        "#     # Load model\n",
        "#     detection_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "#         DETECTION_MODEL_PATH,\n",
        "#         local_files_only=True,\n",
        "#         num_labels=2,\n",
        "#         id2label=id2label,\n",
        "#         label2id=label2id\n",
        "#     )\n",
        "\n",
        "#     detection_model.to(device)\n",
        "#     detection_model.eval()\n",
        "\n",
        "#     print(\"‚úÖ Detection model loaded\")\n",
        "\n",
        "#     return detection_tokenizer, detection_model, device\n",
        "\n",
        "# # ===========================\n",
        "# # PREDICT MARKERS\n",
        "# # ===========================\n",
        "# def predict_markers(text, tokenizer, model, id2label, device, confidence_threshold=0.15):\n",
        "#     \"\"\"Predict markers for text (same as Subtask 1)\"\"\"\n",
        "#     if not text or len(text.strip()) < 10:\n",
        "#         return []\n",
        "\n",
        "#     encoding = tokenizer(\n",
        "#         text,\n",
        "#         truncation=True,\n",
        "#         max_length=512,\n",
        "#         padding='max_length',\n",
        "#         return_offsets_mapping=True,\n",
        "#         return_tensors='pt'\n",
        "#     )\n",
        "\n",
        "#     offset_mapping = encoding['offset_mapping'][0].numpy()\n",
        "#     input_ids = encoding['input_ids'].to(device)\n",
        "#     attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "#         logits = outputs.logits[0]\n",
        "#         probs = torch.softmax(logits, dim=-1)\n",
        "#         predictions = torch.argmax(probs, dim=-1).cpu().numpy()\n",
        "#         confidences = torch.max(probs, dim=-1).values.cpu().numpy()\n",
        "\n",
        "#     # Extract markers\n",
        "#     markers = []\n",
        "#     current_marker = None\n",
        "#     current_start = None\n",
        "#     current_end = None\n",
        "#     current_confidences = []\n",
        "\n",
        "#     for idx, (pred_id, conf) in enumerate(zip(predictions, confidences)):\n",
        "#         token_start, token_end = offset_mapping[idx]\n",
        "\n",
        "#         if token_start == 0 and token_end == 0:\n",
        "#             continue\n",
        "\n",
        "#         label = id2label[pred_id]\n",
        "\n",
        "#         if label.startswith('B-'):\n",
        "#             if current_marker is not None and len(current_confidences) > 0:\n",
        "#                 avg_conf = np.mean(current_confidences)\n",
        "#                 if avg_conf >= confidence_threshold:\n",
        "#                     markers.append({\n",
        "#                         'startIndex': int(current_start),\n",
        "#                         'endIndex': int(current_end),\n",
        "#                         'type': current_marker\n",
        "#                     })\n",
        "\n",
        "#             marker_type = label[2:]\n",
        "#             current_marker = marker_type\n",
        "#             current_start = int(token_start)\n",
        "#             current_end = int(token_end)\n",
        "#             current_confidences = [conf]\n",
        "\n",
        "#         elif label.startswith('I-') and current_marker is not None:\n",
        "#             marker_type = label[2:]\n",
        "#             if marker_type == current_marker:\n",
        "#                 current_end = int(token_end)\n",
        "#                 current_confidences.append(conf)\n",
        "\n",
        "#         elif label == 'O':\n",
        "#             if current_marker is not None and len(current_confidences) > 0:\n",
        "#                 avg_conf = np.mean(current_confidences)\n",
        "#                 if avg_conf >= confidence_threshold:\n",
        "#                     markers.append({\n",
        "#                         'startIndex': int(current_start),\n",
        "#                         'endIndex': int(current_end),\n",
        "#                         'type': current_marker\n",
        "#                     })\n",
        "#             current_marker = None\n",
        "#             current_confidences = []\n",
        "\n",
        "#     # Last marker\n",
        "#     if current_marker is not None and len(current_confidences) > 0:\n",
        "#         avg_conf = np.mean(current_confidences)\n",
        "#         if avg_conf >= confidence_threshold:\n",
        "#             markers.append({\n",
        "#                 'startIndex': int(current_start),\n",
        "#                 'endIndex': int(current_end),\n",
        "#                 'type': current_marker\n",
        "#             })\n",
        "\n",
        "#     return markers\n",
        "\n",
        "# # ===========================\n",
        "# # ADD MARKER TAGS TO TEXT\n",
        "# # ===========================\n",
        "# def add_marker_tags(text, markers):\n",
        "#     \"\"\"Add special tokens around markers for marker-aware detection\"\"\"\n",
        "#     if not markers:\n",
        "#         return text\n",
        "\n",
        "#     # Sort markers by position (reverse for correct insertion)\n",
        "#     sorted_markers = sorted(markers, key=lambda x: x['startIndex'], reverse=True)\n",
        "\n",
        "#     for marker in sorted_markers:\n",
        "#         start = marker['startIndex']\n",
        "#         end = marker['endIndex']\n",
        "#         mtype = marker['type']\n",
        "\n",
        "#         # Insert closing tag\n",
        "#         text = text[:end] + f\"[/{mtype.upper()}]\" + text[end:]\n",
        "#         # Insert opening tag\n",
        "#         text = text[:start] + f\"[{mtype.upper()}]\" + text[start:]\n",
        "\n",
        "#     return text\n",
        "\n",
        "# # ===========================\n",
        "# # PREDICT CONSPIRACY\n",
        "# # ===========================\n",
        "# def predict_conspiracy(text, markers, tokenizer, model, device):\n",
        "#     \"\"\"Predict conspiracy using marker-aware text\"\"\"\n",
        "\n",
        "#     # Add marker tags to text\n",
        "#     marked_text = add_marker_tags(text, markers)\n",
        "\n",
        "#     # Tokenize\n",
        "#     encoding = tokenizer(\n",
        "#         marked_text,\n",
        "#         truncation=True,\n",
        "#         max_length=512,\n",
        "#         padding='max_length',\n",
        "#         return_tensors='pt'\n",
        "#     )\n",
        "\n",
        "#     input_ids = encoding['input_ids'].to(device)\n",
        "#     attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "#     # Predict\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "#         logits = outputs.logits[0]\n",
        "#         probs = torch.softmax(logits, dim=-1)\n",
        "#         pred_id = torch.argmax(probs).item()\n",
        "\n",
        "#     return id2label[pred_id]\n",
        "\n",
        "# # ===========================\n",
        "# # PROCESS DOCUMENTS\n",
        "# # ===========================\n",
        "# def process_document(item, marker_tokenizer, marker_model, marker_id2label,\n",
        "#                     detection_tokenizer, detection_model, device):\n",
        "#     \"\"\"Process a single document - returns BOTH conspiracy AND markers\"\"\"\n",
        "#     doc_id = item.get('_id')\n",
        "#     text = item.get('text', '')\n",
        "\n",
        "#     if not doc_id or not text:\n",
        "#         return None\n",
        "\n",
        "#     # Step 1: Predict markers using marker extraction model\n",
        "#     markers = predict_markers(\n",
        "#         text,\n",
        "#         marker_tokenizer,\n",
        "#         marker_model,\n",
        "#         marker_id2label,\n",
        "#         device\n",
        "#     )\n",
        "\n",
        "#     # Add 'text' field to each marker for submission\n",
        "#     for marker in markers:\n",
        "#         if 'text' not in marker:\n",
        "#             start = marker['startIndex']\n",
        "#             end = marker['endIndex']\n",
        "#             marker['text'] = text[start:end]\n",
        "\n",
        "#     # Step 2: Predict conspiracy using detection model + markers\n",
        "#     conspiracy = predict_conspiracy(\n",
        "#         text,\n",
        "#         markers,\n",
        "#         detection_tokenizer,\n",
        "#         detection_model,\n",
        "#         device\n",
        "#     )\n",
        "\n",
        "#     # ‚≠ê RETURN BOTH FIELDS (required format)\n",
        "#     return {\n",
        "#         \"_id\": doc_id,\n",
        "#         \"conspiracy\": conspiracy,\n",
        "#         \"markers\": markers  # Include markers in submission\n",
        "#     }\n",
        "\n",
        "# # ===========================\n",
        "# # MAIN EXECUTION\n",
        "# # ===========================\n",
        "# def main():\n",
        "#     print(\"\\n\" + \"=\"*70)\n",
        "#     print(\"STARTING MARKER-AWARE DETECTION PIPELINE\")\n",
        "#     print(\"=\"*70)\n",
        "\n",
        "#     # Load models\n",
        "#     marker_tokenizer, marker_model, marker_id2label, marker_device = load_marker_model()\n",
        "#     detection_tokenizer, detection_model, detection_device = load_detection_model()\n",
        "\n",
        "#     # Use same device for both\n",
        "#     device = marker_device\n",
        "\n",
        "#     # Load input data\n",
        "#     print(f\"\\nüìÇ Loading test data from: {INPUT_FILE}\")\n",
        "\n",
        "#     if not os.path.exists(INPUT_FILE):\n",
        "#         print(f\"‚ùå Error: File not found: {INPUT_FILE}\")\n",
        "#         return\n",
        "\n",
        "#     input_data = []\n",
        "#     with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
        "#         for line in f:\n",
        "#             try:\n",
        "#                 input_data.append(json.loads(line))\n",
        "#             except:\n",
        "#                 continue\n",
        "\n",
        "#     print(f\"‚úÖ Loaded {len(input_data)} documents\")\n",
        "\n",
        "#     # Process all documents\n",
        "#     print(f\"\\nüîÑ Generating predictions...\")\n",
        "#     predictions = []\n",
        "\n",
        "#     for item in tqdm(input_data, desc=\"Processing\"):\n",
        "#         pred = process_document(\n",
        "#             item,\n",
        "#             marker_tokenizer,\n",
        "#             marker_model,\n",
        "#             marker_id2label,\n",
        "#             detection_tokenizer,\n",
        "#             detection_model,\n",
        "#             device\n",
        "#         )\n",
        "#         if pred:\n",
        "#             predictions.append(pred)\n",
        "\n",
        "#     print(f\"\\n‚úÖ Generated predictions for {len(predictions)} documents\")\n",
        "\n",
        "#     # Calculate statistics\n",
        "#     yes_count = sum(1 for pred in predictions if pred['conspiracy'] == 'Yes')\n",
        "#     no_count = sum(1 for pred in predictions if pred['conspiracy'] == 'No')\n",
        "#     total_markers = sum(len(pred['markers']) for pred in predictions)\n",
        "#     docs_with_markers = sum(1 for pred in predictions if len(pred['markers']) > 0)\n",
        "\n",
        "#     print(f\"\\nüìä PREDICTION STATISTICS:\")\n",
        "#     print(f\"   Total documents: {len(predictions)}\")\n",
        "#     print(f\"\\nüìä Conspiracy Detection (Subtask 2):\")\n",
        "#     print(f\"   Predicted 'Yes': {yes_count} ({yes_count/len(predictions)*100:.1f}%)\")\n",
        "#     print(f\"   Predicted 'No':  {no_count} ({no_count/len(predictions)*100:.1f}%)\")\n",
        "#     print(f\"\\nüìä Marker Extraction (Subtask 1):\")\n",
        "#     print(f\"   Total markers: {total_markers}\")\n",
        "#     print(f\"   Docs with markers: {docs_with_markers} ({docs_with_markers/len(predictions)*100:.1f}%)\")\n",
        "#     print(f\"   Avg markers/doc: {total_markers/len(predictions):.2f}\")\n",
        "\n",
        "#     # Save to JSONL\n",
        "#     print(f\"\\nüíæ Saving predictions to {TEMP_SUBMISSION_FILE}...\")\n",
        "#     with open(TEMP_SUBMISSION_FILE, 'w', encoding='utf-8') as f:\n",
        "#         for pred in predictions:\n",
        "#             f.write(json.dumps(pred) + '\\n')\n",
        "\n",
        "#     # Create ZIP\n",
        "#     print(f\"üì¶ Creating ZIP archive: {OUTPUT_ZIP}...\")\n",
        "#     with zipfile.ZipFile(OUTPUT_ZIP, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
        "#         zf.write(TEMP_SUBMISSION_FILE, arcname=\"submission.jsonl\")\n",
        "\n",
        "#     # Clean up\n",
        "#     os.remove(TEMP_SUBMISSION_FILE)\n",
        "\n",
        "#     file_size = os.path.getsize(OUTPUT_ZIP) / 1024\n",
        "\n",
        "#     print(f\"\\n‚úÖ SUCCESS!\")\n",
        "#     print(f\"   Submission file: {OUTPUT_ZIP}\")\n",
        "#     print(f\"   File size: {file_size:.2f} KB\")\n",
        "\n",
        "#     # Show sample predictions\n",
        "#     print(f\"\\nüìã SAMPLE PREDICTIONS (first 5):\")\n",
        "#     for i, pred in enumerate(predictions[:5], 1):\n",
        "#         print(f\"\\n{i}. Document: {pred['_id']}\")\n",
        "#         print(f\"   Conspiracy: {pred['conspiracy']}\")\n",
        "#         if pred['markers']:\n",
        "#             print(f\"   Markers ({len(pred['markers'])}):\")\n",
        "#             for j, marker in enumerate(pred['markers'][:3], 1):\n",
        "#                 text_preview = marker['text'][:40] + \"...\" if len(marker['text']) > 40 else marker['text']\n",
        "#                 print(f\"      {j}. [{marker['type']:8s}] \\\"{text_preview}\\\"\")\n",
        "#             if len(pred['markers']) > 3:\n",
        "#                 print(f\"      ... and {len(pred['markers']) - 3} more\")\n",
        "#         else:\n",
        "#             print(f\"   Markers: (none)\")\n",
        "\n",
        "#     print(\"\\n\" + \"=\"*70)\n",
        "#     print(\"SUBMISSION READY FOR UPLOAD!\")\n",
        "#     print(\"=\"*70)\n",
        "#     print(f\"\\nüì§ Next steps:\")\n",
        "#     print(f\"1. Download {OUTPUT_ZIP}\")\n",
        "#     print(f\"2. Go to Codabench ‚Üí Detection task (Subtask 2)\")\n",
        "#     print(f\"3. Upload to 'My Submissions'\")\n",
        "#     print(f\"4. Wait for evaluation\")\n",
        "\n",
        "#     # Performance expectations\n",
        "#     yes_pct = yes_count / len(predictions) * 100\n",
        "#     print(f\"\\nüí° Performance Estimate:\")\n",
        "#     if 30 <= yes_pct <= 70:\n",
        "#         print(f\"   Good balance ({yes_pct:.1f}% 'Yes')\")\n",
        "#         print(f\"   Expected F1: 0.65-0.75\")\n",
        "#     elif yes_pct < 30:\n",
        "#         print(f\"   Conservative ({yes_pct:.1f}% 'Yes')\")\n",
        "#         print(f\"   Expected F1: 0.55-0.65 (high precision, lower recall)\")\n",
        "#     else:\n",
        "#         print(f\"   Liberal ({yes_pct:.1f}% 'Yes')\")\n",
        "#         print(f\"   Expected F1: 0.60-0.70 (high recall, lower precision)\")\n",
        "\n",
        "#     print(\"=\"*70)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     try:\n",
        "#         main()\n",
        "#     except Exception as e:\n",
        "#         print(f\"\\n‚ùå ERROR: {e}\")\n",
        "#         import traceback\n",
        "#         traceback.print_exc()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T13:23:43.005995Z",
          "iopub.status.idle": "2026-01-26T13:23:43.006449Z",
          "shell.execute_reply.started": "2026-01-26T13:23:43.006256Z",
          "shell.execute_reply": "2026-01-26T13:23:43.00628Z"
        },
        "id": "DbmiqupCfgeX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "not keep\n"
      ],
      "metadata": {
        "id": "6KJXLxlWfgeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install fuzzywuzzy python-Levenshtein -q"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T13:23:43.007481Z",
          "iopub.status.idle": "2026-01-26T13:23:43.007733Z",
          "shell.execute_reply.started": "2026-01-26T13:23:43.007616Z",
          "shell.execute_reply": "2026-01-26T13:23:43.007631Z"
        },
        "id": "d8czAvhGfgeY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# \"\"\"\n",
        "# LLM MARKER EXTRACTION - FIXED TWO-STAGE APPROACH\n",
        "# Stage 1: LLM extracts marker TEXT (no positions)\n",
        "# Stage 2: Fuzzy matching finds positions in original text\n",
        "# \"\"\"\n",
        "# import json\n",
        "# import torch\n",
        "# import re\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "# from tqdm import tqdm\n",
        "# from fuzzywuzzy import fuzz\n",
        "# from fuzzywuzzy import process\n",
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')\n",
        "\n",
        "# # ===========================\n",
        "# # IMPROVED PROMPT (TEXT ONLY)\n",
        "# # ===========================\n",
        "# FEW_SHOT_EXAMPLES = \"\"\"\n",
        "# EXAMPLE 1:\n",
        "# Text: \"The CIA and FBI are covering up evidence of election fraud to protect their puppet president.\"\n",
        "\n",
        "# Markers:\n",
        "# - Actor: \"CIA\", \"FBI\"\n",
        "# - Action: \"covering up evidence\", \"protect their puppet president\"\n",
        "# - Victim: (none explicit)\n",
        "# - Effect: (none explicit)\n",
        "# - Evidence: (none explicit)\n",
        "\n",
        "# JSON:\n",
        "# {\n",
        "#   \"Actor\": [\"CIA\", \"FBI\"],\n",
        "#   \"Action\": [\"covering up evidence\", \"protect their puppet president\"],\n",
        "#   \"Victim\": [],\n",
        "#   \"Effect\": [],\n",
        "#   \"Evidence\": []\n",
        "# }\n",
        "\n",
        "# EXAMPLE 2:\n",
        "# Text: \"Big Pharma suppressed the cure for cancer because treating patients is more profitable than curing them.\"\n",
        "\n",
        "# Markers:\n",
        "# - Actor: \"Big Pharma\"\n",
        "# - Action: \"suppressed the cure for cancer\"\n",
        "# - Victim: \"patients\"\n",
        "# - Effect: (implied financial harm, but not explicit)\n",
        "# - Evidence: \"treating patients is more profitable than curing them\"\n",
        "\n",
        "# JSON:\n",
        "# {\n",
        "#   \"Actor\": [\"Big Pharma\"],\n",
        "#   \"Action\": [\"suppressed the cure for cancer\"],\n",
        "#   \"Victim\": [\"patients\"],\n",
        "#   \"Effect\": [],\n",
        "#   \"Evidence\": [\"treating patients is more profitable than curing them\"]\n",
        "# }\n",
        "\n",
        "# EXAMPLE 3:\n",
        "# Text: \"Climate change is a hoax invented by scientists to secure research funding and push a globalist agenda.\"\n",
        "\n",
        "# Markers:\n",
        "# - Actor: \"scientists\"\n",
        "# - Action: \"invented\", \"secure research funding\", \"push a globalist agenda\"\n",
        "# - Victim: (none explicit)\n",
        "# - Effect: (none explicit)\n",
        "# - Evidence: \"to secure research funding\"\n",
        "\n",
        "# JSON:\n",
        "# {\n",
        "#   \"Actor\": [\"scientists\"],\n",
        "#   \"Action\": [\"invented\", \"secure research funding\", \"push a globalist agenda\"],\n",
        "#   \"Victim\": [],\n",
        "#   \"Effect\": [],\n",
        "#   \"Evidence\": [\"to secure research funding\"]\n",
        "# }\n",
        "\n",
        "# EXAMPLE 4:\n",
        "# Text: \"Mandatory vaccines are being used to implant tracking microchips in citizens for government surveillance.\"\n",
        "\n",
        "# Markers:\n",
        "# - Actor: \"government\" (implied)\n",
        "# - Action: \"implant tracking microchips\"\n",
        "# - Victim: \"citizens\"\n",
        "# - Effect: \"surveillance\"\n",
        "# - Evidence: (none explicit)\n",
        "\n",
        "# JSON:\n",
        "# {\n",
        "#   \"Actor\": [\"government\"],\n",
        "#   \"Action\": [\"implant tracking microchips\"],\n",
        "#   \"Victim\": [\"citizens\"],\n",
        "#   \"Effect\": [\"surveillance\"],\n",
        "#   \"Evidence\": []\n",
        "# }\n",
        "\n",
        "# EXAMPLE 5:\n",
        "# Text: \"The WHO deliberately exaggerated COVID death rates according to leaked documents to justify authoritarian lockdowns that stripped people of their freedoms.\"\n",
        "\n",
        "# Markers:\n",
        "# - Actor: \"The WHO\"\n",
        "# - Action: \"deliberately exaggerated COVID death rates\", \"justify authoritarian lockdowns\", \"stripped people of their freedoms\"\n",
        "# - Victim: \"people\"\n",
        "# - Effect: \"stripped people of their freedoms\", \"authoritarian lockdowns\"\n",
        "# - Evidence: \"leaked documents\"\n",
        "\n",
        "# JSON:\n",
        "# {\n",
        "#   \"Actor\": [\"The WHO\"],\n",
        "#   \"Action\": [\"deliberately exaggerated COVID death rates\", \"justify authoritarian lockdowns\", \"stripped people of their freedoms\"],\n",
        "#   \"Victim\": [\"people\"],\n",
        "#   \"Effect\": [\"stripped people of their freedoms\", \"authoritarian lockdowns\"],\n",
        "#   \"Evidence\": [\"leaked documents\"]\n",
        "# }\n",
        "# \"\"\"\n",
        "\n",
        "# SYSTEM_PROMPT = \"\"\"You are an expert at identifying conspiracy theory markers in text.\n",
        "\n",
        "# Extract EXACT TEXT SPANS for each marker type. Do NOT create positions/indices.\n",
        "\n",
        "# MARKER TYPES:\n",
        "# 1. Actor: Who is responsible? (people, groups, organizations with alleged malicious intent)\n",
        "# 2. Action: What are they doing? (malicious acts, plans, agendas)\n",
        "# 3. Effect: What negative consequences? (harm, damage, loss)\n",
        "# 4. Victim: Who is harmed? (targets of the conspiracy)\n",
        "# 5. Evidence: What proof is cited? (reasons to believe the conspiracy)\n",
        "\n",
        "# RULES:\n",
        "# - Extract EXACT phrases from the text (copy verbatim)\n",
        "# - One marker can appear in multiple categories\n",
        "# - If no markers of a type exist, use empty list []\n",
        "# - Return ONLY valid JSON, no explanation\n",
        "# \"\"\"\n",
        "\n",
        "# def create_prompt(text):\n",
        "#     \"\"\"Create improved prompt that asks for TEXT only\"\"\"\n",
        "#     return f\"\"\"<|im_start|>system\n",
        "# {SYSTEM_PROMPT}<|im_end|>\n",
        "# <|im_start|>user\n",
        "# {FEW_SHOT_EXAMPLES}\n",
        "\n",
        "# Now extract conspiracy markers from this text:\n",
        "\n",
        "# Text: \"{text}\"\n",
        "\n",
        "# Extract EXACT text spans (copy from the text above). Return ONLY this JSON format:\n",
        "# {{\n",
        "#   \"Actor\": [\"exact text 1\", \"exact text 2\"],\n",
        "#   \"Action\": [\"exact text 1\"],\n",
        "#   \"Victim\": [],\n",
        "#   \"Effect\": [\"exact text 1\"],\n",
        "#   \"Evidence\": []\n",
        "# }}\n",
        "# <|im_end|>\n",
        "# <|im_start|>assistant\n",
        "# \"\"\"\n",
        "\n",
        "# # ===========================\n",
        "# # FUZZY STRING MATCHING\n",
        "# # ===========================\n",
        "# def find_marker_positions(text, marker_text, marker_type, min_similarity=70):\n",
        "#     \"\"\"\n",
        "#     Find all positions of marker_text in the original text using fuzzy matching\n",
        "#     Returns list of (start, end) tuples\n",
        "#     \"\"\"\n",
        "#     positions = []\n",
        "#     text_lower = text.lower()\n",
        "#     marker_lower = marker_text.lower().strip()\n",
        "\n",
        "#     if not marker_lower or len(marker_lower) < 3:\n",
        "#         return positions\n",
        "\n",
        "#     # Strategy 1: Exact substring match (fastest)\n",
        "#     start = 0\n",
        "#     while True:\n",
        "#         idx = text_lower.find(marker_lower, start)\n",
        "#         if idx == -1:\n",
        "#             break\n",
        "#         positions.append((idx, idx + len(marker_text)))\n",
        "#         start = idx + 1\n",
        "\n",
        "#     if positions:\n",
        "#         return positions\n",
        "\n",
        "#     # Strategy 2: Fuzzy matching for close matches\n",
        "#     # Split text into overlapping windows\n",
        "#     words = text.split()\n",
        "#     marker_word_count = len(marker_text.split())\n",
        "\n",
        "#     best_match = None\n",
        "#     best_score = 0\n",
        "#     best_pos = None\n",
        "\n",
        "#     for i in range(len(words)):\n",
        "#         # Try windows of different sizes around marker length\n",
        "#         for window_size in range(max(1, marker_word_count - 2), marker_word_count + 3):\n",
        "#             if i + window_size > len(words):\n",
        "#                 continue\n",
        "\n",
        "#             window = ' '.join(words[i:i + window_size])\n",
        "#             score = fuzz.ratio(marker_lower, window.lower())\n",
        "\n",
        "#             if score > best_score and score >= min_similarity:\n",
        "#                 best_score = score\n",
        "#                 best_match = window\n",
        "\n",
        "#                 # Find position in original text\n",
        "#                 window_start = text.lower().find(window.lower())\n",
        "#                 if window_start != -1:\n",
        "#                     best_pos = (window_start, window_start + len(window))\n",
        "\n",
        "#     if best_pos:\n",
        "#         positions.append(best_pos)\n",
        "\n",
        "#     return positions\n",
        "\n",
        "# def extract_markers_with_positions(text, llm_markers):\n",
        "#     \"\"\"\n",
        "#     Convert LLM output (text only) to markers with positions\n",
        "#     \"\"\"\n",
        "#     final_markers = []\n",
        "\n",
        "#     for marker_type in ['Actor', 'Action', 'Effect', 'Victim', 'Evidence']:\n",
        "#         marker_texts = llm_markers.get(marker_type, [])\n",
        "\n",
        "#         for marker_text in marker_texts:\n",
        "#             if not marker_text or not isinstance(marker_text, str):\n",
        "#                 continue\n",
        "\n",
        "#             # Find all occurrences\n",
        "#             positions = find_marker_positions(text, marker_text, marker_type)\n",
        "\n",
        "#             for start, end in positions:\n",
        "#                 final_markers.append({\n",
        "#                     'type': marker_type,\n",
        "#                     'startIndex': start,\n",
        "#                     'endIndex': end,\n",
        "#                     'text': text[start:end]\n",
        "#                 })\n",
        "\n",
        "#     return final_markers\n",
        "\n",
        "# # ===========================\n",
        "# # PARSE LLM OUTPUT\n",
        "# # ===========================\n",
        "# def parse_llm_output(response):\n",
        "#     \"\"\"Parse JSON from LLM response\"\"\"\n",
        "#     try:\n",
        "#         # Remove markdown\n",
        "#         response = re.sub(r'```json\\s*|\\s*```', '', response)\n",
        "\n",
        "#         # Find JSON\n",
        "#         json_match = re.search(r'\\{[^}]*\"Actor\".*?\\}', response, re.DOTALL)\n",
        "#         if not json_match:\n",
        "#             return None\n",
        "\n",
        "#         data = json.loads(json_match.group())\n",
        "\n",
        "#         # Validate structure\n",
        "#         required_keys = ['Actor', 'Action', 'Effect', 'Victim', 'Evidence']\n",
        "#         if not all(k in data for k in required_keys):\n",
        "#             return None\n",
        "\n",
        "#         # Ensure all values are lists\n",
        "#         for key in required_keys:\n",
        "#             if not isinstance(data[key], list):\n",
        "#                 data[key] = []\n",
        "\n",
        "#         return data\n",
        "\n",
        "#     except Exception as e:\n",
        "#         return None\n",
        "\n",
        "# # ===========================\n",
        "# # LOAD MODEL\n",
        "# # ===========================\n",
        "# print(\"=\"*70)\n",
        "# print(\"LLM MARKER EXTRACTION - FIXED TWO-STAGE APPROACH\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "# print(f\"\\nüì¶ Loading {model_name}...\")\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     model_name,\n",
        "#     torch_dtype=torch.bfloat16,\n",
        "#     device_map=\"auto\",\n",
        "#     trust_remote_code=True\n",
        "# )\n",
        "\n",
        "# print(f\"‚úÖ Model loaded!\")\n",
        "\n",
        "# # ===========================\n",
        "# # GENERATE FUNCTION\n",
        "# # ===========================\n",
        "# def generate_markers(text):\n",
        "#     \"\"\"Generate marker texts (no positions)\"\"\"\n",
        "#     prompt = create_prompt(text)\n",
        "\n",
        "#     inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model.generate(\n",
        "#             **inputs,\n",
        "#             max_new_tokens=500,\n",
        "#             temperature=0.2,  # Lower temp for more consistent output\n",
        "#             top_p=0.9,\n",
        "#             do_sample=True,\n",
        "#             pad_token_id=tokenizer.pad_token_id,\n",
        "#             eos_token_id=tokenizer.eos_token_id\n",
        "#         )\n",
        "\n",
        "#     response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "#     return response\n",
        "\n",
        "# # ===========================\n",
        "# # MAIN PROCESSING\n",
        "# # ===========================\n",
        "# def process_document(text):\n",
        "#     \"\"\"Process one document: LLM ‚Üí fuzzy match ‚Üí markers\"\"\"\n",
        "\n",
        "#     # Stage 1: LLM extracts marker texts\n",
        "#     response = generate_markers(text)\n",
        "#     llm_markers = parse_llm_output(response)\n",
        "\n",
        "#     if not llm_markers:\n",
        "#         return []\n",
        "\n",
        "#     # Stage 2: Find positions via fuzzy matching\n",
        "#     markers = extract_markers_with_positions(text, llm_markers)\n",
        "\n",
        "#     return markers\n",
        "\n",
        "# # ===========================\n",
        "# # TEST FUNCTION\n",
        "# # ===========================\n",
        "# def test_two_stage(input_file, output_file, num_samples=50):\n",
        "#     \"\"\"Test two-stage extraction\"\"\"\n",
        "\n",
        "#     print(f\"\\n{'='*70}\")\n",
        "#     print(f\"TESTING TWO-STAGE EXTRACTION\")\n",
        "#     print(f\"{'='*70}\")\n",
        "\n",
        "#     # Load data\n",
        "#     print(f\"\\nüìÇ Loading from: {input_file}\")\n",
        "#     data = []\n",
        "#     with open(input_file, 'r', encoding='utf-8') as f:\n",
        "#         for line in f:\n",
        "#             try:\n",
        "#                 item = json.loads(line)\n",
        "#                 if item.get('text') and len(item['text']) >= 30:\n",
        "#                     data.append(item)\n",
        "#             except:\n",
        "#                 continue\n",
        "\n",
        "#     data = data[:num_samples]\n",
        "#     print(f\"‚úÖ Loaded {len(data)} examples\")\n",
        "\n",
        "#     # Process\n",
        "#     predictions = []\n",
        "#     total_markers = 0\n",
        "#     errors = 0\n",
        "\n",
        "#     print(f\"\\nüîÑ Processing (Stage 1: LLM, Stage 2: Position matching)...\")\n",
        "#     for item in tqdm(data, desc=\"Processing\"):\n",
        "#         text = item['text']\n",
        "#         doc_id = item.get('_id', f\"doc_{len(predictions)}\")\n",
        "\n",
        "#         try:\n",
        "#             markers = process_document(text)\n",
        "#             total_markers += len(markers)\n",
        "\n",
        "#             predictions.append({\n",
        "#                 \"_id\": doc_id,\n",
        "#                 \"markers\": markers,\n",
        "#                 \"text\": text\n",
        "#             })\n",
        "\n",
        "#         except Exception as e:\n",
        "#             errors += 1\n",
        "#             predictions.append({\n",
        "#                 \"_id\": doc_id,\n",
        "#                 \"markers\": [],\n",
        "#                 \"text\": text\n",
        "#             })\n",
        "\n",
        "#     # Statistics\n",
        "#     docs_with_markers = sum(1 for p in predictions if len(p['markers']) > 0)\n",
        "\n",
        "#     print(f\"\\n{'='*70}\")\n",
        "#     print(f\"RESULTS\")\n",
        "#     print(f\"{'='*70}\")\n",
        "#     print(f\"‚úÖ Processed: {len(predictions)} documents\")\n",
        "#     print(f\"üìä Total markers: {total_markers}\")\n",
        "#     print(f\"üìÑ Docs with markers: {docs_with_markers} ({docs_with_markers/len(predictions)*100:.1f}%)\")\n",
        "#     print(f\"üìà Avg markers/doc: {total_markers/len(predictions):.2f}\")\n",
        "#     if errors > 0:\n",
        "#         print(f\"‚ö†Ô∏è  Errors: {errors}\")\n",
        "\n",
        "#     # Marker distribution\n",
        "#     marker_counts = {'Actor': 0, 'Action': 0, 'Effect': 0, 'Victim': 0, 'Evidence': 0}\n",
        "#     for pred in predictions:\n",
        "#         for m in pred['markers']:\n",
        "#             marker_counts[m['type']] += 1\n",
        "\n",
        "#     print(f\"\\nüìä Marker Distribution:\")\n",
        "#     for mtype, count in sorted(marker_counts.items(), key=lambda x: -x[1]):\n",
        "#         pct = (count / total_markers * 100) if total_markers > 0 else 0\n",
        "#         print(f\"   {mtype:10s}: {count:4d} ({pct:5.1f}%)\")\n",
        "\n",
        "#     # Save\n",
        "#     print(f\"\\nüíæ Saving to {output_file}...\")\n",
        "#     with open(output_file, 'w', encoding='utf-8') as f:\n",
        "#         for pred in predictions:\n",
        "#             output_pred = {\n",
        "#                 \"_id\": pred[\"_id\"],\n",
        "#                 \"markers\": pred[\"markers\"]\n",
        "#             }\n",
        "#             f.write(json.dumps(output_pred) + '\\n')\n",
        "\n",
        "#     # Samples\n",
        "#     print(f\"\\n{'='*70}\")\n",
        "#     print(f\"SAMPLE PREDICTIONS (first 3)\")\n",
        "#     print(f\"{'='*70}\")\n",
        "#     for i, pred in enumerate(predictions[:3], 1):\n",
        "#         print(f\"\\nüìÑ Document {i}: {pred['_id']}\")\n",
        "#         print(f\"   Text: {pred['text'][:120]}...\")\n",
        "\n",
        "#         if pred['markers']:\n",
        "#             print(f\"   ‚úÖ Markers ({len(pred['markers'])}):\")\n",
        "#             for j, m in enumerate(pred['markers'][:8], 1):\n",
        "#                 # Verify position\n",
        "#                 actual_text = pred['text'][m['startIndex']:m['endIndex']]\n",
        "#                 match = \"‚úì\" if actual_text == m['text'] else \"‚úó\"\n",
        "#                 text_preview = m['text'][:40] + \"...\" if len(m['text']) > 40 else m['text']\n",
        "#                 print(f\"      {j}. [{m['type']:8s}] {match} \\\"{text_preview}\\\"\")\n",
        "#             if len(pred['markers']) > 8:\n",
        "#                 print(f\"      ... and {len(pred['markers']) - 8} more\")\n",
        "#         else:\n",
        "#             print(f\"   ‚ùå No markers\")\n",
        "\n",
        "#     print(f\"\\n{'='*70}\")\n",
        "#     print(f\"‚úÖ TWO-STAGE EXTRACTION COMPLETE!\")\n",
        "#     print(f\"{'='*70}\")\n",
        "\n",
        "#     # Performance estimate\n",
        "#     if total_markers > 0:\n",
        "#         print(f\"\\nüí° Performance Estimate:\")\n",
        "#         avg = total_markers / len(predictions)\n",
        "#         if avg >= 3:\n",
        "#             print(f\"   üéâ Excellent! {avg:.1f} markers/doc\")\n",
        "#             print(f\"   Expected F1: 0.25-0.35\")\n",
        "#         elif avg >= 1.5:\n",
        "#             print(f\"   ‚úÖ Good! {avg:.1f} markers/doc\")\n",
        "#             print(f\"   Expected F1: 0.20-0.28\")\n",
        "#         else:\n",
        "#             print(f\"   ‚ö†Ô∏è  Low: {avg:.1f} markers/doc\")\n",
        "#             print(f\"   Expected F1: 0.15-0.22\")\n",
        "\n",
        "#     return predictions\n",
        "\n",
        "# # ===========================\n",
        "# # RUN\n",
        "# # ===========================\n",
        "# if __name__ == \"__main__\":\n",
        "#     predictions = test_two_stage(\n",
        "#         input_file=\"dev_rehydrated_v2.jsonl\",\n",
        "#         output_file=\"llm_predictions_fixed.jsonl\",\n",
        "#         num_samples=50\n",
        "#     )"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T13:23:43.008789Z",
          "iopub.status.idle": "2026-01-26T13:23:43.009028Z",
          "shell.execute_reply.started": "2026-01-26T13:23:43.008916Z",
          "shell.execute_reply": "2026-01-26T13:23:43.008931Z"
        },
        "id": "dTyCRXR6fgeY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# \"\"\"\n",
        "# THRESHOLD OPTIMIZER\n",
        "# Find the best confidence threshold for your model\n",
        "# \"\"\"\n",
        "\n",
        "# import json\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "# from tqdm import tqdm\n",
        "# from collections import Counter\n",
        "\n",
        "# # Configuration\n",
        "# MODEL_PATH = \"./marker_results_advanced/checkpoint-best\"\n",
        "# MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
        "# TRAIN_FILE = \"train_rehydrated_v2.jsonl\"\n",
        "\n",
        "# # Labels\n",
        "# MARKER_TYPES = [\"Actor\", \"Action\", \"Effect\", \"Evidence\", \"Victim\"]\n",
        "# labels_list = [\"O\"]\n",
        "# for mt in MARKER_TYPES:\n",
        "#     labels_list.extend([f\"B-{mt}\", f\"I-{mt}\"])\n",
        "\n",
        "# label2id = {label: i for i, label in enumerate(labels_list)}\n",
        "# id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "# print(\"=\"*70)\n",
        "# print(\"CONFIDENCE THRESHOLD OPTIMIZER\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# # Load model\n",
        "# print(\"\\nüì¶ Loading model...\")\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(f\"Device: {device}\")\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "# model = AutoModelForTokenClassification.from_pretrained(MODEL_PATH)\n",
        "# model.to(device)  # CRITICAL: Move model to device\n",
        "# model.eval()\n",
        "\n",
        "# print(\"‚úÖ Model loaded\")\n",
        "\n",
        "# # Prediction function\n",
        "# def predict_with_threshold(text, threshold=0.5, max_length=512):\n",
        "#     \"\"\"Predict markers with given threshold\"\"\"\n",
        "\n",
        "#     encoding = tokenizer(\n",
        "#         text,\n",
        "#         truncation=True,\n",
        "#         max_length=max_length,\n",
        "#         padding='max_length',\n",
        "#         return_offsets_mapping=True,\n",
        "#         return_tensors='pt'\n",
        "#     )\n",
        "\n",
        "#     offset_mapping = encoding['offset_mapping'][0].cpu().numpy()\n",
        "\n",
        "#     # Move inputs to device\n",
        "#     input_ids = encoding['input_ids'].to(device)\n",
        "#     attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "#         logits = outputs.logits[0].cpu().numpy()\n",
        "\n",
        "#         predictions = np.argmax(logits, axis=1)\n",
        "#         confidences = np.max(torch.softmax(torch.tensor(logits), dim=1).numpy(), axis=1)\n",
        "\n",
        "#     # Extract markers\n",
        "#     markers = []\n",
        "#     current_marker = None\n",
        "#     current_start = None\n",
        "#     current_end = None\n",
        "#     current_conf = []\n",
        "\n",
        "#     for idx, (pred_id, conf, (token_start, token_end)) in enumerate(zip(predictions, confidences, offset_mapping)):\n",
        "#         if token_start == 0 and token_end == 0:\n",
        "#             continue\n",
        "\n",
        "#         label = id2label[pred_id]\n",
        "\n",
        "#         if label.startswith('B-'):\n",
        "#             if current_marker is not None:\n",
        "#                 avg_conf = np.mean(current_conf)\n",
        "#                 if avg_conf >= threshold:\n",
        "#                     markers.append({\n",
        "#                         'startIndex': current_start,\n",
        "#                         'endIndex': current_end,\n",
        "#                         'type': current_marker\n",
        "#                     })\n",
        "\n",
        "#             marker_type = label[2:]\n",
        "#             current_marker = marker_type\n",
        "#             current_start = int(token_start)\n",
        "#             current_end = int(token_end)\n",
        "#             current_conf = [conf]\n",
        "\n",
        "#         elif label.startswith('I-') and current_marker is not None:\n",
        "#             marker_type = label[2:]\n",
        "#             if marker_type == current_marker:\n",
        "#                 current_end = int(token_end)\n",
        "#                 current_conf.append(conf)\n",
        "\n",
        "#         elif label == 'O':\n",
        "#             if current_marker is not None:\n",
        "#                 avg_conf = np.mean(current_conf)\n",
        "#                 if avg_conf >= threshold:\n",
        "#                     markers.append({\n",
        "#                         'startIndex': current_start,\n",
        "#                         'endIndex': current_end,\n",
        "#                         'type': current_marker\n",
        "#                     })\n",
        "#                 current_marker = None\n",
        "\n",
        "#     if current_marker is not None:\n",
        "#         avg_conf = np.mean(current_conf)\n",
        "#         if avg_conf >= threshold:\n",
        "#             markers.append({\n",
        "#                 'startIndex': current_start,\n",
        "#                 'endIndex': current_end,\n",
        "#                 'type': current_marker\n",
        "#             })\n",
        "\n",
        "#     return markers\n",
        "\n",
        "\n",
        "# # Load sample data\n",
        "# print(f\"\\nüìÇ Loading {TRAIN_FILE}...\")\n",
        "# samples = []\n",
        "# with open(TRAIN_FILE, 'r') as f:\n",
        "#     for i, line in enumerate(f):\n",
        "#         if i >= 100:  # Test on first 100 samples\n",
        "#             break\n",
        "#         try:\n",
        "#             item = json.loads(line)\n",
        "#             if item.get('text') and len(item.get('markers', [])) > 0:\n",
        "#                 samples.append(item)\n",
        "#         except:\n",
        "#             pass\n",
        "\n",
        "# print(f\"‚úÖ Loaded {len(samples)} samples with markers\")\n",
        "\n",
        "# # Test different thresholds\n",
        "# thresholds = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.6, 0.7]\n",
        "\n",
        "# print(f\"\\n{'='*70}\")\n",
        "# print(\"TESTING THRESHOLDS\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# results = []\n",
        "\n",
        "# for threshold in tqdm(thresholds, desc=\"Testing thresholds\"):\n",
        "#     total_true = 0\n",
        "#     total_pred = 0\n",
        "#     marker_counts = Counter()\n",
        "\n",
        "#     for sample in samples:\n",
        "#         true_markers = sample['markers']\n",
        "#         pred_markers = predict_with_threshold(sample['text'], threshold=threshold)\n",
        "\n",
        "#         total_true += len(true_markers)\n",
        "#         total_pred += len(pred_markers)\n",
        "\n",
        "#         for marker in pred_markers:\n",
        "#             marker_counts[marker['type']] += 1\n",
        "\n",
        "#     avg_pred = total_pred / len(samples)\n",
        "#     avg_true = total_true / len(samples)\n",
        "\n",
        "#     results.append({\n",
        "#         'threshold': threshold,\n",
        "#         'avg_pred': avg_pred,\n",
        "#         'avg_true': avg_true,\n",
        "#         'total_pred': total_pred,\n",
        "#         'total_true': total_true,\n",
        "#         'marker_counts': marker_counts\n",
        "#     })\n",
        "\n",
        "# # Display results\n",
        "# print(f\"\\n{'='*70}\")\n",
        "# print(\"RESULTS\")\n",
        "# print(\"=\"*70)\n",
        "# print(f\"True average: {results[0]['avg_true']:.2f} markers/doc\")\n",
        "# print()\n",
        "\n",
        "# print(f\"{'Threshold':<12} {'Avg Pred':<10} {'Total':<8} {'Diff':<8} {'Recommendation'}\")\n",
        "# print(\"-\"*70)\n",
        "\n",
        "# best_threshold = None\n",
        "# best_diff = float('inf')\n",
        "\n",
        "# for r in results:\n",
        "#     diff = abs(r['avg_pred'] - r['avg_true'])\n",
        "#     recommendation = \"\"\n",
        "\n",
        "#     if diff < best_diff:\n",
        "#         best_diff = diff\n",
        "#         best_threshold = r['threshold']\n",
        "#         recommendation = \"‚≠ê BEST\"\n",
        "#     elif abs(r['avg_pred'] - r['avg_true']) < r['avg_true'] * 0.2:\n",
        "#         recommendation = \"‚úÖ Good\"\n",
        "#     elif r['avg_pred'] < r['avg_true'] * 0.5:\n",
        "#         recommendation = \"‚ö†Ô∏è Too low\"\n",
        "#     elif r['avg_pred'] > r['avg_true'] * 1.5:\n",
        "#         recommendation = \"‚ö†Ô∏è Too high\"\n",
        "\n",
        "#     print(f\"{r['threshold']:<12.2f} {r['avg_pred']:<10.2f} {r['total_pred']:<8d} {diff:<8.2f} {recommendation}\")\n",
        "\n",
        "# # Detailed analysis of best threshold\n",
        "# print(f\"\\n{'='*70}\")\n",
        "# print(f\"RECOMMENDED THRESHOLD: {best_threshold}\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# best_result = [r for r in results if r['threshold'] == best_threshold][0]\n",
        "\n",
        "# print(f\"\\nWith threshold {best_threshold}:\")\n",
        "# print(f\"  Average predictions: {best_result['avg_pred']:.2f} markers/doc\")\n",
        "# print(f\"  Average true: {best_result['avg_true']:.2f} markers/doc\")\n",
        "# print(f\"  Difference: {abs(best_result['avg_pred'] - best_result['avg_true']):.2f}\")\n",
        "\n",
        "# print(f\"\\nMarker type distribution:\")\n",
        "# total_markers = sum(best_result['marker_counts'].values())\n",
        "# for mtype in MARKER_TYPES:\n",
        "#     count = best_result['marker_counts'].get(mtype, 0)\n",
        "#     pct = count / total_markers * 100 if total_markers > 0 else 0\n",
        "#     print(f\"  {mtype:10s}: {count:4d} ({pct:5.2f}%)\")\n",
        "\n",
        "# # Show sample prediction with best threshold\n",
        "# print(f\"\\n{'='*70}\")\n",
        "# print(\"SAMPLE PREDICTION WITH BEST THRESHOLD\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# sample = samples[0]\n",
        "# pred_markers = predict_with_threshold(sample['text'], threshold=best_threshold)\n",
        "\n",
        "# print(f\"\\nText: {sample['text'][:150]}...\")\n",
        "# print(f\"\\nTrue markers ({len(sample['markers'])}):\")\n",
        "# for m in sample['markers']:\n",
        "#     print(f\"  [{m['type']:8s}] \\\"{m.get('text', sample['text'][m['startIndex']:m['endIndex']])}\\\"\")\n",
        "\n",
        "# print(f\"\\nPredicted markers ({len(pred_markers)}):\")\n",
        "# for m in pred_markers:\n",
        "#     print(f\"  [{m['type']:8s}] \\\"{sample['text'][m['startIndex']:m['endIndex']]}\\\"\")\n",
        "\n",
        "# # Final recommendation\n",
        "# print(f\"\\n{'='*70}\")\n",
        "# print(\"FINAL RECOMMENDATION\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# print(f\"\\n‚ú® Use threshold: {best_threshold}\")\n",
        "# print(f\"\\nüìù Update your submission script:\")\n",
        "# print(f\"   Change: confidence_threshold={best_threshold}\")\n",
        "# print(f\"\\nüìä Expected results on test set (~938 docs):\")\n",
        "# print(f\"   Total markers: ~{int(best_result['avg_pred'] * 938)}\")\n",
        "# print(f\"   Avg markers/doc: ~{best_result['avg_pred']:.2f}\")\n",
        "\n",
        "# if best_result['avg_pred'] < 2:\n",
        "#     print(f\"\\n‚ö†Ô∏è  Still quite low! Consider threshold {best_threshold - 0.05:.2f} for more markers\")\n",
        "# elif best_result['avg_pred'] > 6:\n",
        "#     print(f\"\\n‚ö†Ô∏è  Might be too many! Consider threshold {best_threshold + 0.05:.2f} for fewer false positives\")\n",
        "# else:\n",
        "#     print(f\"\\n‚úÖ This looks good!\")\n",
        "\n",
        "# print(\"=\"*70)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T13:23:43.010224Z",
          "iopub.status.idle": "2026-01-26T13:23:43.010502Z",
          "shell.execute_reply.started": "2026-01-26T13:23:43.010386Z",
          "shell.execute_reply": "2026-01-26T13:23:43.010401Z"
        },
        "id": "WUUJfhdqfgeY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# \"\"\"\n",
        "# ENSEMBLE TRAINING SCRIPT\n",
        "# Train 3 diverse models for better marker extraction\n",
        "# Target: 0.30-0.40 F1 (up from 0.18)\n",
        "# \"\"\"\n",
        "\n",
        "# import json\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# from datasets import Dataset\n",
        "# from transformers import (\n",
        "#     AutoTokenizer,\n",
        "#     AutoModelForTokenClassification,\n",
        "#     Trainer,\n",
        "#     TrainingArguments,\n",
        "#     DataCollatorForTokenClassification,\n",
        "#     EarlyStoppingCallback\n",
        "# )\n",
        "# from seqeval.metrics import f1_score as seqeval_f1\n",
        "# from seqeval.scheme import IOB2\n",
        "# from collections import Counter\n",
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')\n",
        "\n",
        "# # ===========================\n",
        "# # CONFIGURATION\n",
        "# # ===========================\n",
        "\n",
        "# MARKER_TYPES = [\"Actor\", \"Action\", \"Effect\", \"Evidence\", \"Victim\"]\n",
        "\n",
        "# labels_list = [\"O\"]\n",
        "# for mt in MARKER_TYPES:\n",
        "#     labels_list.extend([f\"B-{mt}\", f\"I-{mt}\"])\n",
        "\n",
        "# label2id = {label: i for i, label in enumerate(labels_list)}\n",
        "# id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "# # ===========================\n",
        "# # ENSEMBLE CONFIGURATIONS\n",
        "# # ===========================\n",
        "\n",
        "# ENSEMBLE_CONFIGS = [\n",
        "#     {\n",
        "#         \"name\": \"model_1_deberta_512\",\n",
        "#         \"model_name\": \"microsoft/deberta-v3-base\",\n",
        "#         \"max_length\": 512,\n",
        "#         \"batch_size\": 4,\n",
        "#         \"learning_rate\": 1e-5,\n",
        "#         \"epochs\": 12,\n",
        "#         \"description\": \"DeBERTa-v3 with 512 tokens (balanced)\"\n",
        "#     },\n",
        "#     {\n",
        "#         \"name\": \"model_2_deberta_768\",\n",
        "#         \"model_name\": \"microsoft/deberta-v3-base\",\n",
        "#         \"max_length\": 768,\n",
        "#         \"batch_size\": 2,  # Smaller batch for longer sequences\n",
        "#         \"learning_rate\": 8e-6,\n",
        "#         \"epochs\": 10,\n",
        "#         \"description\": \"DeBERTa-v3 with 768 tokens (captures long-range dependencies)\"\n",
        "#     },\n",
        "#     {\n",
        "#         \"name\": \"model_3_roberta_large\",\n",
        "#         \"model_name\": \"roberta-large\",\n",
        "#         \"max_length\": 512,\n",
        "#         \"batch_size\": 2,  # Smaller batch for large model\n",
        "#         \"learning_rate\": 5e-6,\n",
        "#         \"epochs\": 10,\n",
        "#         \"description\": \"RoBERTa-Large (different architecture)\"\n",
        "#     }\n",
        "# ]\n",
        "\n",
        "# # ===========================\n",
        "# # DATA LOADING FUNCTIONS\n",
        "# # ===========================\n",
        "\n",
        "# def align_labels_advanced(text, markers, tokenizer, max_length):\n",
        "#     \"\"\"Advanced label alignment\"\"\"\n",
        "#     encoding = tokenizer(\n",
        "#         text,\n",
        "#         truncation=True,\n",
        "#         max_length=max_length,\n",
        "#         padding='max_length',\n",
        "#         return_offsets_mapping=True,\n",
        "#         add_special_tokens=True\n",
        "#     )\n",
        "\n",
        "#     offset_mapping = encoding['offset_mapping']\n",
        "#     labels = []\n",
        "\n",
        "#     for token_start, token_end in offset_mapping:\n",
        "#         if token_start == 0 and token_end == 0:\n",
        "#             labels.append(-100)\n",
        "#         else:\n",
        "#             labels.append(label2id[\"O\"])\n",
        "\n",
        "#     sorted_markers = sorted(markers, key=lambda x: (x['startIndex'], x['endIndex']))\n",
        "\n",
        "#     for marker in sorted_markers:\n",
        "#         start_char = marker['startIndex']\n",
        "#         end_char = marker['endIndex']\n",
        "#         marker_type = marker['type']\n",
        "\n",
        "#         if marker_type not in MARKER_TYPES:\n",
        "#             continue\n",
        "\n",
        "#         first_token_idx = None\n",
        "#         last_token_idx = None\n",
        "\n",
        "#         for idx, (token_start, token_end) in enumerate(offset_mapping):\n",
        "#             if token_start == 0 and token_end == 0:\n",
        "#                 continue\n",
        "\n",
        "#             if token_start < end_char and token_end > start_char:\n",
        "#                 overlap_start = max(token_start, start_char)\n",
        "#                 overlap_end = min(token_end, end_char)\n",
        "#                 overlap_ratio = (overlap_end - overlap_start) / (token_end - token_start)\n",
        "\n",
        "#                 if overlap_ratio >= 0.4:\n",
        "#                     if first_token_idx is None:\n",
        "#                         first_token_idx = idx\n",
        "#                     last_token_idx = idx\n",
        "\n",
        "#         if first_token_idx is not None:\n",
        "#             labels[first_token_idx] = label2id[f\"B-{marker_type}\"]\n",
        "\n",
        "#             if last_token_idx is not None and last_token_idx > first_token_idx:\n",
        "#                 for idx in range(first_token_idx + 1, last_token_idx + 1):\n",
        "#                     if labels[idx] != -100:\n",
        "#                         labels[idx] = label2id[f\"I-{marker_type}\"]\n",
        "\n",
        "#     return labels\n",
        "\n",
        "\n",
        "# def create_dataset(jsonl_path, tokenizer, max_length, min_length=30):\n",
        "#     \"\"\"Create dataset for training\"\"\"\n",
        "#     all_input_ids = []\n",
        "#     all_attention_masks = []\n",
        "#     all_labels = []\n",
        "\n",
        "#     stats = {'total': 0, 'kept': 0, 'total_markers': 0}\n",
        "\n",
        "#     with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
        "#         for line in f:\n",
        "#             stats['total'] += 1\n",
        "\n",
        "#             try:\n",
        "#                 item = json.loads(line)\n",
        "#                 text = item.get('text', '').strip()\n",
        "#                 markers = item.get('markers', [])\n",
        "\n",
        "#                 if len(text) < min_length:\n",
        "#                     continue\n",
        "\n",
        "#                 # For training, keep examples with markers\n",
        "#                 if 'train' in jsonl_path and len(markers) == 0:\n",
        "#                     continue\n",
        "\n",
        "#                 stats['total_markers'] += len(markers)\n",
        "\n",
        "#                 encoding = tokenizer(\n",
        "#                     text,\n",
        "#                     truncation=True,\n",
        "#                     max_length=max_length,\n",
        "#                     padding='max_length'\n",
        "#                 )\n",
        "\n",
        "#                 label_ids = align_labels_advanced(text, markers, tokenizer, max_length)\n",
        "\n",
        "#                 all_input_ids.append(encoding['input_ids'])\n",
        "#                 all_attention_masks.append(encoding['attention_mask'])\n",
        "#                 all_labels.append(label_ids)\n",
        "#                 stats['kept'] += 1\n",
        "\n",
        "#             except:\n",
        "#                 continue\n",
        "\n",
        "#     print(f\"   Kept: {stats['kept']}/{stats['total']} examples\")\n",
        "#     print(f\"   Total markers: {stats['total_markers']}\")\n",
        "#     if stats['kept'] > 0:\n",
        "#         print(f\"   Avg markers/doc: {stats['total_markers'] / stats['kept']:.2f}\")\n",
        "\n",
        "#     return Dataset.from_dict({\n",
        "#         'input_ids': all_input_ids,\n",
        "#         'attention_mask': all_attention_masks,\n",
        "#         'labels': all_labels\n",
        "#     })\n",
        "\n",
        "\n",
        "# # ===========================\n",
        "# # METRICS\n",
        "# # ===========================\n",
        "\n",
        "# def compute_metrics(eval_pred):\n",
        "#     predictions, labels = eval_pred\n",
        "#     predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "#     true_labels = []\n",
        "#     pred_labels = []\n",
        "\n",
        "#     for prediction, label in zip(predictions, labels):\n",
        "#         true_seq = []\n",
        "#         pred_seq = []\n",
        "\n",
        "#         for pred_id, label_id in zip(prediction, label):\n",
        "#             if label_id != -100:\n",
        "#                 true_seq.append(id2label[label_id])\n",
        "#                 pred_seq.append(id2label[pred_id])\n",
        "\n",
        "#         if len(true_seq) > 0:\n",
        "#             true_labels.append(true_seq)\n",
        "#             pred_labels.append(pred_seq)\n",
        "\n",
        "#     try:\n",
        "#         overall_f1 = seqeval_f1(true_labels, pred_labels, mode='strict', scheme=IOB2)\n",
        "#     except:\n",
        "#         overall_f1 = 0.0\n",
        "\n",
        "#     return {\"overall_f1\": overall_f1}\n",
        "\n",
        "\n",
        "# # ===========================\n",
        "# # TRAIN ENSEMBLE\n",
        "# # ===========================\n",
        "\n",
        "# def train_single_model(config_idx):\n",
        "#     \"\"\"Train a single model from ensemble\"\"\"\n",
        "\n",
        "#     config = ENSEMBLE_CONFIGS[config_idx]\n",
        "\n",
        "#     print(\"\\n\" + \"=\"*70)\n",
        "#     print(f\"TRAINING MODEL {config_idx + 1}/3: {config['name']}\")\n",
        "#     print(\"=\"*70)\n",
        "#     print(f\"Description: {config['description']}\")\n",
        "#     print(f\"Model: {config['model_name']}\")\n",
        "#     print(f\"Max length: {config['max_length']}\")\n",
        "#     print(f\"Batch size: {config['batch_size']}\")\n",
        "#     print(\"=\"*70)\n",
        "\n",
        "#     # Load tokenizer\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(config['model_name'])\n",
        "\n",
        "#     # Create datasets\n",
        "#     print(\"\\nLoading training data...\")\n",
        "#     train_dataset = create_dataset(\n",
        "#         \"train_rehydrated_v2.jsonl\",\n",
        "#         tokenizer,\n",
        "#         config['max_length']\n",
        "#     )\n",
        "\n",
        "#     print(\"\\nLoading validation data...\")\n",
        "#     val_dataset_temp = create_dataset(\n",
        "#         \"dev_rehydrated_v2.jsonl\",\n",
        "#         tokenizer,\n",
        "#         config['max_length']\n",
        "#     )\n",
        "\n",
        "#     # Check if we need to split from training\n",
        "#     val_has_markers = False\n",
        "#     if len(val_dataset_temp) > 0:\n",
        "#         for i in range(min(10, len(val_dataset_temp))):\n",
        "#             labels = val_dataset_temp[i]['labels']\n",
        "#             if any(l > 0 and l != -100 for l in labels):\n",
        "#                 val_has_markers = True\n",
        "#                 break\n",
        "\n",
        "#     if not val_has_markers:\n",
        "#         print(\"‚ö†Ô∏è  Dev set has no markers, splitting from training...\")\n",
        "#         split = train_dataset.train_test_split(test_size=0.15, seed=42)\n",
        "#         train_dataset = split['train']\n",
        "#         val_dataset = split['test']\n",
        "#     else:\n",
        "#         val_dataset = val_dataset_temp\n",
        "\n",
        "#     print(f\"\\nFinal split: {len(train_dataset)} train, {len(val_dataset)} val\")\n",
        "\n",
        "#     train_dataset.set_format(\"torch\")\n",
        "#     val_dataset.set_format(\"torch\")\n",
        "\n",
        "#     # Initialize model\n",
        "#     print(f\"\\nInitializing {config['model_name']}...\")\n",
        "#     model = AutoModelForTokenClassification.from_pretrained(\n",
        "#         config['model_name'],\n",
        "#         num_labels=len(labels_list),\n",
        "#         id2label=id2label,\n",
        "#         label2id=label2id,\n",
        "#         ignore_mismatched_sizes=True\n",
        "#     )\n",
        "\n",
        "#     # Training arguments\n",
        "#     training_args = TrainingArguments(\n",
        "#         output_dir=f\"./ensemble/{config['name']}\",\n",
        "#         eval_strategy=\"epoch\",\n",
        "#         save_strategy=\"epoch\",\n",
        "#         learning_rate=config['learning_rate'],\n",
        "#         per_device_train_batch_size=config['batch_size'],\n",
        "#         per_device_eval_batch_size=config['batch_size'] * 2,\n",
        "#         num_train_epochs=config['epochs'],\n",
        "#         weight_decay=0.01,\n",
        "#         warmup_ratio=0.1,\n",
        "#         load_best_model_at_end=True,\n",
        "#         metric_for_best_model=\"overall_f1\",\n",
        "#         greater_is_better=True,\n",
        "#         logging_steps=50,\n",
        "#         report_to=\"none\",\n",
        "#         save_total_limit=2,\n",
        "#         fp16=torch.cuda.is_available(),\n",
        "#         gradient_accumulation_steps=4,\n",
        "#         lr_scheduler_type=\"cosine\",\n",
        "#         seed=42 + config_idx  # Different seed for diversity\n",
        "#     )\n",
        "\n",
        "#     # Data collator\n",
        "#     data_collator = DataCollatorForTokenClassification(\n",
        "#         tokenizer=tokenizer,\n",
        "#         padding=True,\n",
        "#         label_pad_token_id=-100\n",
        "#     )\n",
        "\n",
        "#     # Trainer\n",
        "#     trainer = Trainer(\n",
        "#         model=model,\n",
        "#         args=training_args,\n",
        "#         train_dataset=train_dataset,\n",
        "#         eval_dataset=val_dataset,\n",
        "#         data_collator=data_collator,\n",
        "#         tokenizer=tokenizer,\n",
        "#         compute_metrics=compute_metrics,\n",
        "#         callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "#     )\n",
        "\n",
        "#     # Train\n",
        "#     print(f\"\\nüöÄ Starting training...\")\n",
        "#     trainer.train()\n",
        "\n",
        "#     # Save\n",
        "#     save_path = f\"./ensemble/{config['name']}/checkpoint-best\"\n",
        "#     trainer.save_model(save_path)\n",
        "#     tokenizer.save_pretrained(save_path)\n",
        "\n",
        "#     # Evaluate\n",
        "#     eval_results = trainer.evaluate()\n",
        "\n",
        "#     print(f\"\\n‚úÖ Model {config_idx + 1} complete!\")\n",
        "#     print(f\"   Overall F1: {eval_results['eval_overall_f1']:.4f}\")\n",
        "#     print(f\"   Saved to: {save_path}\")\n",
        "\n",
        "#     return eval_results['eval_overall_f1']\n",
        "\n",
        "\n",
        "# # ===========================\n",
        "# # MAIN\n",
        "# # ===========================\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     print(\"\\n\" + \"=\"*70)\n",
        "#     print(\"ENSEMBLE TRAINING PIPELINE\")\n",
        "#     print(\"=\"*70)\n",
        "#     print(f\"Training {len(ENSEMBLE_CONFIGS)} diverse models\")\n",
        "#     print(f\"Expected improvement: 0.18 ‚Üí 0.30-0.40 F1\")\n",
        "#     print(\"=\"*70)\n",
        "\n",
        "#     # Train all models\n",
        "#     f1_scores = []\n",
        "\n",
        "#     for i in range(len(ENSEMBLE_CONFIGS)):\n",
        "#         try:\n",
        "#             f1 = train_single_model(i)\n",
        "#             f1_scores.append(f1)\n",
        "#         except Exception as e:\n",
        "#             print(f\"\\n‚ùå Model {i+1} failed: {e}\")\n",
        "#             f1_scores.append(0.0)\n",
        "#             continue\n",
        "\n",
        "#     # Summary\n",
        "#     print(\"\\n\" + \"=\"*70)\n",
        "#     print(\"ENSEMBLE TRAINING COMPLETE!\")\n",
        "#     print(\"=\"*70)\n",
        "\n",
        "#     for i, (config, f1) in enumerate(zip(ENSEMBLE_CONFIGS, f1_scores)):\n",
        "#         emoji = \"üéâ\" if f1 > 0.20 else \"‚úÖ\" if f1 > 0.15 else \"‚ö†Ô∏è\"\n",
        "#         print(f\"{emoji} Model {i+1} ({config['name']}): F1 = {f1:.4f}\")\n",
        "\n",
        "#     avg_f1 = np.mean([f for f in f1_scores if f > 0])\n",
        "#     print(f\"\\nüìä Average F1: {avg_f1:.4f}\")\n",
        "#     print(f\"üìä Expected ensemble F1: {avg_f1 * 1.15:.4f} (+15% from averaging)\")\n",
        "\n",
        "#     print(\"\\n\" + \"=\"*70)\n",
        "#     print(\"NEXT STEPS:\")\n",
        "#     print(\"=\"*70)\n",
        "#     print(\"1. Run the ensemble inference script\")\n",
        "#     print(\"2. This will combine predictions from all 3 models\")\n",
        "#     print(\"3. Expected final score: 0.30-0.40 F1\")\n",
        "#     print(\"=\"*70)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T13:23:43.012095Z",
          "iopub.status.idle": "2026-01-26T13:23:43.012329Z",
          "shell.execute_reply.started": "2026-01-26T13:23:43.012221Z",
          "shell.execute_reply": "2026-01-26T13:23:43.012235Z"
        },
        "id": "ShcSxXIXfgeZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# \"\"\"\n",
        "# SPACE-EFFICIENT ENSEMBLE INFERENCE\n",
        "# Single-pass processing to avoid disk space issues\n",
        "# \"\"\"\n",
        "# import json\n",
        "# import os\n",
        "# import zipfile\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "# from tqdm import tqdm\n",
        "# from collections import Counter\n",
        "# import gc\n",
        "\n",
        "# # ===========================\n",
        "# # CONFIGURATION\n",
        "# # ===========================\n",
        "# TEST_FILE = \"test_rehydrated_v2.jsonl\"\n",
        "# OUTPUT_ZIP = \"/kaggle/working/submission.zip\"\n",
        "\n",
        "# MARKER_TYPES = [\"Actor\", \"Action\", \"Effect\", \"Evidence\", \"Victim\"]\n",
        "\n",
        "# labels_list = [\"O\"]\n",
        "# for mt in MARKER_TYPES:\n",
        "#     labels_list.extend([f\"B-{mt}\", f\"I-{mt}\"])\n",
        "\n",
        "# label2id = {label: i for i, label in enumerate(labels_list)}\n",
        "# id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "# # Use ONLY the best model (not ensemble) to save space\n",
        "# MODEL_CONFIG = {\n",
        "#     \"path\": \"./marker_results_advanced/checkpoint-best\",\n",
        "#     \"model_name\": \"microsoft/deberta-v3-base\",\n",
        "#     \"max_length\": 512\n",
        "# }\n",
        "\n",
        "# print(\"=\"*70)\n",
        "# print(\"SPACE-EFFICIENT MARKER EXTRACTION\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# # ===========================\n",
        "# # LOAD MODEL\n",
        "# # ===========================\n",
        "# def load_model():\n",
        "#     \"\"\"Load single best model\"\"\"\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     print(f\"\\nDevice: {device}\")\n",
        "#     print(f\"Loading: {MODEL_CONFIG['model_name']}\")\n",
        "\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(MODEL_CONFIG['model_name'])\n",
        "\n",
        "#     model = AutoModelForTokenClassification.from_pretrained(\n",
        "#         MODEL_CONFIG['path'],\n",
        "#         num_labels=len(labels_list),\n",
        "#         id2label=id2label,\n",
        "#         label2id=label2id\n",
        "#     )\n",
        "#     model.to(device)\n",
        "#     model.eval()\n",
        "\n",
        "#     print(\"‚úÖ Model loaded\")\n",
        "\n",
        "#     return tokenizer, model, device\n",
        "\n",
        "# # ===========================\n",
        "# # PREDICTION\n",
        "# # ===========================\n",
        "# def predict_markers(text, tokenizer, model, device, confidence_threshold=0.15):\n",
        "#     \"\"\"Predict markers for text\"\"\"\n",
        "\n",
        "#     if not text or len(text.strip()) < 10:\n",
        "#         return []\n",
        "\n",
        "#     # Tokenize\n",
        "#     encoding = tokenizer(\n",
        "#         text,\n",
        "#         truncation=True,\n",
        "#         max_length=MODEL_CONFIG['max_length'],\n",
        "#         padding='max_length',\n",
        "#         return_offsets_mapping=True,\n",
        "#         return_tensors='pt'\n",
        "#     )\n",
        "\n",
        "#     offset_mapping = encoding['offset_mapping'][0].cpu().numpy()\n",
        "\n",
        "#     # Move to device\n",
        "#     input_ids = encoding['input_ids'].to(device)\n",
        "#     attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "#     # Predict\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "#         logits = outputs.logits[0].cpu().numpy()\n",
        "\n",
        "#         predictions = np.argmax(logits, axis=1)\n",
        "#         probs = torch.softmax(torch.tensor(logits), dim=1).numpy()\n",
        "#         confidences = np.max(probs, axis=1)\n",
        "\n",
        "#     # Extract markers\n",
        "#     markers = []\n",
        "#     current_marker = None\n",
        "#     current_start = None\n",
        "#     current_end = None\n",
        "#     current_conf = []\n",
        "\n",
        "#     for idx, (pred_id, conf, (token_start, token_end)) in enumerate(zip(predictions, confidences, offset_mapping)):\n",
        "#         if token_start == 0 and token_end == 0:\n",
        "#             continue\n",
        "\n",
        "#         label = id2label[pred_id]\n",
        "\n",
        "#         if label.startswith('B-'):\n",
        "#             # Save previous\n",
        "#             if current_marker is not None and np.mean(current_conf) >= confidence_threshold:\n",
        "#                 markers.append({\n",
        "#                     'startIndex': int(current_start),\n",
        "#                     'endIndex': int(current_end),\n",
        "#                     'type': current_marker,\n",
        "#                     'text': text[current_start:current_end]\n",
        "#                 })\n",
        "\n",
        "#             # Start new\n",
        "#             current_marker = label[2:]\n",
        "#             current_start = int(token_start)\n",
        "#             current_end = int(token_end)\n",
        "#             current_conf = [conf]\n",
        "\n",
        "#         elif label.startswith('I-') and current_marker is not None:\n",
        "#             if label[2:] == current_marker:\n",
        "#                 current_end = int(token_end)\n",
        "#                 current_conf.append(conf)\n",
        "\n",
        "#         elif label == 'O':\n",
        "#             if current_marker is not None and np.mean(current_conf) >= confidence_threshold:\n",
        "#                 markers.append({\n",
        "#                     'startIndex': int(current_start),\n",
        "#                     'endIndex': int(current_end),\n",
        "#                     'type': current_marker,\n",
        "#                     'text': text[current_start:current_end]\n",
        "#                 })\n",
        "#             current_marker = None\n",
        "\n",
        "#     # Don't forget last\n",
        "#     if current_marker is not None and np.mean(current_conf) >= confidence_threshold:\n",
        "#         markers.append({\n",
        "#             'startIndex': int(current_start),\n",
        "#             'endIndex': int(current_end),\n",
        "#             'type': current_marker,\n",
        "#             'text': text[current_start:current_end]\n",
        "#         })\n",
        "\n",
        "#     return markers\n",
        "\n",
        "# # ===========================\n",
        "# # MAIN\n",
        "# # ===========================\n",
        "# def main():\n",
        "#     # Load model\n",
        "#     print(\"\\n\" + \"=\"*70)\n",
        "#     print(\"LOADING MODEL\")\n",
        "#     print(\"=\"*70)\n",
        "\n",
        "#     tokenizer, model, device = load_model()\n",
        "\n",
        "#     # Load test data\n",
        "#     print(f\"\\nüìÇ Loading: {TEST_FILE}\")\n",
        "\n",
        "#     if not os.path.exists(TEST_FILE):\n",
        "#         print(f\"‚ùå Not found: {TEST_FILE}\")\n",
        "#         return\n",
        "\n",
        "#     test_data = []\n",
        "#     with open(TEST_FILE, 'r') as f:\n",
        "#         for line in f:\n",
        "#             try:\n",
        "#                 test_data.append(json.loads(line))\n",
        "#             except:\n",
        "#                 pass\n",
        "\n",
        "#     print(f\"‚úÖ Loaded {len(test_data)} documents\")\n",
        "\n",
        "#     # Single-pass: Generate predictions and write to ZIP directly\n",
        "#     print(f\"\\nüîÑ Generating predictions and creating submission...\")\n",
        "#     print(f\"   Writing to: {OUTPUT_ZIP}\")\n",
        "\n",
        "#     stats = {\n",
        "#         'total_markers': 0,\n",
        "#         'docs_with_markers': 0,\n",
        "#         'marker_counts': Counter()\n",
        "#     }\n",
        "\n",
        "#     # Write directly to ZIP in streaming mode\n",
        "#     with zipfile.ZipFile(OUTPUT_ZIP, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
        "#         with zf.open(\"submission.jsonl\", mode=\"w\") as zf_file:\n",
        "\n",
        "#             for item in tqdm(test_data, desc=\"Processing\"):\n",
        "#                 doc_id = item['_id']\n",
        "#                 text = item.get('text', '')\n",
        "\n",
        "#                 # Predict markers\n",
        "#                 if text and len(text) >= 10:\n",
        "#                     markers = predict_markers(text, tokenizer, model, device, confidence_threshold=0.15)\n",
        "#                 else:\n",
        "#                     markers = []\n",
        "\n",
        "#                 # Update stats\n",
        "#                 stats['total_markers'] += len(markers)\n",
        "#                 if len(markers) > 0:\n",
        "#                     stats['docs_with_markers'] += 1\n",
        "\n",
        "#                 for marker in markers:\n",
        "#                     stats['marker_counts'][marker['type']] += 1\n",
        "\n",
        "#                 # Create output\n",
        "#                 output = {\n",
        "#                     \"_id\": doc_id,\n",
        "#                     \"markers\": markers\n",
        "#                 }\n",
        "\n",
        "#                 # Write to ZIP\n",
        "#                 line = json.dumps(output, ensure_ascii=False) + \"\\n\"\n",
        "#                 zf_file.write(line.encode(\"utf-8\"))\n",
        "\n",
        "#                 # Periodic cleanup to free memory\n",
        "#                 if len(test_data) > 500 and (test_data.index(item) + 1) % 100 == 0:\n",
        "#                     gc.collect()\n",
        "#                     torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "#     # Print statistics\n",
        "#     print(f\"\\n{'='*70}\")\n",
        "#     print(\"SUBMISSION STATISTICS\")\n",
        "#     print(\"=\"*70)\n",
        "\n",
        "#     total_docs = len(test_data)\n",
        "#     total_markers = stats['total_markers']\n",
        "#     docs_with_markers = stats['docs_with_markers']\n",
        "\n",
        "#     print(f\"Total documents: {total_docs}\")\n",
        "#     print(f\"Documents with markers: {docs_with_markers} ({docs_with_markers/total_docs*100:.1f}%)\")\n",
        "#     print(f\"Total markers: {total_markers}\")\n",
        "#     print(f\"Avg markers/doc: {total_markers/total_docs:.2f}\")\n",
        "\n",
        "#     print(f\"\\nMarker Distribution:\")\n",
        "#     for mtype in MARKER_TYPES:\n",
        "#         count = stats['marker_counts'].get(mtype, 0)\n",
        "#         pct = count / total_markers * 100 if total_markers > 0 else 0\n",
        "#         print(f\"   {mtype:10s}: {count:5d} ({pct:5.2f}%)\")\n",
        "\n",
        "#     # File info\n",
        "#     file_size = os.path.getsize(OUTPUT_ZIP) / 1024\n",
        "#     print(f\"\\n‚úÖ Submission created: {OUTPUT_ZIP}\")\n",
        "#     print(f\"üì¶ File size: {file_size:.2f} KB\")\n",
        "\n",
        "#     # Expected performance\n",
        "#     avg_markers = total_markers / total_docs\n",
        "#     print(f\"\\n{'='*70}\")\n",
        "#     print(\"EXPECTED PERFORMANCE\")\n",
        "#     print(\"=\"*70)\n",
        "\n",
        "#     if avg_markers >= 5:\n",
        "#         print(f\"Avg markers/doc: {avg_markers:.2f}\")\n",
        "#         print(f\"Expected F1: 0.35-0.45 üéâ\")\n",
        "#     elif avg_markers >= 3.5:\n",
        "#         print(f\"Avg markers/doc: {avg_markers:.2f}\")\n",
        "#         print(f\"Expected F1: 0.25-0.35 ‚úÖ\")\n",
        "#     elif avg_markers >= 2.5:\n",
        "#         print(f\"Avg markers/doc: {avg_markers:.2f}\")\n",
        "#         print(f\"Expected F1: 0.18-0.28 ‚ö†Ô∏è\")\n",
        "#     else:\n",
        "#         print(f\"Avg markers/doc: {avg_markers:.2f}\")\n",
        "#         print(f\"Expected F1: 0.10-0.20 ‚ùå\")\n",
        "#         print(f\"Tip: Lower confidence_threshold to get more markers\")\n",
        "\n",
        "#     print(\"=\"*70)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T13:23:43.013327Z",
          "iopub.status.idle": "2026-01-26T13:23:43.013619Z",
          "shell.execute_reply.started": "2026-01-26T13:23:43.013489Z",
          "shell.execute_reply": "2026-01-26T13:23:43.013511Z"
        },
        "id": "HYuSouixfgeZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# \"\"\"\n",
        "# SUBTASK 1: MARKER EXTRACTION - FULLY FIXED\n",
        "# Handles empty dev set and improves training\n",
        "# \"\"\"\n",
        "# import json\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# from datasets import Dataset\n",
        "# from transformers import (\n",
        "#     AutoTokenizer,\n",
        "#     AutoModelForTokenClassification,\n",
        "#     Trainer,\n",
        "#     TrainingArguments,\n",
        "#     DataCollatorForTokenClassification\n",
        "# )\n",
        "# from seqeval.metrics import f1_score as seqeval_f1\n",
        "# from seqeval.metrics import classification_report as seqeval_report\n",
        "# from seqeval.scheme import IOB2\n",
        "# from collections import Counter\n",
        "\n",
        "# print(\"=\"*70)\n",
        "# print(\"SUBTASK 1: MARKER EXTRACTION - FULLY FIXED\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# # ===========================\n",
        "# # 1. BIO TAG SCHEME\n",
        "# # ===========================\n",
        "# MARKER_TYPES = [\"Actor\", \"Action\", \"Effect\", \"Evidence\", \"Victim\"]\n",
        "# labels_list = [\"O\"]\n",
        "# for marker_type in MARKER_TYPES:\n",
        "#     labels_list.append(f\"B-{marker_type}\")\n",
        "#     labels_list.append(f\"I-{marker_type}\")\n",
        "\n",
        "# label2id = {label: i for i, label in enumerate(labels_list)}\n",
        "# id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "# print(f\"\\nTotal labels: {len(labels_list)}\")\n",
        "# print(f\"Labels: {labels_list}\")\n",
        "\n",
        "# # ===========================\n",
        "# # 2. FIXED MARKER ALIGNMENT\n",
        "# # ===========================\n",
        "# def align_labels_with_tokens_improved(text, markers, tokenizer):\n",
        "#     \"\"\"\n",
        "#     Improved alignment with better overlap detection\n",
        "#     \"\"\"\n",
        "#     encoding = tokenizer(\n",
        "#         text,\n",
        "#         truncation=True,\n",
        "#         max_length=512,  # Increased from 256\n",
        "#         padding='max_length',\n",
        "#         return_offsets_mapping=True,\n",
        "#         add_special_tokens=True\n",
        "#     )\n",
        "\n",
        "#     offset_mapping = encoding['offset_mapping']\n",
        "#     labels = []\n",
        "\n",
        "#     for token_start, token_end in offset_mapping:\n",
        "#         if token_start == 0 and token_end == 0:\n",
        "#             labels.append(-100)  # Special tokens/padding\n",
        "#         else:\n",
        "#             labels.append(label2id[\"O\"])\n",
        "\n",
        "#     # Process markers (handle overlapping)\n",
        "#     for marker in markers:\n",
        "#         start_char = marker['startIndex']\n",
        "#         end_char = marker['endIndex']\n",
        "#         marker_type = marker['type']\n",
        "\n",
        "#         if marker_type not in MARKER_TYPES:\n",
        "#             continue\n",
        "\n",
        "#         # Find all tokens that overlap with this marker\n",
        "#         overlapping_tokens = []\n",
        "#         for idx, (token_start, token_end) in enumerate(offset_mapping):\n",
        "#             if token_start == 0 and token_end == 0:\n",
        "#                 continue\n",
        "\n",
        "#             # Token overlaps if there's any intersection\n",
        "#             if token_start < end_char and token_end > start_char:\n",
        "#                 overlap_start = max(token_start, start_char)\n",
        "#                 overlap_end = min(token_end, end_char)\n",
        "#                 overlap_ratio = (overlap_end - overlap_start) / (token_end - token_start)\n",
        "\n",
        "#                 # Require at least 50% overlap\n",
        "#                 if overlap_ratio >= 0.5:\n",
        "#                     overlapping_tokens.append(idx)\n",
        "\n",
        "#         # Assign B- and I- tags\n",
        "#         if overlapping_tokens:\n",
        "#             labels[overlapping_tokens[0]] = label2id[f\"B-{marker_type}\"]\n",
        "#             for idx in overlapping_tokens[1:]:\n",
        "#                 labels[idx] = label2id[f\"I-{marker_type}\"]\n",
        "\n",
        "#     return labels\n",
        "\n",
        "# def create_token_dataset(jsonl_path, tokenizer, require_markers=False):\n",
        "#     \"\"\"\n",
        "#     Create dataset with option to filter for examples with markers\n",
        "#     \"\"\"\n",
        "#     all_input_ids = []\n",
        "#     all_attention_masks = []\n",
        "#     all_labels = []\n",
        "\n",
        "#     skipped = 0\n",
        "#     skipped_no_markers = 0\n",
        "#     total_markers = 0\n",
        "#     label_counts = Counter()\n",
        "\n",
        "#     print(f\"\\nProcessing {jsonl_path}...\")\n",
        "\n",
        "#     with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
        "#         for line_num, line in enumerate(f, 1):\n",
        "#             try:\n",
        "#                 item = json.loads(line)\n",
        "#                 text = item.get('text', '').strip()\n",
        "#                 markers = item.get('markers', [])\n",
        "\n",
        "#                 # Skip empty texts\n",
        "#                 if not text or len(text) < 10:\n",
        "#                     skipped += 1\n",
        "#                     continue\n",
        "\n",
        "#                 # Skip if no markers and we require them\n",
        "#                 if require_markers and len(markers) == 0:\n",
        "#                     skipped_no_markers += 1\n",
        "#                     continue\n",
        "\n",
        "#                 total_markers += len(markers)\n",
        "\n",
        "#                 # Tokenize\n",
        "#                 encoding = tokenizer(\n",
        "#                     text,\n",
        "#                     truncation=True,\n",
        "#                     max_length=512,\n",
        "#                     padding='max_length'\n",
        "#                 )\n",
        "\n",
        "#                 # Align labels\n",
        "#                 label_ids = align_labels_with_tokens_improved(text, markers, tokenizer)\n",
        "\n",
        "#                 # Count labels\n",
        "#                 for label_id in label_ids:\n",
        "#                     if label_id != -100:\n",
        "#                         label_counts[id2label[label_id]] += 1\n",
        "\n",
        "#                 all_input_ids.append(encoding['input_ids'])\n",
        "#                 all_attention_masks.append(encoding['attention_mask'])\n",
        "#                 all_labels.append(label_ids)\n",
        "\n",
        "#             except Exception as e:\n",
        "#                 skipped += 1\n",
        "#                 continue\n",
        "\n",
        "#     print(f\"‚úÖ Loaded {len(all_input_ids)} examples\")\n",
        "#     print(f\"   Skipped (general): {skipped}\")\n",
        "#     if require_markers:\n",
        "#         print(f\"   Skipped (no markers): {skipped_no_markers}\")\n",
        "#     print(f\"üìä Total markers: {total_markers}\")\n",
        "\n",
        "#     if len(all_input_ids) > 0:\n",
        "#         print(f\"üìä Avg markers per example: {total_markers / len(all_input_ids):.2f}\")\n",
        "\n",
        "#     # Show label distribution\n",
        "#     print(f\"\\nüìä Label distribution:\")\n",
        "#     for label, count in sorted(label_counts.items(), key=lambda x: -x[1])[:15]:\n",
        "#         print(f\"   {label:15s}: {count:7d}\")\n",
        "\n",
        "#     # Check marker labels\n",
        "#     marker_label_count = sum(count for label, count in label_counts.items() if label != 'O')\n",
        "#     if marker_label_count == 0:\n",
        "#         print(\"\\n‚ö†Ô∏è  WARNING: No marker labels found!\")\n",
        "#     else:\n",
        "#         print(f\"\\n‚úÖ Found {marker_label_count} marker labels\")\n",
        "#         print(f\"   Ratio: {marker_label_count / (marker_label_count + label_counts.get('O', 0)):.2%}\")\n",
        "\n",
        "#     return Dataset.from_dict({\n",
        "#         'input_ids': all_input_ids,\n",
        "#         'attention_mask': all_attention_masks,\n",
        "#         'labels': all_labels\n",
        "#     })\n",
        "\n",
        "# # ===========================\n",
        "# # 3. LOAD DATA - SMART SPLIT\n",
        "# # ===========================\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"LOADING DATA\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# model_name = \"roberta-base\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
        "\n",
        "# # Load training data\n",
        "# train_dataset = create_token_dataset(\"train_rehydrated.jsonl\", tokenizer)\n",
        "\n",
        "# # Check dev set\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"CHECKING VALIDATION SET\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# val_dataset_temp = create_token_dataset(\"dev_rehydrated.jsonl\", tokenizer)\n",
        "\n",
        "# # Calculate marker ratio in validation\n",
        "# val_has_markers = False\n",
        "# if len(val_dataset_temp) > 0:\n",
        "#     # Check if validation has any marker labels\n",
        "#     sample_labels = val_dataset_temp['labels'][:10]\n",
        "#     for labels in sample_labels:\n",
        "#         if any(l > 0 and l != -100 for l in labels):\n",
        "#             val_has_markers = True\n",
        "#             break\n",
        "\n",
        "# if not val_has_markers or len(val_dataset_temp) < 20:\n",
        "#     print(\"\\n‚ö†Ô∏è  CRITICAL: Validation set has no markers or is too small!\")\n",
        "#     print(\"   Solution: Splitting from training data (15% validation)\")\n",
        "\n",
        "#     # Split training data\n",
        "#     split = train_dataset.train_test_split(test_size=0.15, seed=42)\n",
        "#     train_dataset = split['train']\n",
        "#     val_dataset = split['test']\n",
        "\n",
        "#     print(f\"\\n‚úÖ New split created:\")\n",
        "#     print(f\"   Training: {len(train_dataset)} examples\")\n",
        "#     print(f\"   Validation: {len(val_dataset)} examples\")\n",
        "# else:\n",
        "#     val_dataset = val_dataset_temp\n",
        "#     print(f\"\\n‚úÖ Using provided validation set:\")\n",
        "#     print(f\"   Training: {len(train_dataset)} examples\")\n",
        "#     print(f\"   Validation: {len(val_dataset)} examples\")\n",
        "\n",
        "# # Set format\n",
        "# train_dataset.set_format(\"torch\")\n",
        "# val_dataset.set_format(\"torch\")\n",
        "\n",
        "# # ===========================\n",
        "# # 4. MODEL\n",
        "# # ===========================\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"INITIALIZING MODEL\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# model = AutoModelForTokenClassification.from_pretrained(\n",
        "#     model_name,\n",
        "#     num_labels=len(labels_list),\n",
        "#     id2label=id2label,\n",
        "#     label2id=label2id,\n",
        "#     ignore_mismatched_sizes=True\n",
        "# )\n",
        "\n",
        "# print(f\"‚úÖ Model initialized with {len(labels_list)} labels\")\n",
        "\n",
        "# # ===========================\n",
        "# # 5. IMPROVED METRICS\n",
        "# # ===========================\n",
        "# def compute_metrics_robust(eval_pred):\n",
        "#     \"\"\"\n",
        "#     Robust metric computation with detailed logging\n",
        "#     \"\"\"\n",
        "#     predictions, labels = eval_pred\n",
        "#     predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "#     # Convert to label strings\n",
        "#     true_labels = []\n",
        "#     pred_labels = []\n",
        "\n",
        "#     for prediction, label in zip(predictions, labels):\n",
        "#         true_seq = []\n",
        "#         pred_seq = []\n",
        "\n",
        "#         for pred_id, label_id in zip(prediction, label):\n",
        "#             if label_id != -100:\n",
        "#                 true_seq.append(id2label[label_id])\n",
        "#                 pred_seq.append(id2label[pred_id])\n",
        "\n",
        "#         if len(true_seq) > 0:\n",
        "#             true_labels.append(true_seq)\n",
        "#             pred_labels.append(pred_seq)\n",
        "\n",
        "#     # Calculate metrics\n",
        "#     results = {}\n",
        "\n",
        "#     # Overall F1\n",
        "#     try:\n",
        "#         overall_f1 = seqeval_f1(true_labels, pred_labels, mode='strict', scheme=IOB2)\n",
        "#         results[\"overall_f1\"] = overall_f1\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error computing overall F1: {e}\")\n",
        "#         results[\"overall_f1\"] = 0.0\n",
        "\n",
        "#     # Per-marker F1\n",
        "#     for marker_type in MARKER_TYPES:\n",
        "#         try:\n",
        "#             # Filter for this marker type\n",
        "#             filtered_true = []\n",
        "#             filtered_pred = []\n",
        "\n",
        "#             for true_seq, pred_seq in zip(true_labels, pred_labels):\n",
        "#                 ft = []\n",
        "#                 fp = []\n",
        "#                 for t, p in zip(true_seq, pred_seq):\n",
        "#                     if marker_type in t:\n",
        "#                         ft.append(t)\n",
        "#                         fp.append(p if marker_type in p else 'O')\n",
        "#                     else:\n",
        "#                         ft.append('O')\n",
        "#                         fp.append('O')\n",
        "\n",
        "#                 filtered_true.append(ft)\n",
        "#                 filtered_pred.append(fp)\n",
        "\n",
        "#             if any(any(x != 'O' for x in seq) for seq in filtered_true):\n",
        "#                 marker_f1 = seqeval_f1(filtered_true, filtered_pred, mode='strict', scheme=IOB2)\n",
        "#             else:\n",
        "#                 marker_f1 = 0.0\n",
        "\n",
        "#             results[f\"f1_{marker_type}\"] = marker_f1\n",
        "#         except Exception as e:\n",
        "#             results[f\"f1_{marker_type}\"] = 0.0\n",
        "\n",
        "#     return results\n",
        "\n",
        "# # ===========================\n",
        "# # 6. DATA COLLATOR\n",
        "# # ===========================\n",
        "# data_collator = DataCollatorForTokenClassification(\n",
        "#     tokenizer=tokenizer,\n",
        "#     padding=True,\n",
        "#     label_pad_token_id=-100\n",
        "# )\n",
        "\n",
        "# # ===========================\n",
        "# # 7. IMPROVED TRAINING ARGS\n",
        "# # ===========================\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"./marker_results\",\n",
        "#     eval_strategy=\"epoch\",\n",
        "#     save_strategy=\"epoch\",\n",
        "#     learning_rate=3e-5,  # Balanced learning rate\n",
        "#     per_device_train_batch_size=8,  # Smaller batch for better gradients\n",
        "#     per_device_eval_batch_size=16,\n",
        "#     num_train_epochs=10,  # More epochs\n",
        "#     weight_decay=0.01,\n",
        "#     warmup_ratio=0.1,\n",
        "#     load_best_model_at_end=True,\n",
        "#     metric_for_best_model=\"overall_f1\",\n",
        "#     greater_is_better=True,\n",
        "#     logging_steps=50,\n",
        "#     logging_first_step=True,\n",
        "#     report_to=\"none\",\n",
        "#     save_total_limit=3,\n",
        "#     fp16=torch.cuda.is_available(),\n",
        "#     gradient_accumulation_steps=2,  # Effective batch = 16\n",
        "#     dataloader_num_workers=2\n",
        "# )\n",
        "\n",
        "# # ===========================\n",
        "# # 8. TRAINER\n",
        "# # ===========================\n",
        "# trainer = Trainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=train_dataset,\n",
        "#     eval_dataset=val_dataset,\n",
        "#     data_collator=data_collator,\n",
        "#     tokenizer=tokenizer,\n",
        "#     compute_metrics=compute_metrics_robust\n",
        "# )\n",
        "\n",
        "# # ===========================\n",
        "# # 9. TRAIN\n",
        "# # ===========================\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"STARTING TRAINING\")\n",
        "# print(\"=\"*70)\n",
        "# print(\"üìä Expected metrics:\")\n",
        "# print(\"   Epoch 1-2: F1 0.00-0.05 (model learning patterns)\")\n",
        "# print(\"   Epoch 3-5: F1 0.05-0.15 (markers being detected)\")\n",
        "# print(\"   Epoch 6+:  F1 0.15-0.30 (good performance)\")\n",
        "# print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# trainer.train()\n",
        "\n",
        "# # ===========================\n",
        "# # 10. SAVE\n",
        "# # ===========================\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"SAVING MODEL\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# trainer.save_model(\"./marker_results/checkpoint-best\")\n",
        "# tokenizer.save_pretrained(\"./marker_results/checkpoint-best\")\n",
        "# print(\"‚úÖ Model saved to ./marker_results/checkpoint-best\")\n",
        "\n",
        "# # ===========================\n",
        "# # 11. DETAILED EVALUATION\n",
        "# # ===========================\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"FINAL EVALUATION\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# eval_results = trainer.evaluate()\n",
        "\n",
        "# print(f\"\\nüìä OVERALL RESULTS:\")\n",
        "# print(f\"   Overall F1: {eval_results['eval_overall_f1']:.4f}\")\n",
        "\n",
        "# print(f\"\\nüìä PER-MARKER F1 SCORES:\")\n",
        "# for marker_type in MARKER_TYPES:\n",
        "#     key = f\"eval_f1_{marker_type}\"\n",
        "#     if key in eval_results:\n",
        "#         score = eval_results[key]\n",
        "#         if score > 0.15:\n",
        "#             emoji = \"‚úÖ\"\n",
        "#         elif score > 0.08:\n",
        "#             emoji = \"‚ö†Ô∏è\"\n",
        "#         else:\n",
        "#             emoji = \"‚ùå\"\n",
        "#         print(f\"   {emoji} {marker_type:10s}: {score:.4f}\")\n",
        "\n",
        "# # ===========================\n",
        "# # 12. SAMPLE PREDICTIONS\n",
        "# # ===========================\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"SAMPLE PREDICTIONS\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# predictions = trainer.predict(val_dataset)\n",
        "# preds = np.argmax(predictions.predictions, axis=2)\n",
        "# labels_array = predictions.label_ids\n",
        "\n",
        "# # Convert to strings\n",
        "# true_labels = []\n",
        "# pred_labels = []\n",
        "\n",
        "# for pred, label in zip(preds, labels_array):\n",
        "#     true_seq = []\n",
        "#     pred_seq = []\n",
        "#     for p, l in zip(pred, label):\n",
        "#         if l != -100:\n",
        "#             true_seq.append(id2label[l])\n",
        "#             pred_seq.append(id2label[p])\n",
        "#     if len(true_seq) > 0:\n",
        "#         true_labels.append(true_seq)\n",
        "#         pred_labels.append(pred_seq)\n",
        "\n",
        "# # Show examples with markers\n",
        "# print(\"\\nShowing examples with predicted markers:\")\n",
        "# shown = 0\n",
        "# for i in range(min(10, len(true_labels))):\n",
        "#     # Check if this example has any predicted markers\n",
        "#     if any(label != 'O' for label in pred_labels[i]):\n",
        "#         print(f\"\\nExample {shown + 1}:\")\n",
        "#         print(f\"  True: {' '.join(true_labels[i][:30])}\")\n",
        "#         print(f\"  Pred: {' '.join(pred_labels[i][:30])}\")\n",
        "#         shown += 1\n",
        "#         if shown >= 3:\n",
        "#             break\n",
        "\n",
        "# # Classification report\n",
        "# try:\n",
        "#     print(\"\\n\" + \"=\"*70)\n",
        "#     print(\"CLASSIFICATION REPORT\")\n",
        "#     print(\"=\"*70)\n",
        "#     report = seqeval_report(true_labels, pred_labels, mode='strict', scheme=IOB2, digits=4)\n",
        "#     print(report)\n",
        "# except Exception as e:\n",
        "#     print(f\"Could not generate report: {e}\")\n",
        "\n",
        "# # ===========================\n",
        "# # 13. INTERPRETATION\n",
        "# # ===========================\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"TRAINING COMPLETE!\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# final_f1 = eval_results['eval_overall_f1']\n",
        "\n",
        "# if final_f1 > 0.25:\n",
        "#     print(\"üéâ EXCELLENT! F1 > 0.25 - Top tier performance!\")\n",
        "# elif final_f1 > 0.18:\n",
        "#     print(\"‚úÖ GOOD! F1 > 0.18 - Competitive performance\")\n",
        "# elif final_f1 > 0.12:\n",
        "#     print(\"‚ö†Ô∏è  ACCEPTABLE - F1 > 0.12 - Room for improvement\")\n",
        "# elif final_f1 > 0.05:\n",
        "#     print(\"‚ö†Ô∏è  LOW - Model learning but needs tuning\")\n",
        "# else:\n",
        "#     print(\"‚ùå VERY LOW - Check data alignment and quality\")\n",
        "\n",
        "# print(\"\\nüí° TIPS TO IMPROVE:\")\n",
        "# print(\"   1. Increase max_length to 512 if texts are long\")\n",
        "# print(\"   2. Try DeBERTa-v3-base for better performance\")\n",
        "# print(\"   3. Increase epochs to 15 if still improving\")\n",
        "# print(\"   4. Use ensemble of multiple checkpoints\")\n",
        "# print(\"=\"*70)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T13:23:43.015008Z",
          "iopub.status.idle": "2026-01-26T13:23:43.015248Z",
          "shell.execute_reply.started": "2026-01-26T13:23:43.015129Z",
          "shell.execute_reply": "2026-01-26T13:23:43.015143Z"
        },
        "id": "lrbyIlJGfgeZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # ===========================\n",
        "# # DETAILED EVALUATION\n",
        "# # ===========================\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"EVALUATING MODEL ON VALIDATION SET\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# # Get predictions\n",
        "# predictions = trainer.predict(val_dataset)\n",
        "# preds = np.argmax(predictions.predictions, axis=2)\n",
        "# labels_array = predictions.label_ids\n",
        "\n",
        "# # Convert to BIO sequences\n",
        "# true_labels = []\n",
        "# pred_labels = []\n",
        "\n",
        "# for pred, label in zip(preds, labels_array):\n",
        "#     true_seq = []\n",
        "#     pred_seq = []\n",
        "#     for p, l in zip(pred, label):\n",
        "#         if l != -100:\n",
        "#             true_seq.append(id2label[l])\n",
        "#             pred_seq.append(id2label[p])\n",
        "#     if len(true_seq) > 0:\n",
        "#         true_labels.append(true_seq)\n",
        "#         pred_labels.append(pred_seq)\n",
        "\n",
        "# # Overall metrics\n",
        "# overall_f1 = seqeval_f1(true_labels, pred_labels, mode='strict', scheme=IOB2)\n",
        "# print(f\"\\nüìä Overall F1: {overall_f1:.4f}\")\n",
        "\n",
        "# # Per-marker metrics\n",
        "# print(f\"\\nüìä Per-Marker F1 Scores:\")\n",
        "# for marker_type in MARKER_TYPES:\n",
        "#     filtered_true = []\n",
        "#     filtered_pred = []\n",
        "\n",
        "#     for true_seq, pred_seq in zip(true_labels, pred_labels):\n",
        "#         ft = [t if marker_type in t else 'O' for t in true_seq]\n",
        "#         fp = [p if marker_type in p else 'O' for p in pred_seq]\n",
        "#         filtered_true.append(ft)\n",
        "#         filtered_pred.append(fp)\n",
        "\n",
        "#     if any(any(x != 'O' for x in seq) for seq in filtered_true):\n",
        "#         marker_f1 = seqeval_f1(filtered_true, filtered_pred, mode='strict', scheme=IOB2)\n",
        "#         print(f\"   {marker_type:10s}: {marker_f1:.4f}\")\n",
        "\n",
        "# # Show confusion examples\n",
        "# print(\"\\nüìã Sample Predictions:\")\n",
        "# for i in range(min(5, len(true_labels))):\n",
        "#     if any(l != 'O' for l in true_labels[i][:20]):\n",
        "#         print(f\"\\nExample {i+1}:\")\n",
        "#         print(f\"  True: {' '.join(true_labels[i][:20])}\")\n",
        "#         print(f\"  Pred: {' '.join(pred_labels[i][:20])}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T13:23:43.016518Z",
          "iopub.status.idle": "2026-01-26T13:23:43.016779Z",
          "shell.execute_reply.started": "2026-01-26T13:23:43.016639Z",
          "shell.execute_reply": "2026-01-26T13:23:43.016652Z"
        },
        "id": "fCIWecp2fgea"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# \"\"\"\n",
        "# SUBTASK 1: MARKER EXTRACTION - SUBMISSION GENERATOR\n",
        "# Creates predictions for test set and packages them for submission\n",
        "# \"\"\"\n",
        "# import json\n",
        "# import os\n",
        "# import zipfile\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# # ===========================\n",
        "# # CONFIGURATION\n",
        "# # ===========================\n",
        "# # Paths\n",
        "# INPUT_FILE = \"/kaggle/input/test-reducted/test_rehydrated.jsonl\"  # Update this\n",
        "# MODEL_PATH = \"./marker_results/checkpoint-best\"\n",
        "# OUTPUT_DIR = \"/kaggle/working\"\n",
        "# TEMP_SUBMISSION_FILE = os.path.join(OUTPUT_DIR, \"submission.jsonl\")\n",
        "# OUTPUT_ZIP = os.path.join(OUTPUT_DIR, \"submission.zip\")\n",
        "\n",
        "# # Marker types\n",
        "# MARKER_TYPES = [\"Actor\", \"Action\", \"Effect\", \"Evidence\", \"Victim\"]\n",
        "\n",
        "# # Labels\n",
        "# labels_list = [\"O\"]\n",
        "# for marker_type in MARKER_TYPES:\n",
        "#     labels_list.append(f\"B-{marker_type}\")\n",
        "#     labels_list.append(f\"I-{marker_type}\")\n",
        "\n",
        "# label2id = {label: i for i, label in enumerate(labels_list)}\n",
        "# id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "# print(\"=\"*70)\n",
        "# print(\"MARKER EXTRACTION - SUBMISSION GENERATOR\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# # ===========================\n",
        "# # LOAD MODEL\n",
        "# # ===========================\n",
        "# def load_model():\n",
        "#     \"\"\"Load trained marker extraction model\"\"\"\n",
        "#     print(\"\\nüì¶ Loading model and tokenizer...\")\n",
        "\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     print(f\"   Device: {device}\")\n",
        "\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", add_prefix_space=True)\n",
        "\n",
        "#     model = AutoModelForTokenClassification.from_pretrained(\n",
        "#         MODEL_PATH,\n",
        "#         num_labels=len(labels_list),\n",
        "#         id2label=id2label,\n",
        "#         label2id=label2id\n",
        "#     )\n",
        "#     model.to(device)\n",
        "#     model.eval()\n",
        "\n",
        "#     print(\"‚úÖ Model loaded successfully!\")\n",
        "#     return tokenizer, model, device\n",
        "\n",
        "# # ===========================\n",
        "# # PREDICTION FUNCTION\n",
        "# # ===========================\n",
        "# def predict_markers(text, tokenizer, model, device):\n",
        "#     \"\"\"\n",
        "#     Predict markers for a single text\n",
        "\n",
        "#     Returns:\n",
        "#         List of marker dictionaries with startIndex, endIndex, type, text\n",
        "#     \"\"\"\n",
        "#     # Tokenize with offset mapping\n",
        "#     encoding = tokenizer(\n",
        "#         text,\n",
        "#         truncation=True,\n",
        "#         max_length=512,\n",
        "#         padding='max_length',\n",
        "#         return_offsets_mapping=True,\n",
        "#         return_tensors='pt'\n",
        "#     )\n",
        "\n",
        "#     offset_mapping = encoding['offset_mapping'][0].numpy()\n",
        "\n",
        "#     # Move to device\n",
        "#     input_ids = encoding['input_ids'].to(device)\n",
        "#     attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "#     # Predict\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "#         predictions = torch.argmax(outputs.logits, dim=2)[0].cpu().numpy()\n",
        "\n",
        "#     # Convert predictions to BIO tags\n",
        "#     predicted_labels = []\n",
        "#     for pred_id, (token_start, token_end) in zip(predictions, offset_mapping):\n",
        "#         if token_start == 0 and token_end == 0:  # Special token\n",
        "#             continue\n",
        "#         predicted_labels.append(id2label[pred_id])\n",
        "\n",
        "#     # Extract marker spans from BIO tags\n",
        "#     markers = []\n",
        "#     current_marker = None\n",
        "#     current_start = None\n",
        "#     current_end = None\n",
        "\n",
        "#     for idx, label in enumerate(predicted_labels):\n",
        "#         token_start, token_end = offset_mapping[idx]\n",
        "\n",
        "#         if token_start == 0 and token_end == 0:\n",
        "#             continue\n",
        "\n",
        "#         if label.startswith('B-'):\n",
        "#             # Save previous marker if exists\n",
        "#             if current_marker is not None:\n",
        "#                 markers.append({\n",
        "#                     'startIndex': int(current_start),  # Convert to Python int\n",
        "#                     'endIndex': int(current_end),      # Convert to Python int\n",
        "#                     'type': current_marker,\n",
        "#                     'text': text[current_start:current_end]\n",
        "#                 })\n",
        "\n",
        "#             # Start new marker\n",
        "#             marker_type = label[2:]  # Remove 'B-'\n",
        "#             current_marker = marker_type\n",
        "#             current_start = int(token_start)  # Convert to Python int\n",
        "#             current_end = int(token_end)      # Convert to Python int\n",
        "\n",
        "#         elif label.startswith('I-') and current_marker is not None:\n",
        "#             # Continue current marker\n",
        "#             marker_type = label[2:]\n",
        "#             if marker_type == current_marker:\n",
        "#                 current_end = int(token_end)  # Convert to Python int\n",
        "#             else:\n",
        "#                 # Type mismatch, save current and start new\n",
        "#                 markers.append({\n",
        "#                     'startIndex': int(current_start),\n",
        "#                     'endIndex': int(current_end),\n",
        "#                     'type': current_marker,\n",
        "#                     'text': text[current_start:current_end]\n",
        "#                 })\n",
        "#                 current_marker = marker_type\n",
        "#                 current_start = int(token_start)\n",
        "#                 current_end = int(token_end)\n",
        "\n",
        "#         elif label == 'O':\n",
        "#             # End current marker\n",
        "#             if current_marker is not None:\n",
        "#                 markers.append({\n",
        "#                     'startIndex': int(current_start),\n",
        "#                     'endIndex': int(current_end),\n",
        "#                     'type': current_marker,\n",
        "#                     'text': text[current_start:current_end]\n",
        "#                 })\n",
        "#                 current_marker = None\n",
        "\n",
        "#     # Don't forget last marker\n",
        "#     if current_marker is not None:\n",
        "#         markers.append({\n",
        "#             'startIndex': int(current_start),\n",
        "#             'endIndex': int(current_end),\n",
        "#             'type': current_marker,\n",
        "#             'text': text[current_start:current_end]\n",
        "#         })\n",
        "\n",
        "#     return markers\n",
        "\n",
        "# # ===========================\n",
        "# # PROCESS DOCUMENTS\n",
        "# # ===========================\n",
        "# def process_document(item, tokenizer, model, device):\n",
        "#     \"\"\"Process a single document\"\"\"\n",
        "#     doc_id = item.get('_id')\n",
        "#     text = item.get('text', '')\n",
        "\n",
        "#     if not doc_id or not text:\n",
        "#         return None\n",
        "\n",
        "#     # Predict markers\n",
        "#     markers = predict_markers(text, tokenizer, model, device)\n",
        "\n",
        "#     # Return in submission format\n",
        "#     return {\n",
        "#         \"_id\": doc_id,\n",
        "#         \"markers\": markers\n",
        "#     }\n",
        "\n",
        "# # ===========================\n",
        "# # MAIN EXECUTION\n",
        "# # ===========================\n",
        "# def main():\n",
        "#     print(\"\\n\" + \"=\"*70)\n",
        "#     print(\"STARTING PREDICTION PIPELINE\")\n",
        "#     print(\"=\"*70)\n",
        "\n",
        "#     # Load model\n",
        "#     tokenizer, model, device = load_model()\n",
        "\n",
        "#     # Load input data\n",
        "#     print(f\"\\nüìÇ Loading test data from: {INPUT_FILE}\")\n",
        "\n",
        "#     if not os.path.exists(INPUT_FILE):\n",
        "#         print(f\"‚ùå Error: File not found: {INPUT_FILE}\")\n",
        "#         return\n",
        "\n",
        "#     input_data = []\n",
        "#     with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
        "#         for line in f:\n",
        "#             try:\n",
        "#                 input_data.append(json.loads(line))\n",
        "#             except json.JSONDecodeError:\n",
        "#                 continue\n",
        "\n",
        "#     print(f\"‚úÖ Loaded {len(input_data)} documents\")\n",
        "\n",
        "#     # Process all documents\n",
        "#     print(f\"\\nüîÑ Generating predictions...\")\n",
        "#     predictions = []\n",
        "\n",
        "#     for item in tqdm(input_data, desc=\"Processing\"):\n",
        "#         pred = process_document(item, tokenizer, model, device)\n",
        "#         if pred:\n",
        "#             predictions.append(pred)\n",
        "\n",
        "#     print(f\"\\n‚úÖ Generated predictions for {len(predictions)} documents\")\n",
        "\n",
        "#     # Calculate statistics\n",
        "#     total_markers = sum(len(pred['markers']) for pred in predictions)\n",
        "#     print(f\"\\nüìä PREDICTION STATISTICS:\")\n",
        "#     print(f\"   Total markers predicted: {total_markers}\")\n",
        "#     print(f\"   Avg markers per document: {total_markers / len(predictions):.2f}\")\n",
        "\n",
        "#     # Marker type distribution\n",
        "#     marker_counts = {mt: 0 for mt in MARKER_TYPES}\n",
        "#     for pred in predictions:\n",
        "#         for marker in pred['markers']:\n",
        "#             marker_type = marker['type']\n",
        "#             if marker_type in marker_counts:\n",
        "#                 marker_counts[marker_type] += 1\n",
        "\n",
        "#     print(f\"\\nüìä Marker Type Distribution:\")\n",
        "#     for marker_type, count in sorted(marker_counts.items(), key=lambda x: -x[1]):\n",
        "#         percentage = (count / total_markers * 100) if total_markers > 0 else 0\n",
        "#         print(f\"   {marker_type:10s}: {count:5d} ({percentage:5.2f}%)\")\n",
        "\n",
        "#     # Save to JSONL\n",
        "#     print(f\"\\nüíæ Saving predictions to {TEMP_SUBMISSION_FILE}...\")\n",
        "#     with open(TEMP_SUBMISSION_FILE, 'w', encoding='utf-8') as f:\n",
        "#         for pred in predictions:\n",
        "#             f.write(json.dumps(pred) + '\\n')\n",
        "\n",
        "#     # Create ZIP\n",
        "#     print(f\"üì¶ Creating ZIP archive: {OUTPUT_ZIP}...\")\n",
        "#     with zipfile.ZipFile(OUTPUT_ZIP, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
        "#         zf.write(TEMP_SUBMISSION_FILE, arcname=\"submission.jsonl\")\n",
        "\n",
        "#     # Clean up\n",
        "#     os.remove(TEMP_SUBMISSION_FILE)\n",
        "\n",
        "#     file_size = os.path.getsize(OUTPUT_ZIP) / 1024\n",
        "#     print(f\"\\n‚úÖ SUCCESS!\")\n",
        "#     print(f\"   Submission file: {OUTPUT_ZIP}\")\n",
        "#     print(f\"   File size: {file_size:.2f} KB\")\n",
        "\n",
        "#     # Show sample predictions\n",
        "#     print(f\"\\nüìã SAMPLE PREDICTIONS (first 3 documents):\")\n",
        "#     for i, pred in enumerate(predictions[:3]):\n",
        "#         print(f\"\\nDocument {i+1} (ID: {pred['_id']}):\")\n",
        "#         if pred['markers']:\n",
        "#             for marker in pred['markers'][:5]:  # Show first 5 markers\n",
        "#                 print(f\"   [{marker['type']:8s}] \\\"{marker['text'][:50]}...\\\"\")\n",
        "#         else:\n",
        "#             print(\"   (No markers predicted)\")\n",
        "\n",
        "#     print(\"\\n\" + \"=\"*70)\n",
        "#     print(\"SUBMISSION READY FOR UPLOAD!\")\n",
        "#     print(\"=\"*70)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T13:23:43.017821Z",
          "iopub.status.idle": "2026-01-26T13:23:43.018095Z",
          "shell.execute_reply.started": "2026-01-26T13:23:43.017966Z",
          "shell.execute_reply": "2026-01-26T13:23:43.017987Z"
        },
        "id": "duQwl07Ufgea"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import numpy as np\n",
        "# from datasets import Dataset\n",
        "# from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "# from sklearn.utils.class_weight import compute_class_weight\n",
        "# from sklearn.metrics import f1_score, accuracy_score"
      ],
      "metadata": {
        "_uuid": "1c930860-fe54-4894-8a13-cec2b7ea7641",
        "_cell_guid": "06a43554-8945-40c3-88d5-574e4fbea80f",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T13:23:43.019036Z",
          "iopub.status.idle": "2026-01-26T13:23:43.019342Z",
          "shell.execute_reply.started": "2026-01-26T13:23:43.019213Z",
          "shell.execute_reply": "2026-01-26T13:23:43.019235Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "qEy1gqjbfgea"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# from datasets import Dataset\n",
        "# from transformers import (\n",
        "#     AutoTokenizer,\n",
        "#     AutoModelForSequenceClassification,\n",
        "#     Trainer,\n",
        "#     TrainingArguments\n",
        "# )\n",
        "# from sklearn.utils.class_weight import compute_class_weight\n",
        "# from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
        "\n",
        "# # ===========================\n",
        "# # 1. BINARY LABEL MAPPING\n",
        "# # ===========================\n",
        "# # ‚úÖ Remove \"Can't tell\" - it's NOT a prediction class!\n",
        "# label2id = {\"No\": 0, \"Yes\": 1}\n",
        "# id2label = {0: \"No\", 1: \"Yes\"}\n",
        "\n",
        "# # ===========================\n",
        "# # 2. LOAD DATA - BINARY ONLY\n",
        "# # ===========================\n",
        "# def make_marker_text(text, markers):\n",
        "#     \"\"\"Wrap marker spans in special tokens.\"\"\"\n",
        "#     offset = 0\n",
        "#     for m in sorted(markers, key=lambda x: x['startIndex']):\n",
        "#         start = m['startIndex'] + offset\n",
        "#         end = m['endIndex'] + offset\n",
        "#         token_start = f\"[{m['type'].upper()}]\"\n",
        "#         token_end = f\"[/{m['type'].upper()}]\"\n",
        "#         text = text[:start] + token_start + text[start:end] + token_end + text[end:]\n",
        "#         offset += len(token_start) + len(token_end)\n",
        "#     return text\n",
        "\n",
        "\n",
        "# def load_jsonl_binary(path):\n",
        "#     \"\"\"Load JSONL and convert to BINARY classification.\"\"\"\n",
        "#     texts, labels, raw_markers = [], [], []\n",
        "#     skipped_cant_tell = 0\n",
        "#     skipped_other = 0\n",
        "\n",
        "#     with open(path, encoding=\"utf-8\") as f:\n",
        "#         for line in f:\n",
        "#             item = json.loads(line)\n",
        "#             label = item.get(\"conspiracy\")\n",
        "\n",
        "#             # ‚úÖ SKIP \"Can't tell\" - not a valid class for prediction!\n",
        "#             if label == \"Can't tell\":\n",
        "#                 skipped_cant_tell += 1\n",
        "#                 continue\n",
        "\n",
        "#             # ‚úÖ Only keep \"Yes\" and \"No\"\n",
        "#             if label not in label2id:\n",
        "#                 skipped_other += 1\n",
        "#                 continue\n",
        "\n",
        "#             # Marker-aware text\n",
        "#             text_marker = make_marker_text(item[\"text\"], item.get(\"markers\", []))\n",
        "#             texts.append(text_marker)\n",
        "#             labels.append(label2id[label])\n",
        "#             raw_markers.append(item.get(\"markers\", []))\n",
        "\n",
        "#     print(f\"Loaded {len(texts)} samples from {path}\")\n",
        "#     print(f\"  - Skipped 'Can't tell': {skipped_cant_tell}\")\n",
        "#     print(f\"  - Skipped other: {skipped_other}\")\n",
        "\n",
        "#     return Dataset.from_dict({\n",
        "#         \"text\": texts,\n",
        "#         \"label\": labels,\n",
        "#         \"markers\": raw_markers\n",
        "#     })\n",
        "\n",
        "\n",
        "# # Load datasets\n",
        "# print(\"=\"*70)\n",
        "# print(\"LOADING BINARY CLASSIFICATION DATA\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# train_ds = load_jsonl_binary(\"train_rehydrated.jsonl\")\n",
        "# dev_ds_raw = load_jsonl_binary(\"dev_rehydrated.jsonl\")\n",
        "\n",
        "# # Fallback if dev empty\n",
        "# if len(dev_ds_raw) == 0:\n",
        "#     print(\"\\n‚ö†Ô∏è  Dev set empty, splitting train set (90/10)\")\n",
        "#     split = train_ds.train_test_split(test_size=0.1, seed=42)\n",
        "#     train_ds = split[\"train\"]\n",
        "#     dev_ds = split[\"test\"]\n",
        "# else:\n",
        "#     dev_ds = dev_ds_raw\n",
        "\n",
        "# # ===========================\n",
        "# # 3. TOKENIZER SETUP\n",
        "# # ===========================\n",
        "# model_name = \"roberta-base\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# # Add special tokens for markers\n",
        "# special_tokens = [\n",
        "#     \"[EVIDENCE]\", \"[/EVIDENCE]\",\n",
        "#     \"[ACTOR]\", \"[/ACTOR]\",\n",
        "#     \"[ACTION]\", \"[/ACTION]\",\n",
        "#     \"[VICTIM]\", \"[/VICTIM]\",\n",
        "#     \"[EFFECT]\", \"[/EFFECT]\"\n",
        "# ]\n",
        "# tokenizer.add_tokens(special_tokens)\n",
        "\n",
        "\n",
        "# def tokenize_fn(examples):\n",
        "#     return tokenizer(\n",
        "#         examples[\"text\"],\n",
        "#         padding=\"max_length\",\n",
        "#         truncation=True,\n",
        "#         max_length=256\n",
        "#     )\n",
        "\n",
        "\n",
        "# train_ds = train_ds.map(tokenize_fn, batched=True)\n",
        "# dev_ds = dev_ds.map(tokenize_fn, batched=True)\n",
        "\n",
        "# train_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "# dev_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "# # ===========================\n",
        "# # 4. CLASS WEIGHTS\n",
        "# # ===========================\n",
        "# labels = np.array(train_ds[\"label\"])\n",
        "# class_weights = compute_class_weight(\n",
        "#     class_weight=\"balanced\",\n",
        "#     classes=np.unique(labels),\n",
        "#     y=labels\n",
        "# )\n",
        "\n",
        "# # Apply stronger weighting for minority class\n",
        "# class_weights = class_weights ** 1.3  # Moderate boost\n",
        "# class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
        "\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"CLASS DISTRIBUTION & WEIGHTS\")\n",
        "# print(\"=\"*70)\n",
        "# print(f\"Class weights: {class_weights}\")\n",
        "\n",
        "# unique, counts = np.unique(labels, return_counts=True)\n",
        "# print(\"\\nTraining set distribution:\")\n",
        "# for label_id, count in zip(unique, counts):\n",
        "#     pct = count / len(labels) * 100\n",
        "#     print(f\"  {id2label[label_id]:5s}: {count:5d} ({pct:5.2f}%)\")\n",
        "\n",
        "# # ===========================\n",
        "# # 5. MODEL\n",
        "# # ===========================\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"INITIALIZING MODEL\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(\n",
        "#     model_name,\n",
        "#     num_labels=2,  # ‚úÖ BINARY\n",
        "#     id2label=id2label,\n",
        "#     label2id=label2id\n",
        "# )\n",
        "# model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# # ===========================\n",
        "# # 6. WEIGHTED TRAINER\n",
        "# # ===========================\n",
        "# class WeightedTrainer(Trainer):\n",
        "#     def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "#         labels = inputs.pop(\"labels\")\n",
        "#         outputs = model(**inputs)\n",
        "#         logits = outputs.logits\n",
        "#         loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights.to(logits.device))\n",
        "#         loss = loss_fn(logits, labels)\n",
        "#         return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# # ===========================\n",
        "# # 7. METRICS\n",
        "# # ===========================\n",
        "# def compute_metrics(eval_pred):\n",
        "#     logits, labels = eval_pred\n",
        "#     preds = np.argmax(logits, axis=1)\n",
        "\n",
        "#     acc = accuracy_score(labels, preds)\n",
        "#     f1_macro = f1_score(labels, preds, average=\"macro\")\n",
        "#     f1_binary = f1_score(labels, preds, average=\"binary\", pos_label=1)  # \"Yes\" is positive\n",
        "\n",
        "#     # Count predictions per class\n",
        "#     unique, counts = np.unique(preds, return_counts=True)\n",
        "#     pred_dist = {id2label[u]: c for u, c in zip(unique, counts)}\n",
        "\n",
        "#     print(f\"\\nEval Predictions: {pred_dist}\")\n",
        "\n",
        "#     return {\n",
        "#         \"accuracy\": acc,\n",
        "#         \"macro_f1\": f1_macro,\n",
        "#         \"binary_f1\": f1_binary  # F1 for \"Yes\" class\n",
        "#     }\n",
        "\n",
        "# # ===========================\n",
        "# # 8. TRAINING ARGS\n",
        "# # ===========================\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"./results\",\n",
        "#     eval_strategy=\"epoch\",\n",
        "#     save_strategy=\"epoch\",\n",
        "#     learning_rate=2e-5,\n",
        "#     per_device_train_batch_size=16,\n",
        "#     per_device_eval_batch_size=16,\n",
        "#     num_train_epochs=4,  # Moderate epochs\n",
        "#     weight_decay=0.01,\n",
        "#     warmup_ratio=0.1,  # Warmup for 10% of training\n",
        "#     load_best_model_at_end=True,\n",
        "#     metric_for_best_model=\"macro_f1\",\n",
        "#     greater_is_better=True,\n",
        "#     logging_steps=50,\n",
        "#     report_to=\"none\",\n",
        "#     label_smoothing_factor=0.05,  # Light smoothing\n",
        "#     save_total_limit=2,\n",
        "#     fp16=True  # Mixed precision for faster training\n",
        "# )\n",
        "\n",
        "# # ===========================\n",
        "# # 9. TRAIN\n",
        "# # ===========================\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"STARTING TRAINING\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# trainer = WeightedTrainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=train_ds,\n",
        "#     eval_dataset=dev_ds,\n",
        "#     tokenizer=tokenizer,\n",
        "#     compute_metrics=compute_metrics\n",
        "# )\n",
        "\n",
        "# trainer.train()\n",
        "\n",
        "# # ===========================\n",
        "# # 10. SAVE BEST MODEL\n",
        "# # ===========================\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"SAVING BEST MODEL\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# trainer.save_model(\"./results/checkpoint-best\")\n",
        "# tokenizer.save_pretrained(\"./results/checkpoint-best\")\n",
        "# print(\"‚úì Model saved to ./results/checkpoint-best\")\n",
        "\n",
        "# # ===========================\n",
        "# # 11. FINAL EVALUATION\n",
        "# # ===========================\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"FINAL EVALUATION\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# eval_results = trainer.evaluate()\n",
        "# print(f\"\\nAccuracy:   {eval_results['eval_accuracy']:.4f}\")\n",
        "# print(f\"Macro F1:   {eval_results['eval_macro_f1']:.4f}\")\n",
        "# print(f\"Binary F1:  {eval_results['eval_binary_f1']:.4f}\")\n",
        "\n",
        "# # Detailed classification report\n",
        "# predictions = trainer.predict(dev_ds)\n",
        "# pred_labels = np.argmax(predictions.predictions, axis=1)\n",
        "# true_labels = predictions.label_ids\n",
        "\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"CLASSIFICATION REPORT\")\n",
        "# print(\"=\"*70)\n",
        "# print(classification_report(\n",
        "#     true_labels,\n",
        "#     pred_labels,\n",
        "#     target_names=[\"No\", \"Yes\"],\n",
        "#     digits=4\n",
        "# ))\n",
        "\n",
        "# # Check prediction distribution\n",
        "# unique_pred, counts_pred = np.unique(pred_labels, return_counts=True)\n",
        "# print(\"\\nPrediction Distribution:\")\n",
        "# for u, c in zip(unique_pred, counts_pred):\n",
        "#     pct = c / len(pred_labels) * 100\n",
        "#     print(f\"  {id2label[u]:5s}: {c:5d} ({pct:5.2f}%)\")\n",
        "\n",
        "# # Warning if heavily biased\n",
        "# yes_pct = (np.sum(pred_labels == 1) / len(pred_labels)) * 100\n",
        "# if yes_pct < 10:\n",
        "#     print(f\"\\n‚ö†Ô∏è  WARNING: Only {yes_pct:.1f}% 'Yes' predictions!\")\n",
        "#     print(\"   Model is heavily biased toward 'No'\")\n",
        "#     print(\"   Consider:\")\n",
        "#     print(\"   - Increasing class weights more\")\n",
        "#     print(\"   - Using focal loss\")\n",
        "#     print(\"   - Data augmentation for 'Yes' class\")\n",
        "# elif yes_pct > 90:\n",
        "#     print(f\"\\n‚ö†Ô∏è  WARNING: {yes_pct:.1f}% 'Yes' predictions!\")\n",
        "#     print(\"   Model is heavily biased toward 'Yes'\")\n",
        "# else:\n",
        "#     print(f\"\\n‚úÖ Reasonable balance: {yes_pct:.1f}% 'Yes' predictions\")\n",
        "\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"TRAINING COMPLETE!\")\n",
        "# print(\"=\"*70)"
      ],
      "metadata": {
        "_uuid": "09d56527-f9ee-4942-a05a-014f4d10dd3c",
        "_cell_guid": "99e38c37-3174-4a15-8ac2-947dadb02fdc",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T13:23:43.020667Z",
          "iopub.status.idle": "2026-01-26T13:23:43.021013Z",
          "shell.execute_reply.started": "2026-01-26T13:23:43.020823Z",
          "shell.execute_reply": "2026-01-26T13:23:43.020848Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "yt481Cy_fgea"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "# import os\n",
        "# import zipfile\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# from typing import List, Dict, Any\n",
        "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# # --- Configuration ---\n",
        "# INPUT_FILE = \"/kaggle/input/test-reducted/test_rehydrated.jsonl\"\n",
        "# OUTPUT_DIR = \"/kaggle/working\"\n",
        "# TEMP_SUBMISSION_FILE = os.path.join(OUTPUT_DIR, \"submission.jsonl\")\n",
        "# OUTPUT_ZIP = os.path.join(OUTPUT_DIR, \"submission.zip\")\n",
        "# MODEL_PATH = \"./results/checkpoint-best\"\n",
        "# MODEL_NAME = \"roberta-base\"\n",
        "\n",
        "# # ‚úÖ BINARY LABELS ONLY (No \"Can't tell\" in predictions!)\n",
        "# id2label = {0: \"No\", 1: \"Yes\"}\n",
        "# label2id = {\"No\": 0, \"Yes\": 1}\n",
        "\n",
        "# def make_marker_text(text: str, markers: List[Dict]) -> str:\n",
        "#     \"\"\"Wrap marker spans in special tokens for marker-awareness.\"\"\"\n",
        "#     if not markers:\n",
        "#         return text\n",
        "\n",
        "#     offset = 0\n",
        "#     for m in sorted(markers, key=lambda x: x['startIndex']):\n",
        "#         start = m['startIndex'] + offset\n",
        "#         end = m['endIndex'] + offset\n",
        "#         token_start = f\"[{m['type'].upper()}]\"\n",
        "#         token_end = f\"[/{m['type'].upper()}]\"\n",
        "#         text = text[:start] + token_start + text[start:end] + token_end + text[end:]\n",
        "#         offset += len(token_start) + len(token_end)\n",
        "#     return text\n",
        "\n",
        "\n",
        "# def load_model():\n",
        "#     \"\"\"Load the trained conspiracy classification model.\"\"\"\n",
        "#     print(\"Loading model and tokenizer...\")\n",
        "\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     print(f\"Using device: {device}\")\n",
        "\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "#     # Add special tokens\n",
        "#     special_tokens = [\"[EVIDENCE]\", \"[/EVIDENCE]\", \"[ACTOR]\", \"[/ACTOR]\",\n",
        "#                       \"[ACTION]\", \"[/ACTION]\", \"[VICTIM]\", \"[/VICTIM]\",\n",
        "#                       \"[EFFECT]\", \"[/EFFECT]\"]\n",
        "#     tokenizer.add_tokens(special_tokens)\n",
        "\n",
        "#     # Load model with BINARY labels\n",
        "#     model = AutoModelForSequenceClassification.from_pretrained(\n",
        "#         MODEL_PATH,\n",
        "#         num_labels=2,  # ‚úÖ BINARY ONLY\n",
        "#         id2label=id2label,\n",
        "#         label2id=label2id,\n",
        "#         ignore_mismatched_sizes=True  # Allow loading 3-class model as 2-class\n",
        "#     )\n",
        "#     model.to(device)\n",
        "#     model.eval()\n",
        "\n",
        "#     print(\"Model loaded successfully!\")\n",
        "#     return tokenizer, model, device\n",
        "\n",
        "\n",
        "# def predict_conspiracy(text: str, tokenizer, model, device,\n",
        "#                        markers: List[Dict] = None,\n",
        "#                        threshold: float = 0.5) -> str:\n",
        "#     \"\"\"\n",
        "#     Predict conspiracy label using the trained model.\n",
        "#     Uses probability threshold for better calibration.\n",
        "\n",
        "#     Returns: \"Yes\" or \"No\" (NEVER \"Can't tell\")\n",
        "#     \"\"\"\n",
        "#     # Apply marker-aware formatting if markers provided\n",
        "#     if markers:\n",
        "#         text = make_marker_text(text, markers)\n",
        "\n",
        "#     # Tokenize\n",
        "#     inputs = tokenizer(\n",
        "#         text,\n",
        "#         padding=\"max_length\",\n",
        "#         truncation=True,\n",
        "#         max_length=256,\n",
        "#         return_tensors=\"pt\"\n",
        "#     )\n",
        "\n",
        "#     inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "#     # Predict with probability scores\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model(**inputs)\n",
        "#         logits = outputs.logits\n",
        "#         probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "\n",
        "#     # Use threshold for better calibration\n",
        "#     # If prob(Yes) > threshold, predict \"Yes\", else \"No\"\n",
        "#     yes_prob = probs[1] if len(probs) > 1 else probs[0]\n",
        "\n",
        "#     if yes_prob > threshold:\n",
        "#         return \"Yes\"\n",
        "#     else:\n",
        "#         return \"No\"\n",
        "\n",
        "\n",
        "# def predict_markers(doc_id: str, text: str) -> List[Dict[str, Any]]:\n",
        "#     \"\"\"\n",
        "#     Placeholder for marker prediction.\n",
        "#     For Subtask 2 (detection only), return empty list.\n",
        "#     \"\"\"\n",
        "#     return []\n",
        "\n",
        "\n",
        "# def process_document(item: Dict[str, Any], tokenizer, model, device) -> Dict[str, Any]:\n",
        "#     \"\"\"Process a single document and generate prediction.\"\"\"\n",
        "\n",
        "#     doc_id = item.get('_id')\n",
        "#     text = item.get('text', '')\n",
        "\n",
        "#     if not doc_id or not text:\n",
        "#         return None\n",
        "\n",
        "#     markers = item.get('markers', [])\n",
        "\n",
        "#     # Predict conspiracy label (BINARY: Yes or No)\n",
        "#     conspiracy_pred = predict_conspiracy(\n",
        "#         text,\n",
        "#         tokenizer,\n",
        "#         model,\n",
        "#         device,\n",
        "#         markers if markers else None,\n",
        "#         threshold=0.4  # Adjust threshold if needed (lower = more \"Yes\" predictions)\n",
        "#     )\n",
        "\n",
        "#     # Predict markers (empty for detection-only submission)\n",
        "#     marker_preds = predict_markers(doc_id, text)\n",
        "\n",
        "#     return {\n",
        "#         \"_id\": doc_id,\n",
        "#         \"conspiracy\": conspiracy_pred,  # ‚úÖ Always \"Yes\" or \"No\"\n",
        "#         \"markers\": marker_preds\n",
        "#     }\n",
        "\n",
        "\n",
        "# def load_jsonl(file_path: str) -> List[Dict[str, Any]]:\n",
        "#     \"\"\"Load JSONL file.\"\"\"\n",
        "#     data = []\n",
        "\n",
        "#     if not os.path.exists(file_path):\n",
        "#         raise FileNotFoundError(f\"Input file not found: {file_path}\")\n",
        "\n",
        "#     print(f\"Reading data from {file_path}...\")\n",
        "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
        "#         for line_num, line in enumerate(f, 1):\n",
        "#             try:\n",
        "#                 data.append(json.loads(line.strip()))\n",
        "#             except json.JSONDecodeError:\n",
        "#                 print(f\"Warning: Skipping invalid JSON at line {line_num}\")\n",
        "\n",
        "#     print(f\"Loaded {len(data)} documents\")\n",
        "#     return data\n",
        "\n",
        "\n",
        "# def save_and_zip(data: List[Dict], temp_file: str, output_zip: str):\n",
        "#     \"\"\"Save predictions to JSONL and create ZIP.\"\"\"\n",
        "\n",
        "#     print(f\"Saving {len(data)} predictions...\")\n",
        "#     with open(temp_file, 'w', encoding='utf-8') as f:\n",
        "#         for item in data:\n",
        "#             f.write(json.dumps(item) + '\\n')\n",
        "\n",
        "#     print(f\"Creating ZIP: {output_zip}...\")\n",
        "#     with zipfile.ZipFile(OUTPUT_ZIP, 'w', compression=zipfile.ZIP_STORED) as zf:\n",
        "\n",
        "#         zf.write(temp_file, arcname=\"submission.jsonl\")\n",
        "\n",
        "#     os.remove(temp_file)\n",
        "\n",
        "#     print(f\"‚úì Submission created: {output_zip}\")\n",
        "#     print(f\"  File size: {os.path.getsize(output_zip) / 1024:.2f} KB\")\n",
        "\n",
        "\n",
        "# def main():\n",
        "#     \"\"\"Main execution.\"\"\"\n",
        "\n",
        "#     print(\"=\"*70)\n",
        "#     print(\"SEMEVAL 2026 TASK 10 - CONSPIRACY DETECTION SUBMISSION\")\n",
        "#     print(\"=\"*70)\n",
        "\n",
        "#     # Load model\n",
        "#     tokenizer, model, device = load_model()\n",
        "\n",
        "#     # Load input data\n",
        "#     input_data = load_jsonl(INPUT_FILE)\n",
        "\n",
        "#     # Generate predictions\n",
        "#     print(f\"\\nGenerating predictions for {len(input_data)} documents...\")\n",
        "#     final_submission = []\n",
        "\n",
        "#     for i, item in enumerate(input_data):\n",
        "#         if (i + 1) % 100 == 0:\n",
        "#             print(f\"  Progress: {i + 1}/{len(input_data)}\")\n",
        "\n",
        "#         prediction = process_document(item, tokenizer, model, device)\n",
        "#         if prediction:\n",
        "#             final_submission.append(prediction)\n",
        "\n",
        "#     print(f\"\\n‚úì Generated {len(final_submission)} predictions\")\n",
        "\n",
        "#     # Validate predictions\n",
        "#     print(\"\\n\" + \"=\"*70)\n",
        "#     print(\"VALIDATION CHECK\")\n",
        "#     print(\"=\"*70)\n",
        "\n",
        "#     labels = [doc['conspiracy'] for doc in final_submission]\n",
        "\n",
        "#     # Check for invalid labels\n",
        "#     invalid_labels = [l for l in labels if l not in [\"Yes\", \"No\"]]\n",
        "#     if invalid_labels:\n",
        "#         print(f\"‚ùå ERROR: Found {len(invalid_labels)} invalid labels!\")\n",
        "#         print(f\"   Invalid labels: {set(invalid_labels)}\")\n",
        "#         return\n",
        "#     else:\n",
        "#         print(\"‚úÖ All labels are valid (Yes/No only)\")\n",
        "\n",
        "#     # Show distribution\n",
        "#     print(\"\\nPrediction Distribution:\")\n",
        "#     for label in [\"Yes\", \"No\"]:\n",
        "#         count = labels.count(label)\n",
        "#         percentage = (count / len(labels) * 100) if labels else 0\n",
        "#         print(f\"  {label:5s}: {count:5d} ({percentage:5.2f}%)\")\n",
        "\n",
        "#     # Check if model is too biased\n",
        "#     yes_count = labels.count(\"Yes\")\n",
        "#     yes_pct = (yes_count / len(labels) * 100) if labels else 0\n",
        "\n",
        "#     if yes_pct < 5:\n",
        "#         print(f\"\\n‚ö†Ô∏è  WARNING: Only {yes_pct:.1f}% 'Yes' predictions!\")\n",
        "#         print(\"   Your model may be too biased. Consider:\")\n",
        "#         print(\"   - Lowering threshold (currently 0.4)\")\n",
        "#         print(\"   - Retraining with different class weights\")\n",
        "#         print(\"   - Using a different model architecture\")\n",
        "#     elif yes_pct > 95:\n",
        "#         print(f\"\\n‚ö†Ô∏è  WARNING: {yes_pct:.1f}% 'Yes' predictions!\")\n",
        "#         print(\"   Your model may be overpredicting. Consider:\")\n",
        "#         print(\"   - Raising threshold\")\n",
        "#         print(\"   - Checking training data balance\")\n",
        "#     else:\n",
        "#         print(f\"\\n‚úÖ Prediction distribution looks reasonable ({yes_pct:.1f}% Yes)\")\n",
        "\n",
        "#     # Save submission\n",
        "#     print(\"\\n\" + \"=\"*70)\n",
        "#     save_and_zip(final_submission, TEMP_SUBMISSION_FILE, OUTPUT_ZIP)\n",
        "\n",
        "#     print(\"\\n\" + \"=\"*70)\n",
        "#     print(\"SUBMISSION READY FOR CODABENCH!\")\n",
        "#     print(f\"File: {OUTPUT_ZIP}\")\n",
        "#     print(\"=\"*70)\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ],
      "metadata": {
        "_uuid": "85976a40-004f-4e6f-9fa3-eeca98d5768a",
        "_cell_guid": "1278fe2d-423e-4245-811b-b1981bd058a5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T13:23:43.022303Z",
          "iopub.status.idle": "2026-01-26T13:23:43.022537Z",
          "shell.execute_reply.started": "2026-01-26T13:23:43.022427Z",
          "shell.execute_reply": "2026-01-26T13:23:43.022441Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "E6zw5SMdfgea"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Check what your model actually learned\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "# import numpy as np\n",
        "\n",
        "# # Get predictions on dev set\n",
        "# predictions = trainer.predict(dev_ds)\n",
        "# pred_labels = np.argmax(predictions.predictions, axis=1)\n",
        "# true_labels = predictions.label_ids\n",
        "\n",
        "# # Confusion matrix\n",
        "# cm = confusion_matrix(true_labels, pred_labels)\n",
        "# print(\"Confusion Matrix:\")\n",
        "# print(\"         No  Can't  Yes\")\n",
        "# for i, label in enumerate([\"No\", \"Can't\", \"Yes\"]):\n",
        "#     print(f\"{label:8s} {cm[i]}\")\n",
        "\n",
        "# # Check if model ever predicts \"Yes\"\n",
        "# yes_predictions = np.sum(pred_labels == 2)\n",
        "# print(f\"\\nTotal 'Yes' predictions: {yes_predictions}\")"
      ],
      "metadata": {
        "_uuid": "b8e8771c-bc03-4d14-9238-308df5525097",
        "_cell_guid": "4095dbcc-12e0-4151-8df2-ccf0f07e6a0b",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T13:23:43.023853Z",
          "iopub.status.idle": "2026-01-26T13:23:43.02411Z",
          "shell.execute_reply.started": "2026-01-26T13:23:43.023995Z",
          "shell.execute_reply": "2026-01-26T13:23:43.02401Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "FI8Bty6ufgeb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# \"\"\"\n",
        "# COMPLETE INFERENCE PIPELINE FOR SEMEVAL 2026 TASK 10\n",
        "# Step 1: Rehydrate test set\n",
        "# Step 2: Generate predictions\n",
        "# Step 3: Create submission\n",
        "# \"\"\"\n",
        "\n",
        "# import json\n",
        "# import os\n",
        "# import zipfile\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# import requests\n",
        "# import time\n",
        "# from tqdm import tqdm\n",
        "# from typing import List, Dict, Any\n",
        "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "# from bs4 import BeautifulSoup\n",
        "# from markdown import markdown\n",
        "# import re\n",
        "\n",
        "# # =============================\n",
        "# # STEP 1: REHYDRATE TEST DATA\n",
        "# # =============================\n",
        "\n",
        "# def markdown_to_text(markdown_string):\n",
        "#     \"\"\"Converts a markdown string to plaintext\"\"\"\n",
        "#     html = markdown(markdown_string)\n",
        "#     html = re.sub(r'<pre>(.*?)</pre>', ' ', html)\n",
        "#     html = re.sub(r'<code>(.*?)</code>', ' ', html)\n",
        "#     soup = BeautifulSoup(html, \"html.parser\")\n",
        "#     text = ' '.join(soup.find_all(string=True))\n",
        "#     return text\n",
        "\n",
        "\n",
        "# def replace_urls(x, url_replacement_token='[URL]'):\n",
        "#     return re.sub(r\"http\\S+\", url_replacement_token, x)\n",
        "\n",
        "\n",
        "# def replace_ss_prefix(x):\n",
        "#     return re.sub(\n",
        "#         r'^\\W*(summary statement|submission statement|ss)[^a-zA-Z]*',\n",
        "#         \"\", x, flags=re.I | re.U\n",
        "#     ).strip()\n",
        "\n",
        "\n",
        "# def preprocess(x):\n",
        "#     return replace_ss_prefix(replace_urls(markdown_to_text(x)))\n",
        "\n",
        "\n",
        "# def fetch_from_arctic_shift(comment_ids, max_retries=2, timeout=15):\n",
        "#     \"\"\"Fetch comments from Arctic Shift API\"\"\"\n",
        "#     url = \"https://arctic-shift.photon-reddit.com/api/comments/ids\"\n",
        "#     params = {\n",
        "#         \"ids\": \",\".join(comment_ids),\n",
        "#         \"fields\": \"body,subreddit,id\"\n",
        "#     }\n",
        "\n",
        "#     for attempt in range(max_retries):\n",
        "#         try:\n",
        "#             with requests.Session() as session:\n",
        "#                 session.headers.update({'Connection': 'close'})\n",
        "#                 response = session.get(url, params=params, timeout=timeout)\n",
        "#                 response.raise_for_status()\n",
        "#                 data = response.json()\n",
        "\n",
        "#                 if \"data\" in data and isinstance(data[\"data\"], list):\n",
        "#                     return {\n",
        "#                         item['id']: item\n",
        "#                         for item in data[\"data\"]\n",
        "#                         if item.get('body', '[deleted]').strip() not in ['[deleted]', '[removed]', '']\n",
        "#                     }\n",
        "#         except Exception:\n",
        "#             if attempt < max_retries - 1:\n",
        "#                 time.sleep(1)\n",
        "\n",
        "#     return {}\n",
        "\n",
        "\n",
        "# def rehydrate_test_data(input_file, output_file):\n",
        "#     \"\"\"\n",
        "#     Rehydrate the test_redacted.jsonl file\n",
        "#     CRITICAL: This must be done before inference!\n",
        "#     \"\"\"\n",
        "#     print(\"\\n\" + \"=\"*70)\n",
        "#     print(\"STEP 1: REHYDRATING TEST DATA\")\n",
        "#     print(\"=\"*70)\n",
        "\n",
        "#     if not os.path.exists(input_file):\n",
        "#         print(f\"‚ùå ERROR: Test file not found: {input_file}\")\n",
        "#         print(f\"   Please ensure you have the test_redacted.jsonl file!\")\n",
        "#         return False\n",
        "\n",
        "#     # Check if already rehydrated\n",
        "#     if os.path.exists(output_file):\n",
        "#         print(f\"‚ö†Ô∏è  {output_file} already exists!\")\n",
        "#         user_input = input(\"Rehydrate again? (y/n): \").lower()\n",
        "#         if user_input != 'y':\n",
        "#             print(\"Using existing rehydrated file...\")\n",
        "#             return True\n",
        "\n",
        "#     # Load test IDs\n",
        "#     items_to_process = []\n",
        "#     original_data_map = {}\n",
        "\n",
        "#     with open(input_file, encoding='utf-8') as f:\n",
        "#         for line in f:\n",
        "#             try:\n",
        "#                 item = json.loads(line)\n",
        "#                 if '_id' in item and item['_id'].startswith('t1_'):\n",
        "#                     items_to_process.append(item['_id'])\n",
        "#                     original_data_map[item['_id']] = item\n",
        "#             except:\n",
        "#                 pass\n",
        "\n",
        "#     print(f\"üìä Found {len(items_to_process)} test comments to rehydrate\")\n",
        "\n",
        "#     if len(items_to_process) == 0:\n",
        "#         print(\"‚ùå No valid IDs found in test file!\")\n",
        "#         return False\n",
        "\n",
        "#     # Rehydrate in batches\n",
        "#     batch_size = 20\n",
        "#     rehydrated_data = []\n",
        "#     deleted_count = 0\n",
        "\n",
        "#     for i in tqdm(range(0, len(items_to_process), batch_size), desc=\"Rehydrating\"):\n",
        "#         batch_full_ids = items_to_process[i:i + batch_size]\n",
        "#         batch_clean_ids = [x[3:] for x in batch_full_ids]\n",
        "\n",
        "#         rehydrated_map = fetch_from_arctic_shift(batch_clean_ids)\n",
        "\n",
        "#         for fid in batch_full_ids:\n",
        "#             cid = fid[3:]\n",
        "#             if cid in rehydrated_map:\n",
        "#                 r = rehydrated_map[cid]\n",
        "#                 o = original_data_map[fid]\n",
        "#                 rehydrated_data.append({\n",
        "#                     \"_id\": fid,\n",
        "#                     \"text\": preprocess(r[\"body\"]),\n",
        "#                     \"subreddit\": r[\"subreddit\"],\n",
        "#                     \"conspiracy\": o.get(\"conspiracy\"),\n",
        "#                     \"markers\": o.get(\"markers\", []),\n",
        "#                     \"annotator\": o.get(\"annotator\")\n",
        "#                 })\n",
        "#             else:\n",
        "#                 deleted_count += 1\n",
        "\n",
        "#         time.sleep(0.8)\n",
        "\n",
        "#     # Save rehydrated data\n",
        "#     with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "#         for x in rehydrated_data:\n",
        "#             f.write(json.dumps(x, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "#     success_rate = len(rehydrated_data) / len(items_to_process) * 100\n",
        "#     print(f\"\\n‚úÖ Rehydrated {len(rehydrated_data)}/{len(items_to_process)} ({success_rate:.1f}%)\")\n",
        "#     print(f\"üóëÔ∏è  Deleted/removed: {deleted_count}\")\n",
        "\n",
        "#     if success_rate < 50:\n",
        "#         print(\"\\n‚ö†Ô∏è  WARNING: Low success rate!\")\n",
        "#         print(\"   Consider using a VPN or contacting organizers\")\n",
        "\n",
        "#     return True\n",
        "\n",
        "\n",
        "# # =============================\n",
        "# # STEP 2: LOAD MODEL & PREDICT\n",
        "# # =============================\n",
        "\n",
        "# def make_marker_text(text: str, markers: List[Dict]) -> str:\n",
        "#     \"\"\"Wrap marker spans in special tokens\"\"\"\n",
        "#     if not markers:\n",
        "#         return text\n",
        "\n",
        "#     offset = 0\n",
        "#     for m in sorted(markers, key=lambda x: x['startIndex']):\n",
        "#         start = m['startIndex'] + offset\n",
        "#         end = m['endIndex'] + offset\n",
        "#         token_start = f\"[{m['type'].upper()}]\"\n",
        "#         token_end = f\"[/{m['type'].upper()}]\"\n",
        "#         text = text[:start] + token_start + text[start:end] + token_end + text[end:]\n",
        "#         offset += len(token_start) + len(token_end)\n",
        "\n",
        "#     return text\n",
        "\n",
        "\n",
        "# def load_model_for_inference(model_path, model_name=\"roberta-base\"):\n",
        "#     \"\"\"Load trained model for inference\"\"\"\n",
        "#     print(\"\\n\" + \"=\"*70)\n",
        "#     print(\"STEP 2: LOADING MODEL\")\n",
        "#     print(\"=\"*70)\n",
        "\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     print(f\"Device: {device}\")\n",
        "\n",
        "#     # Load tokenizer\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "#     # Add special tokens\n",
        "#     special_tokens = [\n",
        "#         \"[EVIDENCE]\", \"[/EVIDENCE]\",\n",
        "#         \"[ACTOR]\", \"[/ACTOR]\",\n",
        "#         \"[ACTION]\", \"[/ACTION]\",\n",
        "#         \"[VICTIM]\", \"[/VICTIM]\",\n",
        "#         \"[EFFECT]\", \"[/EFFECT]\"\n",
        "#     ]\n",
        "#     tokenizer.add_tokens(special_tokens)\n",
        "\n",
        "#     # Load model\n",
        "#     id2label = {0: \"No\", 1: \"Yes\"}\n",
        "#     label2id = {\"No\": 0, \"Yes\": 1}\n",
        "\n",
        "#     model = AutoModelForSequenceClassification.from_pretrained(\n",
        "#         model_path,\n",
        "#         num_labels=2,\n",
        "#         id2label=id2label,\n",
        "#         label2id=label2id,\n",
        "#         ignore_mismatched_sizes=True\n",
        "#     )\n",
        "#     model.to(device)\n",
        "#     model.eval()\n",
        "\n",
        "#     print(\"‚úÖ Model loaded successfully!\")\n",
        "#     return tokenizer, model, device\n",
        "\n",
        "\n",
        "# def predict_single(text: str, markers: List[Dict], tokenizer, model, device):\n",
        "#     \"\"\"Predict conspiracy label for a single text\"\"\"\n",
        "\n",
        "#     # Apply marker formatting\n",
        "#     if markers:\n",
        "#         text = make_marker_text(text, markers)\n",
        "\n",
        "#     # Tokenize\n",
        "#     inputs = tokenizer(\n",
        "#         text,\n",
        "#         padding=\"max_length\",\n",
        "#         truncation=True,\n",
        "#         max_length=256,\n",
        "#         return_tensors=\"pt\"\n",
        "#     )\n",
        "\n",
        "#     inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "#     # Predict\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model(**inputs)\n",
        "#         logits = outputs.logits\n",
        "#         probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "\n",
        "#     # Get prediction\n",
        "#     pred_id = np.argmax(probs)\n",
        "#     pred_label = \"Yes\" if pred_id == 1 else \"No\"\n",
        "\n",
        "#     return pred_label\n",
        "\n",
        "\n",
        "# # =============================\n",
        "# # STEP 3: GENERATE SUBMISSION\n",
        "# # =============================\n",
        "\n",
        "# def generate_submission(test_file, output_zip, model_path):\n",
        "#     \"\"\"Generate submission file\"\"\"\n",
        "#     print(\"\\n\" + \"=\"*70)\n",
        "#     print(\"STEP 3: GENERATING PREDICTIONS\")\n",
        "#     print(\"=\"*70)\n",
        "\n",
        "#     # Load model\n",
        "#     tokenizer, model, device = load_model_for_inference(model_path)\n",
        "\n",
        "#     # Load test data\n",
        "#     print(f\"\\nLoading test data from: {test_file}\")\n",
        "#     test_data = []\n",
        "\n",
        "#     with open(test_file, 'r', encoding='utf-8') as f:\n",
        "#         for line in f:\n",
        "#             try:\n",
        "#                 test_data.append(json.loads(line))\n",
        "#             except:\n",
        "#                 pass\n",
        "\n",
        "#     print(f\"Loaded {len(test_data)} test examples\")\n",
        "\n",
        "#     # Generate predictions\n",
        "#     predictions = []\n",
        "\n",
        "#     print(\"\\nGenerating predictions...\")\n",
        "#     for item in tqdm(test_data):\n",
        "#         doc_id = item['_id']\n",
        "#         text = item.get('text', '')\n",
        "#         markers = item.get('markers', [])\n",
        "\n",
        "#         if not text:\n",
        "#             # If no text, predict \"No\" as fallback\n",
        "#             pred = \"No\"\n",
        "#         else:\n",
        "#             pred = predict_single(text, markers, tokenizer, model, device)\n",
        "\n",
        "#         predictions.append({\n",
        "#             \"_id\": doc_id,\n",
        "#             \"conspiracy\": pred,\n",
        "#             \"markers\": []  # Empty for detection task\n",
        "#         })\n",
        "\n",
        "#     # Validate predictions\n",
        "#     print(\"\\n\" + \"=\"*70)\n",
        "#     print(\"VALIDATION\")\n",
        "#     print(\"=\"*70)\n",
        "\n",
        "#     labels = [p['conspiracy'] for p in predictions]\n",
        "#     yes_count = labels.count(\"Yes\")\n",
        "#     no_count = labels.count(\"No\")\n",
        "#     yes_pct = yes_count / len(labels) * 100\n",
        "\n",
        "#     print(f\"Total predictions: {len(predictions)}\")\n",
        "#     print(f\"  Yes: {yes_count} ({yes_pct:.2f}%)\")\n",
        "#     print(f\"  No:  {no_count} ({100-yes_pct:.2f}%)\")\n",
        "\n",
        "#     # Check for invalid labels\n",
        "#     invalid = [l for l in labels if l not in [\"Yes\", \"No\"]]\n",
        "#     if invalid:\n",
        "#         print(f\"\\n‚ùå ERROR: Found {len(invalid)} invalid labels!\")\n",
        "#         return False\n",
        "\n",
        "#     print(\"\\n‚úÖ All predictions are valid\")\n",
        "\n",
        "#     # Save submission\n",
        "#     print(\"\\n\" + \"=\"*70)\n",
        "#     print(\"CREATING SUBMISSION FILE\")\n",
        "#     print(\"=\"*70)\n",
        "\n",
        "#     temp_file = \"submission.jsonl\"\n",
        "\n",
        "#     with open(temp_file, 'w', encoding='utf-8') as f:\n",
        "#         for pred in predictions:\n",
        "#             f.write(json.dumps(pred) + '\\n')\n",
        "\n",
        "#     # Create ZIP\n",
        "#     with zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
        "#         zf.write(temp_file, arcname=\"submission.jsonl\")\n",
        "\n",
        "#     os.remove(temp_file)\n",
        "\n",
        "#     print(f\"‚úÖ Submission created: {output_zip}\")\n",
        "#     print(f\"   File size: {os.path.getsize(output_zip) / 1024:.2f} KB\")\n",
        "\n",
        "#     return True\n",
        "\n",
        "\n",
        "# # =============================\n",
        "# # MAIN EXECUTION\n",
        "# # =============================\n",
        "\n",
        "# def main():\n",
        "#     \"\"\"Complete inference pipeline\"\"\"\n",
        "\n",
        "#     print(\"\\n\" + \"=\"*70)\n",
        "#     print(\"SEMEVAL 2026 TASK 10 - COMPLETE INFERENCE PIPELINE\")\n",
        "#     print(\"=\"*70)\n",
        "\n",
        "#     # Configuration - UPDATE THESE PATHS!\n",
        "#     TEST_REDACTED = \"/kaggle/input/test-reducted/test_rehydrated.jsonl\"\n",
        "#     TEST_REHYDRATED = \"test_rehydrated.jsonl\"\n",
        "#     MODEL_PATH = \"./results/checkpoint-best\"\n",
        "#     OUTPUT_ZIP = \"/kaggle/working/submission.zip\"\n",
        "\n",
        "#     # Step 1: Rehydrate test data\n",
        "#     success = rehydrate_test_data(TEST_REDACTED, TEST_REHYDRATED)\n",
        "\n",
        "#     if not success:\n",
        "#         print(\"\\n‚ùå Rehydration failed! Cannot proceed.\")\n",
        "#         print(\"\\nOptions:\")\n",
        "#         print(\"1. Try again with VPN\")\n",
        "#         print(\"2. Contact organizers for pre-rehydrated test set\")\n",
        "#         print(\"3. Check if test file path is correct\")\n",
        "#         return\n",
        "\n",
        "#     # Step 2 & 3: Generate predictions and create submission\n",
        "#     success = generate_submission(TEST_REHYDRATED, OUTPUT_ZIP, MODEL_PATH)\n",
        "\n",
        "#     if success:\n",
        "#         print(\"\\n\" + \"=\"*70)\n",
        "#         print(\"‚úÖ SUBMISSION READY!\")\n",
        "#         print(\"=\"*70)\n",
        "#         print(f\"\\nFile: {OUTPUT_ZIP}\")\n",
        "#         print(\"\\nNext steps:\")\n",
        "#         print(\"1. Download submission.zip\")\n",
        "#         print(\"2. Go to Codabench\")\n",
        "#         print(\"3. Upload to 'My Submissions'\")\n",
        "#         print(\"4. Wait for evaluation\")\n",
        "#         print(\"=\"*70)\n",
        "#     else:\n",
        "#         print(\"\\n‚ùå Submission generation failed!\")\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T13:23:43.025452Z",
          "iopub.status.idle": "2026-01-26T13:23:43.025823Z",
          "shell.execute_reply.started": "2026-01-26T13:23:43.025629Z",
          "shell.execute_reply": "2026-01-26T13:23:43.025649Z"
        },
        "id": "9eNWF2vffgeb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# \"\"\"\n",
        "# DIAGNOSTIC TOOL: Find out why your submission scored 0.00\n",
        "# \"\"\"\n",
        "\n",
        "# import json\n",
        "# import os\n",
        "# import zipfile\n",
        "# from collections import Counter\n",
        "\n",
        "# def check_submission_file(submission_path):\n",
        "#     \"\"\"\n",
        "#     Analyze a submission file to find issues\n",
        "#     \"\"\"\n",
        "#     print(\"\\n\" + \"=\"*70)\n",
        "#     print(\"DIAGNOSTIC: CHECKING SUBMISSION FILE\")\n",
        "#     print(\"=\"*70)\n",
        "\n",
        "#     # Check if file exists\n",
        "#     if not os.path.exists(submission_path):\n",
        "#         print(f\"‚ùå ERROR: File not found: {submission_path}\")\n",
        "#         return False\n",
        "\n",
        "#     print(f\"‚úÖ File exists: {submission_path}\")\n",
        "#     print(f\"   Size: {os.path.getsize(submission_path) / 1024:.2f} KB\")\n",
        "\n",
        "#     # If it's a ZIP, extract it first\n",
        "#     if submission_path.endswith('.zip'):\n",
        "#         print(\"\\nüì¶ Extracting ZIP file...\")\n",
        "#         try:\n",
        "#             with zipfile.ZipFile(submission_path, 'r') as zf:\n",
        "#                 file_list = zf.namelist()\n",
        "#                 print(f\"   Files in ZIP: {file_list}\")\n",
        "\n",
        "#                 if 'submission.jsonl' not in file_list:\n",
        "#                     print(\"‚ùå ERROR: submission.jsonl not found in ZIP!\")\n",
        "#                     return False\n",
        "\n",
        "#                 # Extract to temp location\n",
        "#                 zf.extract('submission.jsonl', '/tmp/')\n",
        "#                 jsonl_path = '/tmp/submission.jsonl'\n",
        "#                 print(\"‚úÖ Extracted submission.jsonl\")\n",
        "#         except Exception as e:\n",
        "#             print(f\"‚ùå ERROR extracting ZIP: {e}\")\n",
        "#             return False\n",
        "#     else:\n",
        "#         jsonl_path = submission_path\n",
        "\n",
        "#     # Analyze JSONL content\n",
        "#     print(\"\\nüìä ANALYZING CONTENT...\")\n",
        "\n",
        "#     try:\n",
        "#         data = []\n",
        "#         with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
        "#             for line_num, line in enumerate(f, 1):\n",
        "#                 try:\n",
        "#                     item = json.loads(line.strip())\n",
        "#                     data.append(item)\n",
        "#                 except json.JSONDecodeError:\n",
        "#                     print(f\"‚ö†Ô∏è  Invalid JSON at line {line_num}\")\n",
        "\n",
        "#         print(f\"‚úÖ Loaded {len(data)} predictions\")\n",
        "\n",
        "#         if len(data) == 0:\n",
        "#             print(\"‚ùå ERROR: Submission file is empty!\")\n",
        "#             return False\n",
        "\n",
        "#         # Check first few entries\n",
        "#         print(\"\\nüìù FIRST 3 ENTRIES:\")\n",
        "#         for i, item in enumerate(data[:3], 1):\n",
        "#             print(f\"\\n{i}. {json.dumps(item, indent=2)}\")\n",
        "\n",
        "#         # Check required fields\n",
        "#         print(\"\\nüîç CHECKING REQUIRED FIELDS...\")\n",
        "\n",
        "#         missing_id = 0\n",
        "#         missing_conspiracy = 0\n",
        "#         missing_markers = 0\n",
        "\n",
        "#         for item in data:\n",
        "#             if '_id' not in item:\n",
        "#                 missing_id += 1\n",
        "#             if 'conspiracy' not in item:\n",
        "#                 missing_conspiracy += 1\n",
        "#             if 'markers' not in item:\n",
        "#                 missing_markers += 1\n",
        "\n",
        "#         if missing_id > 0:\n",
        "#             print(f\"‚ùå {missing_id} entries missing '_id' field!\")\n",
        "#         else:\n",
        "#             print(\"‚úÖ All entries have '_id'\")\n",
        "\n",
        "#         if missing_conspiracy > 0:\n",
        "#             print(f\"‚ùå {missing_conspiracy} entries missing 'conspiracy' field!\")\n",
        "#         else:\n",
        "#             print(\"‚úÖ All entries have 'conspiracy'\")\n",
        "\n",
        "#         if missing_markers > 0:\n",
        "#             print(f\"‚ùå {missing_markers} entries missing 'markers' field!\")\n",
        "#         else:\n",
        "#             print(\"‚úÖ All entries have 'markers'\")\n",
        "\n",
        "#         # Check ID format\n",
        "#         print(\"\\nüÜî CHECKING ID FORMAT...\")\n",
        "#         id_formats = Counter([item['_id'][:3] if '_id' in item else 'MISSING' for item in data])\n",
        "#         print(f\"   ID prefixes: {dict(id_formats)}\")\n",
        "\n",
        "#         if 't1_' not in id_formats:\n",
        "#             print(\"‚ùå WARNING: No 't1_' prefixes found!\")\n",
        "#             print(\"   Test data should have comment IDs starting with 't1_'\")\n",
        "\n",
        "#         # Check conspiracy labels\n",
        "#         print(\"\\nüè∑Ô∏è  CHECKING CONSPIRACY LABELS...\")\n",
        "#         labels = [item.get('conspiracy', 'MISSING') for item in data]\n",
        "#         label_counts = Counter(labels)\n",
        "\n",
        "#         print(\"   Label distribution:\")\n",
        "#         for label, count in label_counts.most_common():\n",
        "#             pct = count / len(labels) * 100\n",
        "#             print(f\"     {label:12s}: {count:5d} ({pct:5.2f}%)\")\n",
        "\n",
        "#         # Check for invalid labels\n",
        "#         invalid_labels = [l for l in labels if l not in ['Yes', 'No']]\n",
        "#         if invalid_labels:\n",
        "#             print(f\"\\n‚ùå ERROR: Found {len(invalid_labels)} invalid labels!\")\n",
        "#             print(f\"   Invalid values: {set(invalid_labels)}\")\n",
        "#             print(\"   Only 'Yes' and 'No' are allowed!\")\n",
        "#         else:\n",
        "#             print(\"\\n‚úÖ All labels are valid (Yes/No only)\")\n",
        "\n",
        "#         # Check marker format\n",
        "#         print(\"\\nüìç CHECKING MARKERS...\")\n",
        "\n",
        "#         total_markers = sum(len(item.get('markers', [])) for item in data)\n",
        "#         items_with_markers = sum(1 for item in data if len(item.get('markers', [])) > 0)\n",
        "\n",
        "#         print(f\"   Total markers: {total_markers}\")\n",
        "#         print(f\"   Items with markers: {items_with_markers}/{len(data)}\")\n",
        "\n",
        "#         if items_with_markers > 0:\n",
        "#             print(\"   ‚ÑπÔ∏è  Note: Markers are optional for detection task\")\n",
        "\n",
        "#         # Final verdict\n",
        "#         print(\"\\n\" + \"=\"*70)\n",
        "#         print(\"DIAGNOSTIC SUMMARY\")\n",
        "#         print(\"=\"*70)\n",
        "\n",
        "#         issues = []\n",
        "\n",
        "#         if len(data) == 0:\n",
        "#             issues.append(\"Empty submission file\")\n",
        "#         if missing_id > 0:\n",
        "#             issues.append(f\"{missing_id} missing '_id' fields\")\n",
        "#         if missing_conspiracy > 0:\n",
        "#             issues.append(f\"{missing_conspiracy} missing 'conspiracy' fields\")\n",
        "#         if invalid_labels:\n",
        "#             issues.append(f\"{len(invalid_labels)} invalid labels\")\n",
        "#         if 't1_' not in id_formats:\n",
        "#             issues.append(\"Wrong ID format (not starting with 't1_')\")\n",
        "\n",
        "#         if issues:\n",
        "#             print(\"‚ùå ISSUES FOUND:\")\n",
        "#             for i, issue in enumerate(issues, 1):\n",
        "#                 print(f\"   {i}. {issue}\")\n",
        "#             print(\"\\nLikely cause of 0.00 score!\")\n",
        "#             return False\n",
        "#         else:\n",
        "#             print(\"‚úÖ No obvious issues found!\")\n",
        "#             print(\"\\nIf still getting 0.00, possible reasons:\")\n",
        "#             print(\"1. Test set IDs don't match what Codabench expects\")\n",
        "#             print(\"2. You uploaded to wrong task (detection vs extraction)\")\n",
        "#             print(\"3. Codabench server issue - try resubmitting\")\n",
        "#             print(\"4. Check if you need to rehydrate test data first\")\n",
        "#             return True\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"‚ùå ERROR analyzing file: {e}\")\n",
        "#         return False\n",
        "\n",
        "\n",
        "# def compare_with_expected(submission_path, expected_test_path=None):\n",
        "#     \"\"\"\n",
        "#     Compare your submission IDs with expected test IDs\n",
        "#     \"\"\"\n",
        "#     print(\"\\n\" + \"=\"*70)\n",
        "#     print(\"COMPARING WITH TEST DATA\")\n",
        "#     print(\"=\"*70)\n",
        "\n",
        "#     if not expected_test_path or not os.path.exists(expected_test_path):\n",
        "#         print(\"‚ö†Ô∏è  Test file not provided or not found\")\n",
        "#         print(\"   Skipping ID comparison\")\n",
        "#         return\n",
        "\n",
        "#     # Load submission IDs\n",
        "#     submission_ids = set()\n",
        "\n",
        "#     if submission_path.endswith('.zip'):\n",
        "#         with zipfile.ZipFile(submission_path, 'r') as zf:\n",
        "#             with zf.open('submission.jsonl') as f:\n",
        "#                 for line in f:\n",
        "#                     try:\n",
        "#                         item = json.loads(line)\n",
        "#                         submission_ids.add(item.get('_id'))\n",
        "#                     except:\n",
        "#                         pass\n",
        "#     else:\n",
        "#         with open(submission_path, 'r') as f:\n",
        "#             for line in f:\n",
        "#                 try:\n",
        "#                     item = json.loads(line)\n",
        "#                     submission_ids.add(item.get('_id'))\n",
        "#                 except:\n",
        "#                     pass\n",
        "\n",
        "#     # Load expected IDs\n",
        "#     expected_ids = set()\n",
        "#     with open(expected_test_path, 'r') as f:\n",
        "#         for line in f:\n",
        "#             try:\n",
        "#                 item = json.loads(line)\n",
        "#                 expected_ids.add(item.get('_id'))\n",
        "#             except:\n",
        "#                 pass\n",
        "\n",
        "#     # Compare\n",
        "#     print(f\"Submission IDs: {len(submission_ids)}\")\n",
        "#     print(f\"Expected IDs:   {len(expected_ids)}\")\n",
        "\n",
        "#     missing = expected_ids - submission_ids\n",
        "#     extra = submission_ids - expected_ids\n",
        "\n",
        "#     if missing:\n",
        "#         print(f\"\\n‚ö†Ô∏è  {len(missing)} IDs in test set but NOT in submission!\")\n",
        "#         print(f\"   First 5 missing: {list(missing)[:5]}\")\n",
        "\n",
        "#     if extra:\n",
        "#         print(f\"\\n‚ö†Ô∏è  {len(extra)} IDs in submission but NOT in test set!\")\n",
        "#         print(f\"   First 5 extra: {list(extra)[:5]}\")\n",
        "\n",
        "#     if not missing and not extra:\n",
        "#         print(\"\\n‚úÖ All IDs match perfectly!\")\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "\n",
        "#     # UPDATE THIS PATH to your submission file\n",
        "#     SUBMISSION_PATH = \"/kaggle/working/submission.zip\"\n",
        "\n",
        "#     # Optional: path to test_redacted.jsonl to compare IDs\n",
        "#     TEST_PATH = \"/kaggle/input/test-reducted/test_rehydrated.jsonl\"\n",
        "\n",
        "#     print(\"\\n\" + \"=\"*70)\n",
        "#     print(\"SUBMISSION DIAGNOSTIC TOOL\")\n",
        "#     print(\"=\"*70)\n",
        "\n",
        "#     # Check submission\n",
        "#     is_valid = check_submission_file(SUBMISSION_PATH)\n",
        "\n",
        "#     # Compare with test data\n",
        "#     if os.path.exists(TEST_PATH):\n",
        "#         compare_with_expected(SUBMISSION_PATH, TEST_PATH)\n",
        "\n",
        "#     print(\"\\n\" + \"=\"*70)\n",
        "#     if is_valid:\n",
        "#         print(\"‚úÖ Submission appears valid\")\n",
        "#         print(\"   If still getting 0.00, contact organizers!\")\n",
        "#     else:\n",
        "#         print(\"‚ùå Issues found - fix them and regenerate submission\")\n",
        "#     print(\"=\"*70 + \"\\n\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T13:23:43.027046Z",
          "iopub.status.idle": "2026-01-26T13:23:43.027334Z",
          "shell.execute_reply.started": "2026-01-26T13:23:43.027218Z",
          "shell.execute_reply": "2026-01-26T13:23:43.027233Z"
        },
        "id": "otwPkjnzfgeb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# \"\"\"\n",
        "# DATA DIAGNOSTIC TOOL\n",
        "# Run this BEFORE training to catch issues early\n",
        "# \"\"\"\n",
        "\n",
        "# import json\n",
        "# from collections import Counter\n",
        "\n",
        "# def diagnose_dataset(file_path):\n",
        "#     \"\"\"\n",
        "#     Comprehensive dataset diagnostics\n",
        "#     \"\"\"\n",
        "#     print(\"\\n\" + \"=\"*70)\n",
        "#     print(f\"DIAGNOSING: {file_path}\")\n",
        "#     print(\"=\"*70)\n",
        "\n",
        "#     try:\n",
        "#         with open(file_path, 'r', encoding='utf-8') as f:\n",
        "#             data = [json.loads(line) for line in f]\n",
        "#     except FileNotFoundError:\n",
        "#         print(f\"‚ùå File not found: {file_path}\")\n",
        "#         return\n",
        "#     except Exception as e:\n",
        "#         print(f\"‚ùå Error loading file: {e}\")\n",
        "#         return\n",
        "\n",
        "#     print(f\"\\n‚úÖ Loaded {len(data)} entries\")\n",
        "\n",
        "#     # Basic checks\n",
        "#     has_id = sum(1 for item in data if '_id' in item)\n",
        "#     has_text = sum(1 for item in data if 'text' in item and item['text'])\n",
        "#     has_markers = sum(1 for item in data if 'markers' in item and len(item['markers']) > 0)\n",
        "#     has_conspiracy = sum(1 for item in data if 'conspiracy' in item)\n",
        "\n",
        "#     print(f\"\\nüìä FIELD COVERAGE:\")\n",
        "#     print(f\"   Has '_id':       {has_id}/{len(data)} ({has_id/len(data)*100:.1f}%)\")\n",
        "#     print(f\"   Has 'text':      {has_text}/{len(data)} ({has_text/len(data)*100:.1f}%)\")\n",
        "#     print(f\"   Has markers:     {has_markers}/{len(data)} ({has_markers/len(data)*100:.1f}%)\")\n",
        "#     print(f\"   Has conspiracy:  {has_conspiracy}/{len(data)} ({has_conspiracy/len(data)*100:.1f}%)\")\n",
        "\n",
        "#     # Text length stats\n",
        "#     text_lengths = [len(item.get('text', '')) for item in data if item.get('text')]\n",
        "#     if text_lengths:\n",
        "#         print(f\"\\nüìè TEXT LENGTH:\")\n",
        "#         print(f\"   Min:     {min(text_lengths)} chars\")\n",
        "#         print(f\"   Max:     {max(text_lengths)} chars\")\n",
        "#         print(f\"   Average: {sum(text_lengths)/len(text_lengths):.1f} chars\")\n",
        "#         print(f\"   Empty:   {sum(1 for l in text_lengths if l == 0)}\")\n",
        "\n",
        "#     # Marker statistics\n",
        "#     total_markers = sum(len(item.get('markers', [])) for item in data)\n",
        "#     marker_types = Counter()\n",
        "#     marker_lengths = []\n",
        "\n",
        "#     for item in data:\n",
        "#         for marker in item.get('markers', []):\n",
        "#             marker_types[marker.get('type', 'Unknown')] += 1\n",
        "#             start = marker.get('startIndex', 0)\n",
        "#             end = marker.get('endIndex', 0)\n",
        "#             marker_lengths.append(end - start)\n",
        "\n",
        "#     print(f\"\\nüéØ MARKERS:\")\n",
        "#     print(f\"   Total markers: {total_markers}\")\n",
        "#     if has_markers > 0:\n",
        "#         print(f\"   Avg per doc:   {total_markers / has_markers:.2f}\")\n",
        "\n",
        "#     if marker_types:\n",
        "#         print(f\"\\nüìä MARKER TYPES:\")\n",
        "#         for mtype, count in marker_types.most_common():\n",
        "#             pct = count / total_markers * 100 if total_markers > 0 else 0\n",
        "#             print(f\"   {mtype:12s}: {count:5d} ({pct:5.2f}%)\")\n",
        "\n",
        "#     if marker_lengths:\n",
        "#         print(f\"\\nüìè MARKER LENGTHS:\")\n",
        "#         print(f\"   Min:     {min(marker_lengths)} chars\")\n",
        "#         print(f\"   Max:     {max(marker_lengths)} chars\")\n",
        "#         print(f\"   Average: {sum(marker_lengths)/len(marker_lengths):.1f} chars\")\n",
        "\n",
        "#     # Conspiracy labels\n",
        "#     conspiracy_labels = Counter(item.get('conspiracy') for item in data)\n",
        "#     print(f\"\\nüè∑Ô∏è  CONSPIRACY LABELS:\")\n",
        "#     for label, count in conspiracy_labels.most_common():\n",
        "#         pct = count / len(data) * 100\n",
        "#         print(f\"   {str(label):12s}: {count:5d} ({pct:5.2f}%)\")\n",
        "\n",
        "#     # Quality checks\n",
        "#     print(f\"\\nüîç QUALITY CHECKS:\")\n",
        "\n",
        "#     issues = []\n",
        "\n",
        "#     # Check 1: Empty texts\n",
        "#     empty_texts = sum(1 for item in data if not item.get('text', '').strip())\n",
        "#     if empty_texts > 0:\n",
        "#         issues.append(f\"‚ùå {empty_texts} empty texts\")\n",
        "#     else:\n",
        "#         print(f\"   ‚úÖ No empty texts\")\n",
        "\n",
        "#     # Check 2: Markers without text\n",
        "#     markers_no_text = sum(1 for item in data if len(item.get('markers', [])) > 0 and not item.get('text'))\n",
        "#     if markers_no_text > 0:\n",
        "#         issues.append(f\"‚ùå {markers_no_text} items have markers but no text\")\n",
        "#     else:\n",
        "#         print(f\"   ‚úÖ All markers have associated text\")\n",
        "\n",
        "#     # Check 3: Marker indices out of bounds\n",
        "#     invalid_markers = 0\n",
        "#     for item in data:\n",
        "#         text = item.get('text', '')\n",
        "#         text_len = len(text)\n",
        "#         for marker in item.get('markers', []):\n",
        "#             start = marker.get('startIndex', 0)\n",
        "#             end = marker.get('endIndex', 0)\n",
        "#             if start < 0 or end > text_len or start >= end:\n",
        "#                 invalid_markers += 1\n",
        "\n",
        "#     if invalid_markers > 0:\n",
        "#         issues.append(f\"‚ùå {invalid_markers} markers have invalid indices\")\n",
        "#     else:\n",
        "#         print(f\"   ‚úÖ All marker indices are valid\")\n",
        "\n",
        "#     # Check 4: Unknown marker types\n",
        "#     valid_types = {\"Actor\", \"Action\", \"Effect\", \"Evidence\", \"Victim\"}\n",
        "#     unknown_types = set(marker_types.keys()) - valid_types\n",
        "#     if unknown_types:\n",
        "#         issues.append(f\"‚ùå Unknown marker types: {unknown_types}\")\n",
        "#     else:\n",
        "#         print(f\"   ‚úÖ All marker types are valid\")\n",
        "\n",
        "#     # Check 5: Conspiracy label coverage\n",
        "#     if has_conspiracy < len(data) * 0.9:\n",
        "#         issues.append(f\"‚ö†Ô∏è  Only {has_conspiracy/len(data)*100:.1f}% have conspiracy labels\")\n",
        "#     else:\n",
        "#         print(f\"   ‚úÖ Good conspiracy label coverage\")\n",
        "\n",
        "#     # Summary\n",
        "#     print(f\"\\n\" + \"=\"*70)\n",
        "#     if issues:\n",
        "#         print(\"‚ö†Ô∏è  ISSUES FOUND:\")\n",
        "#         for issue in issues:\n",
        "#             print(f\"   {issue}\")\n",
        "#     else:\n",
        "#         print(\"‚úÖ NO ISSUES FOUND - Data looks good!\")\n",
        "#     print(\"=\"*70)\n",
        "\n",
        "#     # Sample entries\n",
        "#     print(f\"\\nüìù SAMPLE ENTRIES (First 2):\")\n",
        "#     for i, item in enumerate(data[:2], 1):\n",
        "#         print(f\"\\n{i}. ID: {item.get('_id', 'N/A')}\")\n",
        "#         print(f\"   Text: {item.get('text', '')[:100]}...\")\n",
        "#         print(f\"   Conspiracy: {item.get('conspiracy', 'N/A')}\")\n",
        "#         print(f\"   Markers: {len(item.get('markers', []))}\")\n",
        "#         if item.get('markers'):\n",
        "#             for j, marker in enumerate(item['markers'][:2], 1):\n",
        "#                 print(f\"      {j}. {marker.get('type')}: \\\"{marker.get('text', '')}\\\"\")\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     print(\"\\n\" + \"=\"*70)\n",
        "#     print(\"DATA DIAGNOSTIC TOOL\")\n",
        "#     print(\"=\"*70)\n",
        "\n",
        "#     # Diagnose train\n",
        "#     diagnose_dataset(\"train_rehydrated.jsonl\")\n",
        "\n",
        "#     # Diagnose dev\n",
        "#     diagnose_dataset(\"dev_rehydrated.jsonl\")\n",
        "\n",
        "#     print(\"\\n\" + \"=\"*70)\n",
        "#     print(\"DIAGNOSTIC COMPLETE!\")\n",
        "#     print(\"=\"*70)\n",
        "#     print(\"\\nIf you see issues above, fix them before training.\")\n",
        "#     print(\"If all looks good, proceed with training!\")\n",
        "#     print(\"=\"*70 + \"\\n\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T13:23:43.02877Z",
          "iopub.status.idle": "2026-01-26T13:23:43.029137Z",
          "shell.execute_reply.started": "2026-01-26T13:23:43.028944Z",
          "shell.execute_reply": "2026-01-26T13:23:43.028968Z"
        },
        "id": "NlDzfeavfgeb"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}